# bot/services/content_analyzer.py - å…§å®¹åˆ†ææœå‹™
"""
å…§å®¹åˆ†ææœå‹™ v2.2.0
æä¾›æ–‡æœ¬æƒ…æ„Ÿåˆ†æã€å…§å®¹å¯©æ ¸ã€é€£çµåˆ†æã€çµ±è¨ˆæ´å¯Ÿç­‰åŠŸèƒ½
"""

import asyncio
import hashlib
import re
import time
import urllib.parse
from dataclasses import dataclass, field
from datetime import datetime, timezone
from enum import Enum
from typing import Any, Dict, List, Optional

from shared.cache_manager import cache_manager
from shared.logger import logger


class SentimentType(Enum):
    """æƒ…æ„Ÿé¡å‹"""

    POSITIVE = "positive"
    NEGATIVE = "negative"
    NEUTRAL = "neutral"
    MIXED = "mixed"


class ContentRiskLevel(Enum):
    """å…§å®¹é¢¨éšªç­‰ç´š"""

    SAFE = "safe"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class AnalysisType(Enum):
    """åˆ†æé¡å‹"""

    SENTIMENT = "sentiment"
    TOXICITY = "toxicity"
    SPAM = "spam"
    LANGUAGE = "language"
    KEYWORDS = "keywords"
    SUMMARY = "summary"
    LINK_SAFETY = "link_safety"


@dataclass
class SentimentResult:
    """æƒ…æ„Ÿåˆ†æçµæœ"""

    sentiment: SentimentType
    confidence: float
    positive_score: float
    negative_score: float
    neutral_score: float
    keywords: List[str] = field(default_factory=list)


@dataclass
class ToxicityResult:
    """æ¯’æ€§åˆ†æçµæœ"""

    is_toxic: bool
    toxicity_score: float
    categories: Dict[str, float] = field(default_factory=dict)  # å„é¡æ¯’æ€§è©•åˆ†
    flagged_phrases: List[str] = field(default_factory=list)


@dataclass
class LinkAnalysisResult:
    """é€£çµåˆ†æçµæœ"""

    url: str
    is_safe: bool
    risk_level: ContentRiskLevel
    risk_factors: List[str] = field(default_factory=list)
    domain_reputation: float = 0.0
    is_shortened: bool = False
    expanded_url: Optional[str] = None


@dataclass
class ContentAnalysisResult:
    """å…§å®¹åˆ†æçµæœ"""

    text: str
    analysis_type: AnalysisType
    timestamp: datetime
    processing_time: float
    success: bool
    sentiment: Optional[SentimentResult] = None
    toxicity: Optional[ToxicityResult] = None
    language: Optional[str] = None
    keywords: List[str] = field(default_factory=list)
    summary: Optional[str] = None
    links: List[LinkAnalysisResult] = field(default_factory=list)
    risk_level: ContentRiskLevel = ContentRiskLevel.SAFE
    confidence: float = 0.0
    error_message: Optional[str] = None


class ContentAnalyzer:
    """å…§å®¹åˆ†æå™¨"""

    def __init__(self):
        # æ¯’æ€§é—œéµè©åº«ï¼ˆå¯æ“´å±•ï¼‰
        self.toxic_keywords = {
            "harassment": ["é¨·æ“¾", "éœ¸å‡Œ", "å¨è„…", "æåš‡", "ä¾®è¾±", "ç¾è¾±", "æ­§è¦–"],
            "hate_speech": ["ä»‡æ¨", "ç¨®æ—", "æ­§è¦–", "åè¦‹", "æ’æ–¥"],
            "violence": ["æš´åŠ›", "å‚·å®³", "æ”»æ“Š", "æ®ºå®³", "æ¯†æ‰“", "æ‰“æ¶"],
            "inappropriate": ["è‰²æƒ…", "æ·«ç©¢", "çŒ¥è¤»", "ä¸é›…", "éœ²éª¨"],
            "spam": ["å»£å‘Š", "æ¨éŠ·", "è²·è³£", "ä»£è³¼", "åˆ·å–®", "é»æ“Š", "å…è²»", "è³ºéŒ¢"],
        }

        # æƒ…æ„Ÿè©åº«
        self.sentiment_words = {
            "positive": [
                "å¥½",
                "æ£’",
                "è®š",
                "æ„›",
                "å–œæ­¡",
                "é–‹å¿ƒ",
                "å¿«æ¨‚",
                "æ»¿æ„",
                "æˆåŠŸ",
                "å„ªç§€",
                "ç¾å¥½",
                "ç²¾å½©",
                "å®Œç¾",
                "è®šç¾",
                "æ„Ÿè¬",
                "å¹¸ç¦",
                "æº«æš–",
                "å‹å–„",
                "ç©æ¥µ",
            ],
            "negative": [
                "å£",
                "ç³Ÿ",
                "çˆ›",
                "æ¨",
                "è¨å­",
                "ç”Ÿæ°£",
                "é›£é",
                "å¤±æœ›",
                "å¤±æ•—",
                "ç³Ÿç³•",
                "å¯æ€•",
                "å™å¿ƒ",
                "ç—›è‹¦",
                "æ²®å–ª",
                "æ†¤æ€’",
                "ç…©èº",
                "ç„¦æ…®",
                "æ“”å¿ƒ",
                "å®³æ€•",
            ],
        }

        # å±éšªåŸŸåæ¸…å–®
        self.dangerous_domains = [
            "malware-example.com",
            "phishing-site.net",
            "spam-domain.org",
        ]

        # çŸ­ç¶²å€æœå‹™æ¸…å–®
        self.url_shorteners = [
            "bit.ly",
            "tinyurl.com",
            "goo.gl",
            "t.co",
            "short.link",
            "ow.ly",
            "is.gd",
            "buff.ly",
            "tiny.cc",
        ]

        logger.info("ğŸ“Š å…§å®¹åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")

    # ========== ä¸»è¦åˆ†ææ–¹æ³• ==========

    async def analyze_content(
        self, text: str, user_id: int = 0, analysis_types: List[AnalysisType] = None
    ) -> ContentAnalysisResult:
        """ç¶œåˆå…§å®¹åˆ†æ"""
        start_time = time.time()

        try:
            if analysis_types is None:
                analysis_types = [
                    AnalysisType.SENTIMENT,
                    AnalysisType.TOXICITY,
                    AnalysisType.KEYWORDS,
                ]

            result = ContentAnalysisResult(
                text=text,
                analysis_type=AnalysisType.SENTIMENT,  # ä¸»è¦é¡å‹
                timestamp=datetime.now(timezone.utc),
                processing_time=0,
                success=False,
            )

            # åŸ·è¡Œå„ç¨®åˆ†æ
            tasks = []

            if AnalysisType.SENTIMENT in analysis_types:
                tasks.append(self._analyze_sentiment(text))

            if AnalysisType.TOXICITY in analysis_types:
                tasks.append(self._analyze_toxicity(text))

            if AnalysisType.LANGUAGE in analysis_types:
                tasks.append(self._detect_language(text))

            if AnalysisType.KEYWORDS in analysis_types:
                tasks.append(self._extract_keywords(text))

            if AnalysisType.LINK_SAFETY in analysis_types:
                tasks.append(self._analyze_links(text))

            # ä¸¦è¡ŒåŸ·è¡Œåˆ†æä»»å‹™
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # è™•ç†çµæœ
            task_index = 0

            if AnalysisType.SENTIMENT in analysis_types:
                if not isinstance(results[task_index], Exception):
                    result.sentiment = results[task_index]
                task_index += 1

            if AnalysisType.TOXICITY in analysis_types:
                if not isinstance(results[task_index], Exception):
                    result.toxicity = results[task_index]
                task_index += 1

            if AnalysisType.LANGUAGE in analysis_types:
                if not isinstance(results[task_index], Exception):
                    result.language = results[task_index]
                task_index += 1

            if AnalysisType.KEYWORDS in analysis_types:
                if not isinstance(results[task_index], Exception):
                    result.keywords = results[task_index]
                task_index += 1

            if AnalysisType.LINK_SAFETY in analysis_types:
                if not isinstance(results[task_index], Exception):
                    result.links = results[task_index]
                task_index += 1

            # è¨ˆç®—æ•´é«”é¢¨éšªç­‰ç´š
            result.risk_level = await self._calculate_risk_level(result)

            # è¨ˆç®—ä¿¡å¿ƒåº¦
            result.confidence = await self._calculate_confidence(result)

            result.processing_time = time.time() - start_time
            result.success = True

            return result

        except Exception as e:
            logger.error(f"âŒ å…§å®¹åˆ†æå¤±æ•—: {e}")
            return ContentAnalysisResult(
                text=text,
                analysis_type=AnalysisType.SENTIMENT,
                timestamp=datetime.now(timezone.utc),
                processing_time=time.time() - start_time,
                success=False,
                error_message=str(e),
            )

    # ========== æƒ…æ„Ÿåˆ†æ ==========

    async def _analyze_sentiment(self, text: str) -> SentimentResult:
        """æƒ…æ„Ÿåˆ†æ"""
        try:
            # æª¢æŸ¥å¿«å– - ä½¿ç”¨ MD5 åƒ…ä½œç‚ºéå®‰å…¨çš„å¿«å–éµç”Ÿæˆ
            cache_key = f"sentiment:{hashlib.md5(text.encode(), usedforsecurity=False).hexdigest()}"
            cached_result = await cache_manager.get(cache_key)
            if cached_result:
                return SentimentResult(**cached_result)

            # ç°¡å–®çš„è¦å‰‡åŸºç¤æƒ…æ„Ÿåˆ†æ
            text_lower = text.lower()

            positive_count = 0
            negative_count = 0
            found_keywords = []

            # è¨ˆç®—æ­£é¢è©å½™
            for word in self.sentiment_words["positive"]:
                if word in text_lower:
                    positive_count += text_lower.count(word)
                    found_keywords.append(word)

            # è¨ˆç®—è² é¢è©å½™
            for word in self.sentiment_words["negative"]:
                if word in text_lower:
                    negative_count += text_lower.count(word)
                    found_keywords.append(word)

            # ç‰¹æ®Šè¡¨æƒ…ç¬¦è™Ÿå’Œæ¨™é»ç¬¦è™Ÿåˆ†æ
            positive_emojis = (
                text.count("ğŸ˜Š") + text.count("ğŸ˜„") + text.count("â¤ï¸") + text.count("ğŸ‘")
            )
            negative_emojis = (
                text.count("ğŸ˜¢") + text.count("ğŸ˜¡") + text.count("ğŸ’”") + text.count("ğŸ‘")
            )

            positive_count += positive_emojis
            negative_count += negative_emojis

            # è¨ˆç®—åˆ†æ•¸
            len(text.split())
            total_sentiment = positive_count + negative_count

            if total_sentiment == 0:
                sentiment = SentimentType.NEUTRAL
                positive_score = 0.5
                negative_score = 0.5
                confidence = 0.3
            else:
                positive_score = positive_count / total_sentiment
                negative_score = negative_count / total_sentiment

                if positive_score > negative_score:
                    sentiment = SentimentType.POSITIVE
                    confidence = min(0.9, positive_score * 0.8 + 0.1)
                elif negative_score > positive_score:
                    sentiment = SentimentType.NEGATIVE
                    confidence = min(0.9, negative_score * 0.8 + 0.1)
                else:
                    sentiment = SentimentType.MIXED
                    confidence = 0.5

            neutral_score = 1.0 - positive_score - negative_score

            result = SentimentResult(
                sentiment=sentiment,
                confidence=confidence,
                positive_score=positive_score,
                negative_score=negative_score,
                neutral_score=max(0, neutral_score),
                keywords=found_keywords[:10],  # é™åˆ¶é—œéµè©æ•¸é‡
            )

            # å¿«å–çµæœ
            await cache_manager.set(cache_key, result.__dict__, 1800)  # 30åˆ†é˜å¿«å–

            return result

        except Exception as e:
            logger.error(f"âŒ æƒ…æ„Ÿåˆ†æå¤±æ•—: {e}")
            return SentimentResult(
                sentiment=SentimentType.NEUTRAL,
                confidence=0.0,
                positive_score=0.0,
                negative_score=0.0,
                neutral_score=1.0,
            )

    # ========== æ¯’æ€§æª¢æ¸¬ ==========

    async def _analyze_toxicity(self, text: str) -> ToxicityResult:
        """æ¯’æ€§åˆ†æ"""
        try:
            text_lower = text.lower()

            toxicity_scores = {}
            flagged_phrases = []
            overall_toxicity = 0.0

            # æª¢æŸ¥å„é¡æ¯’æ€§å…§å®¹
            for category, keywords in self.toxic_keywords.items():
                category_score = 0.0
                category_count = 0

                for keyword in keywords:
                    if keyword in text_lower:
                        count = text_lower.count(keyword)
                        category_count += count
                        flagged_phrases.extend([keyword] * count)

                # è¨ˆç®—é¡åˆ¥åˆ†æ•¸ï¼ˆåŸºæ–¼å‡ºç¾é »ç‡å’Œæ–‡æœ¬é•·åº¦ï¼‰
                text_length = len(text.split())
                if text_length > 0 and category_count > 0:
                    category_score = min(1.0, (category_count / text_length) * 10)

                toxicity_scores[category] = category_score
                overall_toxicity = max(overall_toxicity, category_score)

            # æª¢æŸ¥é‡è¤‡å­—ç¬¦ï¼ˆå¯èƒ½æ˜¯spamï¼‰
            repeated_chars = re.findall(r"(.)\1{4,}", text)
            if repeated_chars:
                toxicity_scores["spam"] = min(1.0, len(repeated_chars) * 0.2)
                overall_toxicity = max(overall_toxicity, toxicity_scores["spam"])

            # æª¢æŸ¥éåº¦ä½¿ç”¨å¤§å¯«å­—æ¯
            upper_ratio = sum(1 for c in text if c.isupper()) / max(1, len(text))
            if upper_ratio > 0.7 and len(text) > 10:
                toxicity_scores["aggressive"] = min(1.0, upper_ratio)
                overall_toxicity = max(overall_toxicity, toxicity_scores["aggressive"])

            is_toxic = overall_toxicity > 0.3

            return ToxicityResult(
                is_toxic=is_toxic,
                toxicity_score=overall_toxicity,
                categories=toxicity_scores,
                flagged_phrases=list(set(flagged_phrases)),
            )

        except Exception as e:
            logger.error(f"âŒ æ¯’æ€§åˆ†æå¤±æ•—: {e}")
            return ToxicityResult(is_toxic=False, toxicity_score=0.0)

    # ========== èªè¨€æª¢æ¸¬ ==========

    async def _detect_language(self, text: str) -> str:
        """èªè¨€æª¢æ¸¬"""
        try:
            # ç°¡å–®çš„èªè¨€æª¢æ¸¬ï¼ˆåŸºæ–¼å­—ç¬¦ç‰¹å¾µï¼‰
            chinese_chars = len([c for c in text if "\u4e00" <= c <= "\u9fff"])
            english_chars = len([c for c in text if c.isalpha() and ord(c) < 128])
            total_chars = len([c for c in text if c.isalnum()])

            if total_chars == 0:
                return "unknown"

            chinese_ratio = chinese_chars / total_chars
            english_ratio = english_chars / total_chars

            if chinese_ratio > 0.3:
                return "zh-TW"  # ç¹é«”ä¸­æ–‡
            elif english_ratio > 0.7:
                return "en"  # è‹±èª
            elif chinese_ratio > 0.1:
                return "zh-CN"  # ç°¡é«”ä¸­æ–‡
            else:
                return "mixed"  # æ··åˆèªè¨€

        except Exception as e:
            logger.error(f"âŒ èªè¨€æª¢æ¸¬å¤±æ•—: {e}")
            return "unknown"

    # ========== é—œéµè©æå– ==========

    async def _extract_keywords(self, text: str) -> List[str]:
        """é—œéµè©æå–"""
        try:
            # ç°¡å–®çš„é—œéµè©æå–
            words = re.findall(r"\b\w+\b", text.lower())

            # åœç”¨è©
            stop_words = {
                "çš„",
                "æ˜¯",
                "åœ¨",
                "äº†",
                "å’Œ",
                "æœ‰",
                "æˆ‘",
                "ä½ ",
                "ä»–",
                "å¥¹",
                "å®ƒ",
                "é€™",
                "é‚£",
                "ä¸€",
                "äºŒ",
                "ä¸‰",
                "æœƒ",
                "ä¸",
                "ä¹Ÿ",
                "éƒ½",
                "å°±",
                "the",
                "a",
                "an",
                "and",
                "or",
                "but",
                "in",
                "on",
                "at",
                "to",
                "for",
                "of",
                "with",
                "by",
                "is",
                "are",
                "was",
                "were",
                "be",
            }

            # éæ¿¾åœç”¨è©å’ŒçŸ­è©
            keywords = [word for word in words if len(word) > 2 and word not in stop_words]

            # è¨ˆç®—è©é »
            word_freq = {}
            for word in keywords:
                word_freq[word] = word_freq.get(word, 0) + 1

            # æŒ‰é »ç‡æ’åºä¸¦è¿”å›å‰10å€‹
            sorted_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)

            return [word for word, freq in sorted_keywords[:10]]

        except Exception as e:
            logger.error(f"âŒ é—œéµè©æå–å¤±æ•—: {e}")
            return []

    # ========== é€£çµå®‰å…¨åˆ†æ ==========

    async def _analyze_links(self, text: str) -> List[LinkAnalysisResult]:
        """é€£çµå®‰å…¨åˆ†æ"""
        try:
            # æå–URL
            url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
            urls = re.findall(url_pattern, text)

            if not urls:
                return []

            results = []

            for url in urls:
                try:
                    result = await self._analyze_single_link(url)
                    results.append(result)
                except Exception as e:
                    logger.error(f"âŒ åˆ†æé€£çµå¤±æ•— {url}: {e}")
                    results.append(
                        LinkAnalysisResult(
                            url=url,
                            is_safe=False,
                            risk_level=ContentRiskLevel.MEDIUM,
                            risk_factors=["åˆ†æå¤±æ•—"],
                        )
                    )

            return results

        except Exception as e:
            logger.error(f"âŒ é€£çµåˆ†æå¤±æ•—: {e}")
            return []

    async def _analyze_single_link(self, url: str) -> LinkAnalysisResult:
        """åˆ†æå–®å€‹é€£çµ"""
        try:
            parsed_url = urllib.parse.urlparse(url)
            domain = parsed_url.netloc.lower()

            risk_factors = []
            risk_level = ContentRiskLevel.SAFE
            is_safe = True
            domain_reputation = 1.0

            # æª¢æŸ¥æ˜¯å¦ç‚ºçŸ­ç¶²å€
            is_shortened = any(shortener in domain for shortener in self.url_shorteners)
            if is_shortened:
                risk_factors.append("çŸ­ç¶²å€")
                risk_level = ContentRiskLevel.LOW

            # æª¢æŸ¥å±éšªåŸŸå
            if domain in self.dangerous_domains:
                risk_factors.append("å·²çŸ¥æƒ¡æ„åŸŸå")
                risk_level = ContentRiskLevel.CRITICAL
                is_safe = False
                domain_reputation = 0.0

            # æª¢æŸ¥å¯ç–‘ç‰¹å¾µ
            if re.search(r"[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}", domain):
                risk_factors.append("IPåœ°å€")
                risk_level = ContentRiskLevel.MEDIUM
                domain_reputation *= 0.7

            if len(domain) > 50:
                risk_factors.append("åŸŸåéé•·")
                risk_level = ContentRiskLevel.LOW
                domain_reputation *= 0.8

            # æª¢æŸ¥å¯ç–‘å­åŸŸå
            suspicious_subdomains = ["secure", "login", "verify", "update", "confirm"]
            for subdomain in suspicious_subdomains:
                if subdomain in domain:
                    risk_factors.append("å¯ç–‘å­åŸŸå")
                    risk_level = ContentRiskLevel.MEDIUM
                    domain_reputation *= 0.6
                    break

            # å˜—è©¦å±•é–‹çŸ­ç¶²å€ï¼ˆæ¨¡æ“¬ï¼‰
            expanded_url = None
            if is_shortened:
                expanded_url = await self._expand_url(url)

            return LinkAnalysisResult(
                url=url,
                is_safe=is_safe,
                risk_level=risk_level,
                risk_factors=risk_factors,
                domain_reputation=domain_reputation,
                is_shortened=is_shortened,
                expanded_url=expanded_url,
            )

        except Exception as e:
            logger.error(f"âŒ åˆ†æé€£çµå¤±æ•—: {e}")
            return LinkAnalysisResult(
                url=url,
                is_safe=False,
                risk_level=ContentRiskLevel.MEDIUM,
                risk_factors=["åˆ†æéŒ¯èª¤"],
            )

    async def _expand_url(self, url: str) -> Optional[str]:
        """å±•é–‹çŸ­ç¶²å€"""
        try:
            # æ¨¡æ“¬çŸ­ç¶²å€å±•é–‹ï¼ˆå¯¦éš›å¯¦ç¾éœ€è¦HTTPè«‹æ±‚ï¼‰
            # é€™è£¡è¿”å›æ¨¡æ“¬çµæœ
            return f"https://expanded-{url.split('//')[1]}.example.com"

        except Exception as e:
            logger.error(f"âŒ å±•é–‹URLå¤±æ•—: {e}")
            return None

    # ========== é¢¨éšªè©•ä¼° ==========

    async def _calculate_risk_level(self, result: ContentAnalysisResult) -> ContentRiskLevel:
        """è¨ˆç®—æ•´é«”é¢¨éšªç­‰ç´š"""
        try:
            risk_score = 0.0

            # æ¯’æ€§åˆ†æé¢¨éšª
            if result.toxicity:
                risk_score += result.toxicity.toxicity_score * 0.4

            # æƒ…æ„Ÿåˆ†æé¢¨éšªï¼ˆæ¥µç«¯è² é¢æƒ…æ„Ÿå¯èƒ½æœ‰é¢¨éšªï¼‰
            if result.sentiment and result.sentiment.sentiment == SentimentType.NEGATIVE:
                risk_score += result.sentiment.negative_score * 0.2

            # é€£çµé¢¨éšª
            if result.links:
                link_risk = max(
                    [self._risk_level_to_score(link.risk_level) for link in result.links]
                )
                risk_score += link_risk * 0.3

            # å…¶ä»–å› ç´ 
            if result.keywords:
                # æª¢æŸ¥é—œéµè©ä¸­æ˜¯å¦æœ‰é¢¨éšªè©å½™
                risk_keywords = ["ç—…æ¯’", "é§­å®¢", "è©é¨™", "é‡£é­š", "æƒ¡æ„"]
                for keyword in result.keywords:
                    if keyword in risk_keywords:
                        risk_score += 0.1

            # è½‰æ›ç‚ºé¢¨éšªç­‰ç´š
            if risk_score < 0.2:
                return ContentRiskLevel.SAFE
            elif risk_score < 0.4:
                return ContentRiskLevel.LOW
            elif risk_score < 0.6:
                return ContentRiskLevel.MEDIUM
            elif risk_score < 0.8:
                return ContentRiskLevel.HIGH
            else:
                return ContentRiskLevel.CRITICAL

        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—é¢¨éšªç­‰ç´šå¤±æ•—: {e}")
            return ContentRiskLevel.SAFE

    def _risk_level_to_score(self, risk_level: ContentRiskLevel) -> float:
        """é¢¨éšªç­‰ç´šè½‰åˆ†æ•¸"""
        mapping = {
            ContentRiskLevel.SAFE: 0.0,
            ContentRiskLevel.LOW: 0.2,
            ContentRiskLevel.MEDIUM: 0.5,
            ContentRiskLevel.HIGH: 0.7,
            ContentRiskLevel.CRITICAL: 1.0,
        }
        return mapping.get(risk_level, 0.0)

    async def _calculate_confidence(self, result: ContentAnalysisResult) -> float:
        """è¨ˆç®—åˆ†æä¿¡å¿ƒåº¦"""
        try:
            confidence_scores = []

            if result.sentiment:
                confidence_scores.append(result.sentiment.confidence)

            if result.toxicity:
                # æ¯’æ€§åˆ†æçš„ä¿¡å¿ƒåº¦åŸºæ–¼æª¢æ¸¬åˆ°çš„é—œéµè©æ•¸é‡
                keyword_confidence = min(1.0, len(result.toxicity.flagged_phrases) * 0.2 + 0.3)
                confidence_scores.append(keyword_confidence)

            if result.language:
                # èªè¨€æª¢æ¸¬ä¿¡å¿ƒåº¦ï¼ˆç°¡åŒ–ï¼‰
                confidence_scores.append(0.8 if result.language != "unknown" else 0.3)

            if confidence_scores:
                return sum(confidence_scores) / len(confidence_scores)
            else:
                return 0.5

        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—ä¿¡å¿ƒåº¦å¤±æ•—: {e}")
            return 0.0

    # ========== çµ±è¨ˆåˆ†æ ==========

    async def get_content_statistics(self, guild_id: int, days: int = 7) -> Dict[str, Any]:
        """ç²å–å…§å®¹çµ±è¨ˆ"""
        try:
            cache_key = f"content_stats:{guild_id}:{days}"
            cached_stats = await cache_manager.get(cache_key)
            if cached_stats:
                return cached_stats

            # æ¨¡æ“¬çµ±è¨ˆæ•¸æ“šï¼ˆå¯¦éš›å¯¦ç¾éœ€è¦æ•¸æ“šåº«æŸ¥è©¢ï¼‰
            stats = {
                "total_messages": 1250,
                "sentiment_distribution": {
                    "positive": 45.2,
                    "negative": 12.8,
                    "neutral": 35.5,
                    "mixed": 6.5,
                },
                "toxicity_stats": {
                    "toxic_messages": 23,
                    "toxicity_rate": 1.84,
                    "most_common_issues": ["spam", "harassment", "inappropriate"],
                },
                "language_distribution": {"zh-TW": 67.3, "en": 28.7, "mixed": 4.0},
                "link_analysis": {
                    "total_links": 156,
                    "safe_links": 142,
                    "risky_links": 14,
                    "blocked_links": 3,
                },
                "top_keywords": [
                    "éŠæˆ²",
                    "éŸ³æ¨‚",
                    "é›»å½±",
                    "å­¸ç¿’",
                    "å·¥ä½œ",
                    "æœ‹å‹",
                    "è¨è«–",
                    "åˆ†äº«",
                    "æ¨è–¦",
                    "å•é¡Œ",
                ],
            }

            # å¿«å–30åˆ†é˜
            await cache_manager.set(cache_key, stats, 1800)

            return stats

        except Exception as e:
            logger.error(f"âŒ ç²å–å…§å®¹çµ±è¨ˆå¤±æ•—: {e}")
            return {}


# å…¨åŸŸå¯¦ä¾‹
content_analyzer = ContentAnalyzer()
