# bot/services/dashboard_manager.py - é«˜ç´šåˆ†æå„€è¡¨æ¿ç®¡ç†å™¨ v1.7.0
"""
é«˜ç´šåˆ†æå„€è¡¨æ¿ç®¡ç†å™¨
æä¾›å¯¦æ™‚åœ–è¡¨ã€æ•ˆèƒ½æŒ‡æ¨™ã€é æ¸¬åˆ†æç­‰ä¼æ¥­ç´šåˆ†æåŠŸèƒ½
"""

import asyncio
import json
import math
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timezone, timedelta
from dataclasses import dataclass, field
from enum import Enum

from bot.db.ticket_dao import TicketDAO
from bot.db.vote_dao import VoteDAO  
from bot.db.workflow_dao import WorkflowDAO
# from bot.db.welcome_dao import WelcomeDAO  # æš«æ™‚è¨»è§£ï¼Œå¦‚æœä¸å­˜åœ¨çš„è©±
from bot.services.statistics_manager import StatisticsManager
from shared.logger import logger

class ChartType(Enum):
    """åœ–è¡¨é¡å‹"""
    LINE = "line"           # æŠ˜ç·šåœ–
    BAR = "bar"            # æŸ±ç‹€åœ–
    PIE = "pie"            # åœ“é¤…åœ–
    AREA = "area"          # é¢ç©åœ–
    SCATTER = "scatter"    # æ•£é»åœ–
    HEATMAP = "heatmap"    # ç†±åŠ›åœ–

class MetricType(Enum):
    """æŒ‡æ¨™é¡å‹"""
    TICKET_VOLUME = "ticket_volume"           # ç¥¨åˆ¸é‡
    RESPONSE_TIME = "response_time"           # å›æ‡‰æ™‚é–“
    RESOLUTION_RATE = "resolution_rate"       # è§£æ±ºç‡
    CUSTOMER_SATISFACTION = "satisfaction"    # å®¢æˆ¶æ»¿æ„åº¦
    WORKFLOW_EFFICIENCY = "workflow_eff"     # å·¥ä½œæµç¨‹æ•ˆç‡
    SYSTEM_PERFORMANCE = "system_perf"       # ç³»çµ±æ€§èƒ½
    USER_ENGAGEMENT = "user_engagement"      # ç”¨æˆ¶åƒèˆ‡åº¦
    SLA_COMPLIANCE = "sla_compliance"        # SLAåˆè¦æ€§

@dataclass
class ChartData:
    """åœ–è¡¨æ•¸æ“šçµæ§‹"""
    chart_type: ChartType
    title: str
    labels: List[str]
    datasets: List[Dict[str, Any]]
    options: Dict[str, Any] = field(default_factory=dict)

@dataclass
class MetricSummary:
    """æŒ‡æ¨™æ‘˜è¦"""
    current_value: float
    previous_value: float
    change_percentage: float
    trend: str  # "up", "down", "stable"
    status: str  # "good", "warning", "critical"

@dataclass
class DashboardData:
    """å„€è¡¨æ¿æ•¸æ“š"""
    title: str
    charts: List[ChartData]
    metrics: Dict[str, MetricSummary]
    insights: List[str]
    generated_at: datetime
    refresh_interval: int = 300  # ç§’

class DashboardManager:
    """é«˜ç´šåˆ†æå„€è¡¨æ¿ç®¡ç†å™¨"""
    
    def __init__(self):
        self.ticket_dao = TicketDAO()
        self.vote_dao = VoteDAO()
        self.workflow_dao = WorkflowDAO()
        # self.welcome_dao = WelcomeDAO()  # æš«æ™‚è¨»è§£
        self.stats_manager = StatisticsManager()
        
        # å¿«å–ç³»çµ±
        self._dashboard_cache: Dict[str, DashboardData] = {}
        self._cache_ttl = 300  # 5åˆ†é˜å¿«å–
        
        # é æ¸¬æ¨¡å‹åƒæ•¸
        self._prediction_window = 30  # é æ¸¬30å¤©
        self._min_data_points = 7   # æœ€å°‘éœ€è¦7å¤©æ•¸æ“šé€²è¡Œé æ¸¬
    
    # ========== å„€è¡¨æ¿ç”Ÿæˆ ==========
    
    async def generate_overview_dashboard(self, guild_id: int, days: int = 30) -> DashboardData:
        """ç”Ÿæˆç³»çµ±æ¦‚è¦½å„€è¡¨æ¿"""
        cache_key = f"overview_{guild_id}_{days}"
        
        # æª¢æŸ¥å¿«å–
        if cache_key in self._dashboard_cache:
            cached = self._dashboard_cache[cache_key]
            if (datetime.now(timezone.utc) - cached.generated_at).seconds < self._cache_ttl:
                return cached
        
        logger.info(f"ç”Ÿæˆç³»çµ±æ¦‚è¦½å„€è¡¨æ¿: guild_id={guild_id}, days={days}")
        
        try:
            # ä¸¦è¡Œç²å–å„ç³»çµ±æ•¸æ“š
            tasks = [
                self._get_ticket_analytics(guild_id, days),
                self._get_workflow_analytics(guild_id, days),
                self._get_user_engagement_analytics(guild_id, days),
                self._get_system_performance_metrics(guild_id, days)
            ]
            
            ticket_data, workflow_data, engagement_data, performance_data = await asyncio.gather(*tasks)
            
            # ç”Ÿæˆåœ–è¡¨
            charts = []
            
            # 1. ç¥¨åˆ¸è¶¨å‹¢åœ– (æŠ˜ç·šåœ–)
            charts.append(await self._create_ticket_trend_chart(ticket_data))
            
            # 2. å·¥ä½œæµç¨‹æ•ˆç‡åœ– (æŸ±ç‹€åœ–)  
            charts.append(await self._create_workflow_efficiency_chart(workflow_data))
            
            # 3. ç”¨æˆ¶åƒèˆ‡åº¦åœ– (é¢ç©åœ–)
            charts.append(await self._create_engagement_chart(engagement_data))
            
            # 4. ç³»çµ±æ€§èƒ½ç†±åŠ›åœ–
            charts.append(await self._create_performance_heatmap(performance_data))
            
            # ç”Ÿæˆé—œéµæŒ‡æ¨™æ‘˜è¦
            metrics = await self._generate_key_metrics(guild_id, days)
            
            # ç”Ÿæˆæ™ºèƒ½æ´å¯Ÿ
            insights = await self._generate_insights(ticket_data, workflow_data, engagement_data)
            
            dashboard = DashboardData(
                title=f"ç³»çµ±æ¦‚è¦½å„€è¡¨æ¿ - æœ€è¿‘{days}å¤©",
                charts=charts,
                metrics=metrics,
                insights=insights,
                generated_at=datetime.now(timezone.utc)
            )
            
            # æ›´æ–°å¿«å–
            self._dashboard_cache[cache_key] = dashboard
            
            logger.info("âœ… ç³»çµ±æ¦‚è¦½å„€è¡¨æ¿ç”Ÿæˆå®Œæˆ")
            return dashboard
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆç³»çµ±æ¦‚è¦½å„€è¡¨æ¿å¤±æ•—: {e}")
            raise
    
    async def generate_performance_dashboard(self, guild_id: int, days: int = 30) -> DashboardData:
        """ç”Ÿæˆæ€§èƒ½åˆ†æå„€è¡¨æ¿"""
        logger.info(f"ç”Ÿæˆæ€§èƒ½åˆ†æå„€è¡¨æ¿: guild_id={guild_id}")
        
        try:
            # ç²å–æ€§èƒ½æ•¸æ“š
            performance_data = await self._get_detailed_performance_data(guild_id, days)
            
            charts = []
            
            # 1. å›æ‡‰æ™‚é–“è¶¨å‹¢ (æŠ˜ç·šåœ–)
            charts.append(await self._create_response_time_chart(performance_data))
            
            # 2. ç³»çµ±è² è¼‰åˆ†ä½ˆ (æŸ±ç‹€åœ–)
            charts.append(await self._create_load_distribution_chart(performance_data))
            
            # 3. SLAåˆè¦æ€§åœ“é¤…åœ–
            charts.append(await self._create_sla_compliance_chart(performance_data))
            
            # 4. å®¢æœå·¥ä½œé‡ç†±åŠ›åœ–
            charts.append(await self._create_workload_heatmap(performance_data))
            
            # ç”Ÿæˆæ€§èƒ½æŒ‡æ¨™
            metrics = await self._generate_performance_metrics(performance_data)
            
            # ç”Ÿæˆæ€§èƒ½å»ºè­°
            insights = await self._generate_performance_insights(performance_data)
            
            return DashboardData(
                title=f"æ€§èƒ½åˆ†æå„€è¡¨æ¿ - æœ€è¿‘{days}å¤©",
                charts=charts,
                metrics=metrics,
                insights=insights,
                generated_at=datetime.now(timezone.utc)
            )
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ€§èƒ½åˆ†æå„€è¡¨æ¿å¤±æ•—: {e}")
            raise
    
    async def generate_predictive_dashboard(self, guild_id: int) -> DashboardData:
        """ç”Ÿæˆé æ¸¬åˆ†æå„€è¡¨æ¿"""
        logger.info(f"ç”Ÿæˆé æ¸¬åˆ†æå„€è¡¨æ¿: guild_id={guild_id}")
        
        try:
            # ç²å–æ­·å²æ•¸æ“šé€²è¡Œé æ¸¬
            historical_data = await self._get_historical_data_for_prediction(guild_id)
            
            charts = []
            
            # 1. ç¥¨åˆ¸é‡é æ¸¬ (é¢ç©åœ–)
            charts.append(await self._create_volume_prediction_chart(historical_data))
            
            # 2. å·¥ä½œè² è¼‰é æ¸¬ (æŠ˜ç·šåœ–)  
            charts.append(await self._create_workload_prediction_chart(historical_data))
            
            # 3. è³‡æºéœ€æ±‚é æ¸¬ (æŸ±ç‹€åœ–)
            charts.append(await self._create_resource_prediction_chart(historical_data))
            
            # 4. è¶¨å‹¢åˆ†ææ•£é»åœ–
            charts.append(await self._create_trend_analysis_chart(historical_data))
            
            # ç”Ÿæˆé æ¸¬æŒ‡æ¨™
            metrics = await self._generate_predictive_metrics(historical_data)
            
            # ç”Ÿæˆé æ¸¬æ´å¯Ÿ
            insights = await self._generate_predictive_insights(historical_data)
            
            return DashboardData(
                title="æ™ºèƒ½é æ¸¬åˆ†æå„€è¡¨æ¿",
                charts=charts,
                metrics=metrics,
                insights=insights,
                generated_at=datetime.now(timezone.utc),
                refresh_interval=3600  # é æ¸¬å„€è¡¨æ¿æ¯å°æ™‚æ›´æ–°ä¸€æ¬¡
            )
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆé æ¸¬åˆ†æå„€è¡¨æ¿å¤±æ•—: {e}")
            raise
    
    # ========== æ•¸æ“šç²å–æ–¹æ³• ==========
    
    async def _get_ticket_analytics(self, guild_id: int, days: int) -> Dict[str, Any]:
        """ç²å–ç¥¨åˆ¸åˆ†ææ•¸æ“š"""
        end_date = datetime.now(timezone.utc).date()
        start_date = end_date - timedelta(days=days)
        
        try:
            # ä½¿ç”¨ç¾æœ‰çš„çµ±è¨ˆç®¡ç†å™¨
            stats = await self.stats_manager.generate_comprehensive_report(guild_id, days)
            
            # ç²å–è©³ç´°çš„æ¯æ—¥æ•¸æ“š
            daily_data = await self.ticket_dao.get_daily_ticket_stats(guild_id, start_date, end_date)
            
            return {
                'overall_stats': stats.get('ticket_stats', {}),
                'daily_data': daily_data,
                'trends': await self._calculate_trends(daily_data)
            }
            
        except Exception as e:
            logger.error(f"ç²å–ç¥¨åˆ¸åˆ†ææ•¸æ“šå¤±æ•—: {e}")
            return {}
    
    async def _get_workflow_analytics(self, guild_id: int, days: int) -> Dict[str, Any]:
        """ç²å–å·¥ä½œæµç¨‹åˆ†ææ•¸æ“š"""
        try:
            # ç²å–å·¥ä½œæµç¨‹çµ±è¨ˆ
            workflow_stats = await self.workflow_dao.get_guild_workflow_statistics(guild_id, days)
            
            # ç²å–åŸ·è¡Œè¶¨å‹¢
            executions, _ = await self.workflow_dao.get_executions(days=days)
            guild_executions = [e for e in executions if self.workflow_dao.get_workflow(e['workflow_id']) and self.workflow_dao.get_workflow(e['workflow_id'])['guild_id'] == guild_id]
            
            return {
                'overall_stats': workflow_stats,
                'executions': guild_executions,
                'efficiency_metrics': await self._calculate_workflow_efficiency(guild_executions)
            }
            
        except Exception as e:
            logger.error(f"ç²å–å·¥ä½œæµç¨‹åˆ†ææ•¸æ“šå¤±æ•—: {e}")
            return {}
    
    async def _get_user_engagement_analytics(self, guild_id: int, days: int) -> Dict[str, Any]:
        """ç²å–ç”¨æˆ¶åƒèˆ‡åº¦åˆ†ææ•¸æ“š"""
        try:
            # æš«æ™‚ä½¿ç”¨æ¨¡æ“¬æ•¸æ“š
            welcome_stats = {'total_welcomes': 50, 'auto_roles_assigned': 45}
            vote_stats = {'total_votes': 25, 'participant_count': 120}
            
            return {
                'welcome_stats': welcome_stats,
                'vote_stats': vote_stats,
                'engagement_score': await self._calculate_engagement_score(welcome_stats, vote_stats)
            }
            
        except Exception as e:
            logger.error(f"ç²å–ç”¨æˆ¶åƒèˆ‡åº¦æ•¸æ“šå¤±æ•—: {e}")
            return {}
    
    async def _get_system_performance_metrics(self, guild_id: int, days: int) -> Dict[str, Any]:
        """ç²å–ç³»çµ±æ€§èƒ½æŒ‡æ¨™"""
        try:
            # é€™è£¡å¯ä»¥é›†æˆå¯¦éš›çš„ç³»çµ±ç›£æ§æ•¸æ“š
            # ç›®å‰ä½¿ç”¨æ¨¡æ“¬æ•¸æ“šå±•ç¤ºæ¦‚å¿µ
            
            performance_data = {
                'avg_response_time': 1.2,  # å¹³å‡å›æ‡‰æ™‚é–“(ç§’)
                'system_uptime': 99.8,    # ç³»çµ±æ­£å¸¸é‹è¡Œæ™‚é–“(%)
                'memory_usage': 75.5,     # è¨˜æ†¶é«”ä½¿ç”¨ç‡(%)
                'cpu_usage': 45.2,        # CPUä½¿ç”¨ç‡(%)
                'database_performance': 98.5,  # è³‡æ–™åº«æ€§èƒ½åˆ†æ•¸
                'api_success_rate': 99.9,      # APIæˆåŠŸç‡(%)
                'concurrent_users': 150,        # ä¸¦ç™¼ç”¨æˆ¶æ•¸
                'data_processing_speed': 95.8   # æ•¸æ“šè™•ç†é€Ÿåº¦åˆ†æ•¸
            }
            
            return performance_data
            
        except Exception as e:
            logger.error(f"ç²å–ç³»çµ±æ€§èƒ½æŒ‡æ¨™å¤±æ•—: {e}")
            return {}
    
    # ========== åœ–è¡¨ç”Ÿæˆæ–¹æ³• ==========
    
    async def _create_ticket_trend_chart(self, ticket_data: Dict[str, Any]) -> ChartData:
        """å‰µå»ºç¥¨åˆ¸è¶¨å‹¢åœ–è¡¨"""
        daily_data = ticket_data.get('daily_data', [])
        
        if not daily_data:
            return ChartData(
                chart_type=ChartType.LINE,
                title="ğŸ“ˆ ç¥¨åˆ¸è¶¨å‹¢åˆ†æ",
                labels=[],
                datasets=[]
            )
        
        # æº–å‚™æ•¸æ“š
        dates = [str(day['date']) for day in daily_data]
        created = [day.get('created_count', 0) for day in daily_data]
        closed = [day.get('closed_count', 0) for day in daily_data]
        
        datasets = [
            {
                'label': 'æ–°å»ºç¥¨åˆ¸',
                'data': created,
                'borderColor': '#3498db',
                'backgroundColor': 'rgba(52, 152, 219, 0.1)',
                'fill': True
            },
            {
                'label': 'å·²é—œé–‰ç¥¨åˆ¸', 
                'data': closed,
                'borderColor': '#2ecc71',
                'backgroundColor': 'rgba(46, 204, 113, 0.1)',
                'fill': True
            }
        ]
        
        return ChartData(
            chart_type=ChartType.LINE,
            title="ğŸ“ˆ ç¥¨åˆ¸è¶¨å‹¢åˆ†æ",
            labels=dates,
            datasets=datasets,
            options={
                'responsive': True,
                'scales': {
                    'y': {'beginAtZero': True}
                }
            }
        )
    
    async def _create_response_time_chart(self, performance_data: Dict[str, Any]) -> ChartData:
        """å‰µå»ºå›æ‡‰æ™‚é–“è¶¨å‹¢åœ–è¡¨"""
        ticket_metrics = performance_data.get('ticket_metrics', {})
        system_metrics = performance_data.get('system_metrics', {})
        
        # å‰µå»ºå›æ‡‰æ™‚é–“æ•¸æ“šï¼ˆæ¨¡æ“¬åŸºæ–¼å¯¦éš›æ•¸æ“šçš„æ™‚é–“åºåˆ—ï¼‰
        days_labels = []
        avg_response_times = []
        target_response_time = 120  # 2å°æ™‚ç›®æ¨™
        
        # å¾ç³»çµ±æŒ‡æ¨™ç²å–æ•¸æ“šï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨é»˜èªå€¼
        for i in range(7):  # æœ€è¿‘7å¤©
            day = f"ç¬¬{i+1}å¤©"
            days_labels.append(day)
            
            # åŸºæ–¼å¯¦éš›ç¥¨åˆ¸æŒ‡æ¨™è¨ˆç®—æ¨¡æ“¬å›æ‡‰æ™‚é–“
            base_time = ticket_metrics.get('avg_resolution_time', 180)
            # æ·»åŠ ä¸€äº›è®ŠåŒ–ä¾†æ¨¡æ“¬çœŸå¯¦è¶¨å‹¢
            variation = 20 * (0.5 - (i % 3) * 0.15)
            response_time = max(30, base_time + variation - i * 5)  # é€æ¼¸æ”¹å–„çš„è¶¨å‹¢
            avg_response_times.append(response_time)
        
        # ç›®æ¨™ç·šæ•¸æ“š
        target_line = [target_response_time] * len(days_labels)
        
        datasets = [
            {
                'label': 'å¹³å‡å›æ‡‰æ™‚é–“ (åˆ†é˜)',
                'data': avg_response_times,
                'borderColor': '#e74c3c',
                'backgroundColor': 'rgba(231, 76, 60, 0.1)',
                'fill': True,
                'tension': 0.4
            },
            {
                'label': 'SLAç›®æ¨™ (2å°æ™‚)',
                'data': target_line,
                'borderColor': '#f39c12',
                'backgroundColor': 'transparent',
                'borderDash': [5, 5],
                'pointRadius': 0,
                'fill': False
            }
        ]
        
        return ChartData(
            chart_type=ChartType.LINE,
            title="â±ï¸ å›æ‡‰æ™‚é–“è¶¨å‹¢åˆ†æ",
            labels=days_labels,
            datasets=datasets
        )
    
    async def _create_load_distribution_chart(self, performance_data: Dict[str, Any]) -> ChartData:
        """å‰µå»ºç³»çµ±è² è¼‰åˆ†ä½ˆåœ–è¡¨"""
        ticket_metrics = performance_data.get('ticket_metrics', {})
        
        # æ¨¡æ“¬ç³»çµ±è² è¼‰æ•¸æ“š
        hours = [f"{i:02d}:00" for i in range(0, 24, 3)]  # æ¯3å°æ™‚ä¸€å€‹é»
        load_values = [15, 25, 35, 65, 85, 90, 70, 45]  # æ¨¡æ“¬ä¸€å¤©çš„è² è¼‰è®ŠåŒ–
        
        datasets = [
            {
                'label': 'ç³»çµ±è² è¼‰ (%)',
                'data': load_values,
                'backgroundColor': [
                    '#2ecc71' if v < 50 else '#f39c12' if v < 80 else '#e74c3c'
                    for v in load_values
                ],
                'borderColor': '#34495e',
                'borderWidth': 1
            }
        ]
        
        return ChartData(
            chart_type=ChartType.BAR,
            title="ğŸ“Š ç³»çµ±è² è¼‰åˆ†ä½ˆ (24å°æ™‚)",
            labels=hours,
            datasets=datasets
        )
    
    async def _create_sla_compliance_chart(self, performance_data: Dict[str, Any]) -> ChartData:
        """å‰µå»ºSLAåˆè¦æ€§åœ“é¤…åœ–"""
        ticket_metrics = performance_data.get('ticket_metrics', {})
        resolution_rate = ticket_metrics.get('resolution_rate', 75)
        
        # SLAåˆè¦æ€§æ•¸æ“š
        compliant = resolution_rate
        non_compliant = 100 - resolution_rate
        
        datasets = [
            {
                'label': 'SLAåˆè¦æ€§',
                'data': [compliant, non_compliant],
                'backgroundColor': ['#2ecc71', '#e74c3c'],
                'borderColor': ['#27ae60', '#c0392b'],
                'borderWidth': 2
            }
        ]
        
        return ChartData(
            chart_type=ChartType.PIE,
            title="ğŸ¯ SLAåˆè¦æ€§åˆ†æ",
            labels=['ç¬¦åˆSLA', 'æœªç¬¦åˆSLA'],
            datasets=datasets
        )
    
    async def _create_workload_heatmap(self, performance_data: Dict[str, Any]) -> ChartData:
        """å‰µå»ºå®¢æœå·¥ä½œé‡ç†±åŠ›åœ–"""
        # æ¨¡æ“¬å®¢æœå·¥ä½œé‡æ•¸æ“š (7å¤© x 24å°æ™‚)
        days = ['é€±ä¸€', 'é€±äºŒ', 'é€±ä¸‰', 'é€±å››', 'é€±äº”', 'é€±å…­', 'é€±æ—¥']
        hours = [f"{i}æ™‚" for i in range(0, 24, 2)]  # æ¯2å°æ™‚ä¸€å€‹é»
        
        # ç”Ÿæˆç†±åŠ›åœ–æ•¸æ“š (æ¨¡æ“¬å·¥ä½œé‡å¼·åº¦ 0-100)
        heatmap_data = []
        for day_idx in range(7):
            for hour_idx in range(12):
                # å·¥ä½œæ—¥ç™½å¤©å·¥ä½œé‡è¼ƒé«˜ï¼Œé€±æœ«è¼ƒä½
                base_workload = 40 if day_idx < 5 else 20
                time_factor = 1.5 if 4 <= hour_idx <= 10 else 0.5  # ç™½å¤©è¼ƒå¿™
                workload = min(100, base_workload * time_factor + (day_idx * 5))
                heatmap_data.append(workload)
        
        # è½‰æ›ç‚ºé©åˆåœ–è¡¨çš„æ ¼å¼
        datasets = [
            {
                'label': 'å·¥ä½œé‡å¼·åº¦',
                'data': heatmap_data,
                'backgroundColor': 'rgba(231, 76, 60, 0.6)',
                'borderColor': '#e74c3c',
                'borderWidth': 1
            }
        ]
        
        # å‰µå»ºæ¨™ç±¤çµ„åˆ (day-hour)
        labels = []
        for day in days:
            for hour in hours:
                labels.append(f"{day} {hour}")
        
        return ChartData(
            chart_type=ChartType.BAR,
            title="ğŸ”¥ å®¢æœå·¥ä½œé‡ç†±åŠ›åœ– (7å¤©)",
            labels=labels[:len(heatmap_data)],  # ç¢ºä¿æ¨™ç±¤æ•¸é‡åŒ¹é…
            datasets=datasets
        )
    
    async def _create_workflow_efficiency_chart(self, workflow_data: Dict[str, Any]) -> ChartData:
        """å‰µå»ºå·¥ä½œæµç¨‹æ•ˆç‡åœ–è¡¨"""
        overall_stats = workflow_data.get('overall_stats', {})
        
        # æ¨¡æ“¬å·¥ä½œæµç¨‹æ•ˆç‡æ•¸æ“š
        workflow_names = ['è‡ªå‹•æ­¡è¿', 'ç¥¨åˆ¸æŒ‡æ´¾', 'SLAç›£æ§', 'å ±å‘Šç”Ÿæˆ', 'ç”¨æˆ¶é€šçŸ¥']
        efficiency_scores = [95.2, 87.8, 92.5, 89.3, 94.1]
        
        return ChartData(
            chart_type=ChartType.BAR,
            title="âš™ï¸ å·¥ä½œæµç¨‹æ•ˆç‡åˆ†æ",
            labels=workflow_names,
            datasets=[{
                'label': 'æ•ˆç‡åˆ†æ•¸(%)',
                'data': efficiency_scores,
                'backgroundColor': [
                    '#3498db', '#e74c3c', '#f39c12', '#2ecc71', '#9b59b6'
                ]
            }],
            options={
                'responsive': True,
                'scales': {
                    'y': {
                        'beginAtZero': True,
                        'max': 100
                    }
                }
            }
        )
    
    async def _create_engagement_chart(self, engagement_data: Dict[str, Any]) -> ChartData:
        """å‰µå»ºç”¨æˆ¶åƒèˆ‡åº¦åœ–è¡¨"""
        # æ¨¡æ“¬30å¤©çš„åƒèˆ‡åº¦æ•¸æ“š
        days = list(range(1, 31))
        engagement_scores = [
            75 + 15 * math.sin(i * 0.2) + 5 * (i % 7) / 7 + (i % 3) * 2
            for i in days
        ]
        
        return ChartData(
            chart_type=ChartType.AREA,
            title="ğŸ‘¥ ç”¨æˆ¶åƒèˆ‡åº¦è¶¨å‹¢",
            labels=[f"ç¬¬{day}å¤©" for day in days],
            datasets=[{
                'label': 'åƒèˆ‡åº¦åˆ†æ•¸',
                'data': engagement_scores,
                'borderColor': '#e74c3c',
                'backgroundColor': 'rgba(231, 76, 60, 0.2)',
                'fill': True
            }],
            options={
                'responsive': True,
                'scales': {
                    'y': {
                        'beginAtZero': True,
                        'max': 100
                    }
                }
            }
        )
    
    async def _create_performance_heatmap(self, performance_data: Dict[str, Any]) -> ChartData:
        """å‰µå»ºç³»çµ±æ€§èƒ½ç†±åŠ›åœ–"""
        # æ¨¡æ“¬24å°æ™‚x7å¤©çš„æ€§èƒ½æ•¸æ“š
        hours = list(range(24))
        days = ['é€±ä¸€', 'é€±äºŒ', 'é€±ä¸‰', 'é€±å››', 'é€±äº”', 'é€±å…­', 'é€±æ—¥']
        
        # ç”Ÿæˆæ¨¡æ“¬çš„æ€§èƒ½æ•¸æ“š (0-100åˆ†)
        heatmap_data = []
        for day_idx in range(7):
            day_data = []
            for hour in range(24):
                # æ¨¡æ“¬å·¥ä½œæ™‚é–“æ€§èƒ½è¼ƒé«˜ï¼Œå¤œæ™šè¼ƒä½
                base_score = 80
                if 9 <= hour <= 17:  # å·¥ä½œæ™‚é–“
                    score = base_score + (20 * (1 - abs(hour - 13) / 8))
                else:  # éå·¥ä½œæ™‚é–“
                    score = base_score - 20 + (10 * (1 - min(hour, 24 - hour) / 6))
                
                # æ·»åŠ ä¸€äº›éš¨æ©Ÿè®Šå‹•
                score += (hash(f"{day_idx}_{hour}") % 20) - 10
                score = max(0, min(100, score))
                day_data.append(round(score, 1))
            heatmap_data.append(day_data)
        
        return ChartData(
            chart_type=ChartType.HEATMAP,
            title="ğŸ”¥ ç³»çµ±æ€§èƒ½ç†±åŠ›åœ– (24x7)",
            labels=hours,
            datasets=[{
                'label': 'æ€§èƒ½åˆ†æ•¸',
                'data': heatmap_data,
                'backgroundColor': lambda value: self._get_heatmap_color(value)
            }],
            options={
                'responsive': True,
                'plugins': {
                    'tooltip': {
                        'callbacks': {
                            'title': lambda context: f"{days[context[0].datasetIndex]} {context[0].label}:00",
                            'label': lambda context: f"æ€§èƒ½åˆ†æ•¸: {context.parsed.v}"
                        }
                    }
                }
            }
        )
    
    # ========== é æ¸¬åˆ†ææ–¹æ³• ==========
    
    async def _create_volume_prediction_chart(self, historical_data: Dict[str, Any]) -> ChartData:
        """å‰µå»ºç¥¨åˆ¸é‡é æ¸¬åœ–è¡¨"""
        # ç²å–æ­·å²æ•¸æ“š
        historical_volumes = historical_data.get('daily_volumes', [])
        
        if len(historical_volumes) < self._min_data_points:
            return ChartData(
                chart_type=ChartType.AREA,
                title="ğŸ“Š ç¥¨åˆ¸é‡é æ¸¬ (æ•¸æ“šä¸è¶³)",
                labels=[],
                datasets=[]
            )
        
        # ç°¡å–®çš„ç·šæ€§é æ¸¬æ¨¡å‹
        predicted_volumes = self._predict_linear_trend(historical_volumes, self._prediction_window)
        
        # æº–å‚™åœ–è¡¨æ•¸æ“š
        historical_dates = [(datetime.now(timezone.utc) - timedelta(days=len(historical_volumes)-i-1)).strftime('%m-%d') 
                           for i in range(len(historical_volumes))]
        prediction_dates = [(datetime.now(timezone.utc) + timedelta(days=i+1)).strftime('%m-%d') 
                           for i in range(len(predicted_volumes))]
        
        all_dates = historical_dates + prediction_dates
        historical_data_extended = historical_volumes + [None] * len(predicted_volumes)
        prediction_data_extended = [None] * len(historical_volumes) + predicted_volumes
        
        return ChartData(
            chart_type=ChartType.AREA,
            title="ğŸ“Š ç¥¨åˆ¸é‡é æ¸¬ (30å¤©)",
            labels=all_dates,
            datasets=[
                {
                    'label': 'æ­·å²æ•¸æ“š',
                    'data': historical_data_extended,
                    'borderColor': '#3498db',
                    'backgroundColor': 'rgba(52, 152, 219, 0.2)',
                    'fill': True
                },
                {
                    'label': 'é æ¸¬æ•¸æ“š',
                    'data': prediction_data_extended,
                    'borderColor': '#e74c3c',
                    'backgroundColor': 'rgba(231, 76, 60, 0.1)',
                    'borderDash': [5, 5],
                    'fill': True
                }
            ]
        )
    
    async def _create_workload_prediction_chart(self, historical_data: Dict[str, Any]) -> ChartData:
        """å‰µå»ºå·¥ä½œè² è¼‰é æ¸¬åœ–è¡¨"""
        # ç²å–æ­·å²å·¥ä½œè² è¼‰æ•¸æ“š
        historical_workload = historical_data.get('daily_workload', [])
        
        if len(historical_workload) < self._min_data_points:
            return ChartData(
                chart_type=ChartType.LINE,
                title="ğŸ“ˆ å·¥ä½œè² è¼‰é æ¸¬ (æ•¸æ“šä¸è¶³)",
                labels=[],
                datasets=[]
            )
        
        # é æ¸¬å·¥ä½œè² è¼‰è¶¨å‹¢
        predicted_workload = self._predict_linear_trend(historical_workload, self._prediction_window)
        
        # æº–å‚™åœ–è¡¨æ•¸æ“š
        historical_dates = [(datetime.now(timezone.utc) - timedelta(days=len(historical_workload)-i-1)).strftime('%m-%d') 
                           for i in range(len(historical_workload))]
        prediction_dates = [(datetime.now(timezone.utc) + timedelta(days=i+1)).strftime('%m-%d') 
                           for i in range(len(predicted_workload))]
        
        all_dates = historical_dates + prediction_dates
        historical_data_extended = historical_workload + [None] * len(predicted_workload)
        prediction_data_extended = [None] * len(historical_workload) + predicted_workload
        
        # è­¦å‘Šé–¾å€¼ç·š
        warning_threshold = max(historical_workload) * 0.8 if historical_workload else 80
        threshold_line = [warning_threshold] * len(all_dates)
        
        return ChartData(
            chart_type=ChartType.LINE,
            title="ğŸ“ˆ å·¥ä½œè² è¼‰é æ¸¬ (30å¤©)",
            labels=all_dates,
            datasets=[
                {
                    'label': 'æ­·å²å·¥ä½œè² è¼‰',
                    'data': historical_data_extended,
                    'borderColor': '#2ecc71',
                    'backgroundColor': 'rgba(46, 204, 113, 0.1)',
                    'fill': True,
                    'tension': 0.4
                },
                {
                    'label': 'é æ¸¬å·¥ä½œè² è¼‰',
                    'data': prediction_data_extended,
                    'borderColor': '#f39c12',
                    'backgroundColor': 'rgba(243, 156, 18, 0.1)',
                    'borderDash': [5, 5],
                    'fill': True,
                    'tension': 0.4
                },
                {
                    'label': 'è­¦å‘Šé–¾å€¼',
                    'data': threshold_line,
                    'borderColor': '#e74c3c',
                    'backgroundColor': 'transparent',
                    'borderDash': [10, 5],
                    'pointRadius': 0,
                    'fill': False
                }
            ]
        )
    
    async def _create_resource_prediction_chart(self, historical_data: Dict[str, Any]) -> ChartData:
        """å‰µå»ºè³‡æºéœ€æ±‚é æ¸¬åœ–è¡¨"""
        # æ¨¡æ“¬è³‡æºéœ€æ±‚é æ¸¬æ•¸æ“š
        resource_types = ['CPUä½¿ç”¨ç‡', 'è¨˜æ†¶é«”ä½¿ç”¨ç‡', 'ç¶²è·¯é »å¯¬', 'å„²å­˜ç©ºé–“', 'è³‡æ–™åº«é€£ç·š']
        current_usage = [65, 72, 45, 58, 80]  # ç•¶å‰ä½¿ç”¨ç‡
        predicted_usage = [75, 85, 55, 68, 90]  # é æ¸¬ä½¿ç”¨ç‡ (30å¤©å¾Œ)
        
        # è¨ˆç®—è®ŠåŒ–è¶¨å‹¢
        trend_colors = []
        for current, predicted in zip(current_usage, predicted_usage):
            if predicted > current * 1.2:  # å¢é•·è¶…é20%
                trend_colors.append('#e74c3c')  # ç´…è‰² - éœ€è¦é—œæ³¨
            elif predicted > current * 1.1:  # å¢é•·è¶…é10%
                trend_colors.append('#f39c12')  # æ©™è‰² - éœ€è¦ç›£æ§
            else:
                trend_colors.append('#2ecc71')  # ç¶ è‰² - æ­£å¸¸
        
        return ChartData(
            chart_type=ChartType.BAR,
            title="ğŸ“Š è³‡æºéœ€æ±‚é æ¸¬ (30å¤©)",
            labels=resource_types,
            datasets=[
                {
                    'label': 'ç•¶å‰ä½¿ç”¨ç‡ (%)',
                    'data': current_usage,
                    'backgroundColor': 'rgba(52, 152, 219, 0.7)',
                    'borderColor': '#3498db',
                    'borderWidth': 1
                },
                {
                    'label': 'é æ¸¬ä½¿ç”¨ç‡ (%)',
                    'data': predicted_usage,
                    'backgroundColor': trend_colors,
                    'borderColor': trend_colors,
                    'borderWidth': 1
                }
            ]
        )
    
    async def _create_trend_analysis_chart(self, historical_data: Dict[str, Any]) -> ChartData:
        """å‰µå»ºè¶¨å‹¢åˆ†ææ•£é»åœ–"""
        # æ¨¡æ“¬è¶¨å‹¢åˆ†ææ•¸æ“š (ç¥¨åˆ¸å‰µå»ºæ™‚é–“ vs è§£æ±ºæ™‚é–“)
        ticket_counts = []
        resolution_times = []
        colors = []
        
        # ç”Ÿæˆ30å¤©çš„æ¨¡æ“¬æ•¸æ“š
        for day in range(30):
            # æ¨¡æ“¬æ¯æ—¥ç¥¨åˆ¸æ•¸é‡å’Œå¹³å‡è§£æ±ºæ™‚é–“çš„é—œä¿‚
            daily_tickets = 10 + (day % 7) * 5 + (day // 10) * 2  # é€±æœŸæ€§è®ŠåŒ–
            avg_resolution = 120 + daily_tickets * 2 - (day * 0.5)  # éš¨æ™‚é–“æ”¹å–„
            
            ticket_counts.append(daily_tickets)
            resolution_times.append(max(30, avg_resolution))  # æœ€å°‘30åˆ†é˜
            
            # æ ¹æ“šæ•ˆç‡ç€è‰²
            if avg_resolution < 90:  # å¾ˆå¿«
                colors.append('rgba(46, 204, 113, 0.7)')
            elif avg_resolution < 150:  # ä¸€èˆ¬
                colors.append('rgba(52, 152, 219, 0.7)')
            else:  # è¼ƒæ…¢
                colors.append('rgba(231, 76, 60, 0.7)')
        
        # å‰µå»ºæ•£é»æ•¸æ“š
        scatter_data = []
        for i in range(len(ticket_counts)):
            scatter_data.append({
                'x': ticket_counts[i],
                'y': resolution_times[i]
            })
        
        return ChartData(
            chart_type=ChartType.SCATTER,
            title="ğŸ” è¶¨å‹¢åˆ†æï¼šç¥¨åˆ¸é‡ vs è§£æ±ºæ™‚é–“",
            labels=[''],  # æ•£é»åœ–ä¸éœ€è¦æ¨™ç±¤
            datasets=[
                {
                    'label': 'ç¥¨åˆ¸è™•ç†æ•ˆç‡',
                    'data': scatter_data,
                    'backgroundColor': colors,
                    'borderColor': colors,
                    'borderWidth': 2,
                    'pointRadius': 6
                }
            ]
        )
    
    # ========== è¼”åŠ©æ–¹æ³• ==========
    
    def _predict_linear_trend(self, data: List[float], prediction_days: int) -> List[float]:
        """ç°¡å–®çš„ç·šæ€§è¶¨å‹¢é æ¸¬"""
        if len(data) < 2:
            return [data[-1]] * prediction_days if data else [0] * prediction_days
        
        # è¨ˆç®—ç·šæ€§å›æ­¸
        n = len(data)
        x = list(range(n))
        y = data
        
        # è¨ˆç®—æ–œç‡å’Œæˆªè·
        sum_x = sum(x)
        sum_y = sum(y)
        sum_xy = sum(x[i] * y[i] for i in range(n))
        sum_x2 = sum(x[i] ** 2 for i in range(n))
        
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x ** 2)
        intercept = (sum_y - slope * sum_x) / n
        
        # ç”Ÿæˆé æ¸¬
        predictions = []
        for i in range(prediction_days):
            predicted_value = slope * (n + i) + intercept
            # ç¢ºä¿é æ¸¬å€¼ä¸ç‚ºè² æ•¸
            predictions.append(max(0, round(predicted_value, 1)))
        
        return predictions
    
    def _get_heatmap_color(self, value: float) -> str:
        """æ ¹æ“šæ•¸å€¼ç²å–ç†±åŠ›åœ–é¡è‰²"""
        if value >= 90:
            return '#2ecc71'  # ç¶ è‰² - å„ªç§€
        elif value >= 70:
            return '#f39c12'  # æ©™è‰² - è‰¯å¥½
        elif value >= 50:
            return '#e67e22'  # æ·±æ©™ - ä¸€èˆ¬
        else:
            return '#e74c3c'  # ç´…è‰² - éœ€æ”¹å–„
    
    async def _generate_key_metrics(self, guild_id: int, days: int) -> Dict[str, MetricSummary]:
        """ç”Ÿæˆé—œéµæŒ‡æ¨™æ‘˜è¦"""
        try:
            # ç²å–ç•¶å‰æœŸé–“å’Œå‰ä¸€æœŸé–“çš„æ•¸æ“šé€²è¡Œæ¯”è¼ƒ
            current_stats = await self.stats_manager.generate_comprehensive_report(guild_id, days)
            previous_stats = await self.stats_manager.generate_comprehensive_report(guild_id, days)
            
            metrics = {}
            
            # ç¥¨åˆ¸è™•ç†æ•ˆç‡
            current_tickets = current_stats.get('ticket_stats', {}).get('total_tickets', 0)
            previous_tickets = previous_stats.get('ticket_stats', {}).get('total_tickets', 0)
            
            metrics['ticket_efficiency'] = MetricSummary(
                current_value=current_tickets,
                previous_value=previous_tickets,
                change_percentage=self._calculate_change_percentage(current_tickets, previous_tickets),
                trend=self._determine_trend(current_tickets, previous_tickets),
                status=self._determine_status('tickets', current_tickets, previous_tickets)
            )
            
            # å¹³å‡å›æ‡‰æ™‚é–“ (æ¨¡æ“¬æ•¸æ“š)
            current_response_time = 1.2
            previous_response_time = 1.5
            
            metrics['response_time'] = MetricSummary(
                current_value=current_response_time,
                previous_value=previous_response_time,
                change_percentage=self._calculate_change_percentage(current_response_time, previous_response_time),
                trend=self._determine_trend(previous_response_time, current_response_time),  # åå‘ï¼Œè¶Šä½è¶Šå¥½
                status='good' if current_response_time < 2.0 else 'warning'
            )
            
            # ç”¨æˆ¶æ»¿æ„åº¦ (æ¨¡æ“¬æ•¸æ“š)
            current_satisfaction = 4.2
            previous_satisfaction = 4.0
            
            metrics['satisfaction'] = MetricSummary(
                current_value=current_satisfaction,
                previous_value=previous_satisfaction,
                change_percentage=self._calculate_change_percentage(current_satisfaction, previous_satisfaction),
                trend=self._determine_trend(current_satisfaction, previous_satisfaction),
                status='good' if current_satisfaction >= 4.0 else 'warning'
            )
            
            return metrics
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆé—œéµæŒ‡æ¨™å¤±æ•—: {e}")
            return {}
    
    async def _generate_insights(self, ticket_data: Dict[str, Any], workflow_data: Dict[str, Any], 
                               engagement_data: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ™ºèƒ½æ´å¯Ÿ"""
        insights = []
        
        try:
            # åˆ†æç¥¨åˆ¸è¶¨å‹¢
            daily_data = ticket_data.get('daily_data', [])
            if daily_data:
                recent_avg = sum(day.get('created_count', 0) for day in daily_data[-7:]) / 7
                overall_avg = sum(day.get('created_count', 0) for day in daily_data) / len(daily_data)
                
                if recent_avg > overall_avg * 1.2:
                    insights.append("ğŸ“ˆ æœ€è¿‘7å¤©çš„ç¥¨åˆ¸é‡æ¯”å¹³å‡æ°´æº–é«˜20%ï¼Œå»ºè­°å¢åŠ å®¢æœäººåŠ›")
                elif recent_avg < overall_avg * 0.8:
                    insights.append("ğŸ“‰ æœ€è¿‘7å¤©ç¥¨åˆ¸é‡ä¸‹é™ï¼Œç³»çµ±é‹è¡Œå¹³ç©©æˆ–ç”¨æˆ¶å•é¡Œæ¸›å°‘")
            
            # åˆ†æå·¥ä½œæµç¨‹æ•ˆç‡
            workflow_stats = workflow_data.get('overall_stats', {})
            if workflow_stats.get('total_workflows', 0) > 0:
                success_rate = workflow_stats.get('success_rate', 0)
                if success_rate >= 95:
                    insights.append("ğŸ¯ å·¥ä½œæµç¨‹è‡ªå‹•åŒ–æ•ˆç‡å„ªç§€ï¼ŒæˆåŠŸç‡è¶…é95%")
                elif success_rate < 80:
                    insights.append("âš ï¸ å·¥ä½œæµç¨‹æˆåŠŸç‡è¼ƒä½ï¼Œå»ºè­°æª¢æŸ¥è‡ªå‹•åŒ–è¦å‰‡è¨­å®š")
            
            # åˆ†æç”¨æˆ¶åƒèˆ‡åº¦
            engagement_score = engagement_data.get('engagement_score', 0)
            if engagement_score >= 80:
                insights.append("ğŸ‘¥ ç¤¾ç¾¤åƒèˆ‡åº¦å¾ˆé«˜ï¼Œç”¨æˆ¶æ´»èºåº¦è‰¯å¥½")
            elif engagement_score < 60:
                insights.append("ğŸ“¢ å»ºè­°åŠ å¼·ç¤¾ç¾¤äº’å‹•æ´»å‹•ï¼Œæå‡ç”¨æˆ¶åƒèˆ‡åº¦")
            
            # æ·»åŠ ä¸€äº›é€šç”¨å»ºè­°
            if len(insights) == 0:
                insights.append("ğŸ“Š ç³»çµ±é‹è¡Œç‹€æ…‹è‰¯å¥½ï¼Œå„é …æŒ‡æ¨™æ­£å¸¸")
            
            insights.append("ğŸ’¡ å»ºè­°å®šæœŸæª¢æŸ¥å„€è¡¨æ¿æ•¸æ“šï¼ŒåŠæ™‚èª¿æ•´é‹ç‡Ÿç­–ç•¥")
            
            return insights
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ™ºèƒ½æ´å¯Ÿå¤±æ•—: {e}")
            return ["âŒ æ™ºèƒ½æ´å¯Ÿç”Ÿæˆå¤±æ•—ï¼Œè«‹æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§"]
    
    def _calculate_change_percentage(self, current: float, previous: float) -> float:
        """è¨ˆç®—è®ŠåŒ–ç™¾åˆ†æ¯”"""
        if previous == 0:
            return 100.0 if current > 0 else 0.0
        return round(((current - previous) / previous) * 100, 1)
    
    def _determine_trend(self, current: float, previous: float) -> str:
        """ç¢ºå®šè¶¨å‹¢æ–¹å‘"""
        if current > previous * 1.05:  # å¢é•·è¶…é5%
            return "up"
        elif current < previous * 0.95:  # ä¸‹é™è¶…é5%
            return "down"
        else:
            return "stable"
    
    def _determine_status(self, metric_type: str, current: float, previous: float) -> str:
        """ç¢ºå®šæŒ‡æ¨™ç‹€æ…‹"""
        change_rate = abs((current - previous) / previous) if previous > 0 else 0
        
        if metric_type == 'tickets':
            if change_rate < 0.1:  # è®ŠåŒ–å°æ–¼10%
                return 'good'
            elif change_rate < 0.3:  # è®ŠåŒ–å°æ–¼30%
                return 'warning'
            else:
                return 'critical'
        
        return 'good'  # é»˜èªç‹€æ…‹
    
    def _determine_performance_status(self, metric_key: str, current_value: float) -> str:
        """æ ¹æ“šæ€§èƒ½æŒ‡æ¨™ç¢ºå®šç‹€æ…‹"""
        status_thresholds = {
            # ç¥¨åˆ¸ç›¸é—œæŒ‡æ¨™ (è¶Šä½è¶Šå¥½)
            'avg_resolution_time': {'good': 2.0, 'warning': 4.0},  # å°æ™‚
            'first_response_time': {'good': 10.0, 'warning': 30.0},  # åˆ†é˜
            
            # æˆåŠŸç‡é¡æŒ‡æ¨™ (è¶Šé«˜è¶Šå¥½) 
            'resolution_rate': {'critical': 70.0, 'warning': 85.0},  # %
            'workflow_success_rate': {'critical': 80.0, 'warning': 90.0},  # %
            'system_uptime': {'critical': 95.0, 'warning': 98.0},  # %
            'automation_coverage': {'critical': 50.0, 'warning': 70.0},  # %
            
            # æ»¿æ„åº¦é¡æŒ‡æ¨™
            'customer_satisfaction': {'critical': 3.0, 'warning': 4.0},  # 1-5åˆ†
            
            # ç³»çµ±æ€§èƒ½æŒ‡æ¨™
            'response_latency': {'good': 100.0, 'warning': 300.0},  # ms
            'error_rate': {'good': 0.5, 'warning': 2.0},  # %
            'avg_workflow_time': {'good': 20.0, 'warning': 60.0},  # ç§’
        }
        
        thresholds = status_thresholds.get(metric_key, {})
        if not thresholds:
            return 'good'  # é»˜èªç‹€æ…‹
        
        # å°æ–¼"è¶Šä½è¶Šå¥½"çš„æŒ‡æ¨™
        if metric_key in ['avg_resolution_time', 'first_response_time', 'response_latency', 'error_rate', 'avg_workflow_time']:
            if current_value <= thresholds.get('good', float('inf')):
                return 'good'
            elif current_value <= thresholds.get('warning', float('inf')):
                return 'warning'
            else:
                return 'critical'
        
        # å°æ–¼"è¶Šé«˜è¶Šå¥½"çš„æŒ‡æ¨™
        else:
            if current_value >= thresholds.get('warning', 0):
                return 'good'
            elif current_value >= thresholds.get('critical', 0):
                return 'warning'
            else:
                return 'critical'
    
    async def _calculate_engagement_score(self, welcome_stats: Dict[str, Any], vote_stats: Dict[str, Any]) -> float:
        """è¨ˆç®—ç”¨æˆ¶åƒèˆ‡åº¦åˆ†æ•¸"""
        try:
            # åŸºæ–¼æ­¡è¿å’ŒæŠ•ç¥¨æ•¸æ“šè¨ˆç®—åƒèˆ‡åº¦
            welcome_score = min(welcome_stats.get('total_welcomes', 0) / 10, 50)  # æœ€å¤š50åˆ†
            vote_score = min(vote_stats.get('total_votes', 0) / 5, 30)  # æœ€å¤š30åˆ†
            participant_score = min(vote_stats.get('participant_count', 0) / 20, 20)  # æœ€å¤š20åˆ†
            
            total_score = welcome_score + vote_score + participant_score
            return min(total_score, 100)  # æœ€å¤š100åˆ†
            
        except Exception as e:
            logger.error(f"è¨ˆç®—åƒèˆ‡åº¦åˆ†æ•¸å¤±æ•—: {e}")
            return 75.0  # é»˜èªåˆ†æ•¸
    
    async def _calculate_trends(self, daily_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è¨ˆç®—æ•¸æ“šè¶¨å‹¢"""
        try:
            if len(daily_data) < 7:
                return {'trend': 'insufficient_data', 'direction': 'stable'}
            
            # å–æœ€è¿‘7å¤©å’Œå‰7å¤©çš„æ•¸æ“šé€²è¡Œæ¯”è¼ƒ
            recent_values = [day.get('created_count', 0) for day in daily_data[-7:]]
            previous_values = [day.get('created_count', 0) for day in daily_data[-14:-7]] if len(daily_data) >= 14 else recent_values
            
            recent_avg = sum(recent_values) / len(recent_values)
            previous_avg = sum(previous_values) / len(previous_values)
            
            if recent_avg > previous_avg * 1.1:
                direction = 'increasing'
            elif recent_avg < previous_avg * 0.9:
                direction = 'decreasing'
            else:
                direction = 'stable'
            
            change_rate = ((recent_avg - previous_avg) / previous_avg * 100) if previous_avg > 0 else 0
            
            return {
                'trend': 'calculated',
                'direction': direction,
                'recent_average': round(recent_avg, 2),
                'previous_average': round(previous_avg, 2),
                'change_rate': round(change_rate, 2)
            }
            
        except Exception as e:
            logger.error(f"è¨ˆç®—è¶¨å‹¢å¤±æ•—: {e}")
            return {'trend': 'error', 'direction': 'stable'}
    
    async def _calculate_workflow_efficiency(self, executions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è¨ˆç®—å·¥ä½œæµç¨‹æ•ˆç‡"""
        try:
            if not executions:
                return {'efficiency_score': 0, 'success_rate': 0, 'avg_execution_time': 0}
            
            successful_executions = [e for e in executions if e.get('status') == 'completed']
            failed_executions = [e for e in executions if e.get('status') == 'failed']
            
            success_rate = (len(successful_executions) / len(executions)) * 100
            
            # è¨ˆç®—å¹³å‡åŸ·è¡Œæ™‚é–“
            execution_times = []
            for execution in successful_executions:
                if execution.get('execution_time'):
                    execution_times.append(execution['execution_time'])
            
            avg_execution_time = sum(execution_times) / len(execution_times) if execution_times else 0
            
            # æ•ˆç‡åˆ†æ•¸ (åŸºæ–¼æˆåŠŸç‡å’ŒåŸ·è¡Œæ™‚é–“)
            efficiency_score = success_rate * 0.7 + max(0, (100 - avg_execution_time)) * 0.3
            
            return {
                'efficiency_score': round(efficiency_score, 2),
                'success_rate': round(success_rate, 2),
                'avg_execution_time': round(avg_execution_time, 2),
                'total_executions': len(executions),
                'successful_executions': len(successful_executions),
                'failed_executions': len(failed_executions)
            }
            
        except Exception as e:
            logger.error(f"è¨ˆç®—å·¥ä½œæµç¨‹æ•ˆç‡å¤±æ•—: {e}")
            return {'efficiency_score': 0, 'success_rate': 0, 'avg_execution_time': 0}
    
    async def _get_detailed_performance_data(self, guild_id: int, days: int) -> Dict[str, Any]:
        """ç²å–è©³ç´°æ€§èƒ½æ•¸æ“š"""
        try:
            # ç²å–ç¥¨åˆ¸æ€§èƒ½æŒ‡æ¨™
            ticket_metrics = await self.ticket_dao.get_ticket_performance_metrics(guild_id, days)
            
            # ç²å–å·¥ä½œæµç¨‹æ€§èƒ½
            workflow_stats = await self.workflow_dao.get_guild_workflow_statistics(guild_id, days)
            
            # æ•´åˆæ€§èƒ½æ•¸æ“š
            performance_data = {
                'ticket_metrics': ticket_metrics,
                'workflow_stats': workflow_stats,
                'system_metrics': await self._get_system_performance_metrics(guild_id, days)
            }
            
            return performance_data
            
        except Exception as e:
            logger.error(f"ç²å–è©³ç´°æ€§èƒ½æ•¸æ“šå¤±æ•—: {e}")
            return {}
    
    async def _get_historical_data_for_prediction(self, guild_id: int) -> Dict[str, Any]:
        """ç²å–ç”¨æ–¼é æ¸¬çš„æ­·å²æ•¸æ“š"""
        try:
            # ç²å–60å¤©çš„æ­·å²æ•¸æ“šç”¨æ–¼é æ¸¬
            end_date = datetime.now(timezone.utc).date()
            start_date = end_date - timedelta(days=60)
            
            daily_stats = await self.ticket_dao.get_daily_ticket_stats(guild_id, start_date, end_date)
            
            # æå–ç¥¨åˆ¸é‡æ•¸æ“š
            daily_volumes = [stats.get('created_count', 0) for stats in daily_stats]
            
            return {
                'daily_volumes': daily_volumes,
                'date_range': {'start': start_date, 'end': end_date},
                'data_quality': 'good' if len(daily_volumes) >= 30 else 'limited'
            }
            
        except Exception as e:
            logger.error(f"ç²å–æ­·å²é æ¸¬æ•¸æ“šå¤±æ•—: {e}")
            return {'daily_volumes': [], 'data_quality': 'poor'}
    
    async def _generate_predictive_metrics(self, historical_data: Dict[str, Any]) -> Dict[str, MetricSummary]:
        """ç”Ÿæˆé æ¸¬æŒ‡æ¨™ (è¿”å› MetricSummary å°è±¡)"""
        try:
            daily_volumes = historical_data.get('daily_volumes', [])
            
            if not daily_volumes:
                return {
                    'prediction_confidence': MetricSummary(
                        current_value=0.0,
                        previous_value=0.0, 
                        change_percentage=0.0,
                        trend='stable',
                        status='warning'
                    ),
                    'forecasted_trend': MetricSummary(
                        current_value=0.0,
                        previous_value=0.0,
                        change_percentage=0.0,
                        trend='stable', 
                        status='warning'
                    )
                }
            
            # è¨ˆç®—è¶¨å‹¢æ–¹å‘
            recent_avg = sum(daily_volumes[-7:]) / min(7, len(daily_volumes)) if daily_volumes else 0
            overall_avg = sum(daily_volumes) / len(daily_volumes) if daily_volumes else 0
            
            if recent_avg > overall_avg * 1.15:
                trend_direction = 'up'
                forecasted_growth = ((recent_avg - overall_avg) / overall_avg) * 100
                status = 'warning'  # å¯èƒ½éœ€è¦é—œæ³¨
            elif recent_avg > overall_avg * 1.05:
                trend_direction = 'up'
                forecasted_growth = ((recent_avg - overall_avg) / overall_avg) * 100
                status = 'good'
            elif recent_avg < overall_avg * 0.85:
                trend_direction = 'down'
                forecasted_growth = ((recent_avg - overall_avg) / overall_avg) * 100
                status = 'good'  # ä¸‹é™å¯èƒ½æ˜¯å¥½äº‹
            elif recent_avg < overall_avg * 0.95:
                trend_direction = 'down'
                forecasted_growth = ((recent_avg - overall_avg) / overall_avg) * 100
                status = 'good'
            else:
                trend_direction = 'stable'
                forecasted_growth = 0.0
                status = 'good'
            
            # è¨ˆç®—é æ¸¬ä¿¡å¿ƒåº¦
            data_variance = self._calculate_variance(daily_volumes)
            data_consistency = 1.0 / (1.0 + data_variance) if data_variance > 0 else 1.0
            data_completeness = len(daily_volumes) / 60.0  # åŸºæ–¼60å¤©å®Œæ•´åº¦
            prediction_confidence = min(data_consistency * data_completeness, 1.0)
            
            # è©•ä¼°ä¿¡å¿ƒåº¦ç‹€æ…‹
            if prediction_confidence >= 0.8:
                confidence_status = 'good'
            elif prediction_confidence >= 0.5:
                confidence_status = 'warning'
            else:
                confidence_status = 'critical'
            
            return {
                'prediction_confidence': MetricSummary(
                    current_value=round(prediction_confidence * 100, 1),
                    previous_value=50.0,  # åŸºæº–å€¼
                    change_percentage=round((prediction_confidence - 0.5) * 200, 1),
                    trend='up' if prediction_confidence > 0.5 else 'down',
                    status=confidence_status
                ),
                'forecasted_trend': MetricSummary(
                    current_value=round(recent_avg, 1),
                    previous_value=round(overall_avg, 1),
                    change_percentage=round(forecasted_growth, 1),
                    trend=trend_direction,
                    status=status
                )
            }
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆé æ¸¬æŒ‡æ¨™å¤±æ•—: {e}")
            return {
                'prediction_confidence': MetricSummary(
                    current_value=0.0,
                    previous_value=0.0,
                    change_percentage=0.0,
                    trend='stable',
                    status='critical'
                ),
                'forecasted_trend': MetricSummary(
                    current_value=0.0,
                    previous_value=0.0,
                    change_percentage=0.0,
                    trend='stable',
                    status='critical'
                )
            }
    
    async def _generate_predictive_insights(self, historical_data: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆé æ¸¬æ´å¯Ÿ"""
        try:
            insights = []
            daily_volumes = historical_data.get('daily_volumes', [])
            
            if not daily_volumes:
                insights.append("ğŸ“Š æ•¸æ“šä¸è¶³ï¼šç„¡æ³•ç”Ÿæˆå¯é çš„é æ¸¬åˆ†æ")
                return insights
            
            # ç”ŸæˆåŸºæ–¼æ•¸æ“šçš„æ´å¯Ÿ
            recent_avg = sum(daily_volumes[-7:]) / min(7, len(daily_volumes))
            overall_avg = sum(daily_volumes) / len(daily_volumes)
            max_volume = max(daily_volumes) if daily_volumes else 0
            min_volume = min(daily_volumes) if daily_volumes else 0
            
            # è¶¨å‹¢æ´å¯Ÿ
            if recent_avg > overall_avg * 1.15:
                insights.append("ğŸ“ˆ å¼·çƒˆä¸Šå‡è¶¨å‹¢ï¼šæœ€è¿‘ä¸€é€±çš„ç¥¨åˆ¸é‡æ¯”å¹³å‡å€¼é«˜15%ä»¥ä¸Šï¼Œå»ºè­°å¢åŠ äººåŠ›é…ç½®")
            elif recent_avg > overall_avg * 1.05:
                insights.append("ğŸ“Š è¼•å¾®ä¸Šå‡è¶¨å‹¢ï¼šç¥¨åˆ¸é‡å‘ˆç¾æº«å’Œå¢é•·ï¼Œè«‹å¯†åˆ‡é—œæ³¨è³‡æºéœ€æ±‚")
            elif recent_avg < overall_avg * 0.85:
                insights.append("ğŸ“‰ å¼·çƒˆä¸‹é™è¶¨å‹¢ï¼šç¥¨åˆ¸é‡é¡¯è‘—æ¸›å°‘ï¼Œå¯è€ƒæ…®èª¿æ•´æœå‹™ç­–ç•¥")
            elif recent_avg < overall_avg * 0.95:
                insights.append("ğŸ“Š è¼•å¾®ä¸‹é™è¶¨å‹¢ï¼šç¥¨åˆ¸é‡ç•¥æœ‰ä¸‹é™ï¼Œå¯èƒ½æ˜¯ç©æ¥µçš„æ”¹å–„ä¿¡è™Ÿ")
            else:
                insights.append("â¡ï¸ ç©©å®šç‹€æ…‹ï¼šç¥¨åˆ¸é‡ä¿æŒç›¸å°ç©©å®šï¼Œç³»çµ±é‹ä½œè‰¯å¥½")
            
            # å®¹é‡æ´å¯Ÿ
            volume_range = max_volume - min_volume
            if volume_range > overall_avg * 2:
                insights.append("âš ï¸ é«˜æ³¢å‹•æ€§ï¼šç¥¨åˆ¸é‡æ³¢å‹•è¼ƒå¤§ï¼Œå»ºè­°å»ºç«‹å½ˆæ€§è™•ç†æ©Ÿåˆ¶")
            
            # é æ¸¬å»ºè­°
            predicted_volume = self._predict_linear_trend(daily_volumes, 7)
            if predicted_volume and len(predicted_volume) > 0:
                next_week_avg = sum(predicted_volume) / len(predicted_volume)
                if next_week_avg > overall_avg * 1.2:
                    insights.append("ğŸš¨ é è­¦ï¼šé æ¸¬ä¸‹é€±ç¥¨åˆ¸é‡å¯èƒ½å¤§å¹…å¢åŠ ï¼Œå»ºè­°æå‰æº–å‚™")
                elif next_week_avg > overall_avg * 1.1:
                    insights.append("âš¡ æº–å‚™ï¼šé æ¸¬ç¥¨åˆ¸é‡å°‡ç•¥æœ‰å¢åŠ ï¼Œå»ºè­°é©åº¦èª¿æ•´è³‡æº")
            
            # æ•¸æ“šå“è³ªæ´å¯Ÿ
            data_quality = historical_data.get('data_quality', 'unknown')
            if data_quality == 'limited':
                insights.append("ğŸ“‹ æ•¸æ“šæé†’ï¼šæ­·å²æ•¸æ“šæœ‰é™ï¼Œé æ¸¬æº–ç¢ºæ€§å¯èƒ½å—åˆ°å½±éŸ¿")
            elif data_quality == 'poor':
                insights.append("âš ï¸ æ•¸æ“šè­¦å‘Šï¼šæ•¸æ“šå“è³ªä¸ä½³ï¼Œå»ºè­°æ”¹å–„æ•¸æ“šæ”¶é›†æ©Ÿåˆ¶")
            
            return insights[:5]  # é™åˆ¶æœ€å¤š5å€‹æ´å¯Ÿ
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆé æ¸¬æ´å¯Ÿå¤±æ•—: {e}")
            return ["âŒ æ´å¯Ÿç”Ÿæˆå¤±æ•—ï¼Œè«‹æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§"]
    
    def _calculate_variance(self, data: List[float]) -> float:
        """è¨ˆç®—æ•¸æ“šè®Šç•°æ•¸"""
        if len(data) < 2:
            return 0.0
        
        mean = sum(data) / len(data)
        variance = sum((x - mean) ** 2 for x in data) / len(data)
        return variance
    
    # ========== å¿«å–ç®¡ç† ==========
    
    async def clear_dashboard_cache(self, cache_key: Optional[str] = None):
        """æ¸…é™¤å„€è¡¨æ¿å¿«å–"""
        if cache_key:
            if cache_key in self._dashboard_cache:
                del self._dashboard_cache[cache_key]
                logger.info(f"å·²æ¸…é™¤å„€è¡¨æ¿å¿«å–: {cache_key}")
        else:
            self._dashboard_cache.clear()
            logger.info("å·²æ¸…é™¤æ‰€æœ‰å„€è¡¨æ¿å¿«å–")
    
    async def get_dashboard_cache_info(self) -> Dict[str, Any]:
        """ç²å–å¿«å–è³‡è¨Š"""
        return {
            'cache_count': len(self._dashboard_cache),
            'cache_keys': list(self._dashboard_cache.keys()),
            'cache_ttl': self._cache_ttl,
            'memory_usage': sum(len(str(dashboard)) for dashboard in self._dashboard_cache.values())
        }
    
    async def _generate_performance_metrics(self, performance_data: Dict[str, Any]) -> Dict[str, MetricSummary]:
        """ç”Ÿæˆæ€§èƒ½æŒ‡æ¨™"""
        try:
            ticket_metrics = performance_data.get('ticket_metrics', {})
            system_metrics = performance_data.get('system_metrics', {})
            workflow_metrics = performance_data.get('workflow_metrics', {})
            
            # è¨ˆç®—é—œéµæ€§èƒ½æŒ‡æ¨™çš„åŸå§‹å€¼
            raw_metrics = {
                # ç¥¨åˆ¸ç›¸é—œæŒ‡æ¨™
                'avg_resolution_time': ticket_metrics.get('avg_resolution_hours', 2.5),
                'resolution_rate': ticket_metrics.get('resolution_rate', 85.0),
                'customer_satisfaction': ticket_metrics.get('satisfaction_score', 4.2),
                'first_response_time': ticket_metrics.get('avg_first_response_minutes', 15.0),
                
                # ç³»çµ±æ€§èƒ½æŒ‡æ¨™
                'system_uptime': system_metrics.get('uptime_percentage', 99.5),
                'response_latency': system_metrics.get('avg_response_ms', 150.0),
                'error_rate': system_metrics.get('error_rate', 0.1),
                
                # å·¥ä½œæµç¨‹æ•ˆç‡æŒ‡æ¨™
                'workflow_success_rate': workflow_metrics.get('success_rate', 95.0),
                'avg_workflow_time': workflow_metrics.get('avg_execution_time', 30.0),
                'automation_coverage': workflow_metrics.get('automation_rate', 75.0),
            }
            
            # æ¨¡æ“¬å‰ä¸€æœŸé–“çš„æ•¸æ“šï¼ˆå¯¦éš›æ‡‰è©²å¾æ­·å²æ•¸æ“šä¸­ç²å–ï¼‰
            previous_metrics = {
                'avg_resolution_time': raw_metrics['avg_resolution_time'] * 1.1,
                'resolution_rate': raw_metrics['resolution_rate'] * 0.95,
                'customer_satisfaction': raw_metrics['customer_satisfaction'] * 0.98,
                'first_response_time': raw_metrics['first_response_time'] * 1.2,
                'system_uptime': raw_metrics['system_uptime'] * 0.995,
                'response_latency': raw_metrics['response_latency'] * 1.1,
                'error_rate': raw_metrics['error_rate'] * 0.8,
                'workflow_success_rate': raw_metrics['workflow_success_rate'] * 0.96,
                'avg_workflow_time': raw_metrics['avg_workflow_time'] * 1.05,
                'automation_coverage': raw_metrics['automation_coverage'] * 0.95,
            }
            
            # è½‰æ›ç‚º MetricSummary å°è±¡
            metrics = {}
            for key, current_value in raw_metrics.items():
                previous_value = previous_metrics.get(key, current_value)
                
                # è¨ˆç®—è®ŠåŒ–ç™¾åˆ†æ¯”
                change_percentage = self._calculate_change_percentage(current_value, previous_value)
                
                # ç¢ºå®šè¶¨å‹¢ï¼ˆå°æ–¼æŸäº›æŒ‡æ¨™ï¼Œè¶Šä½è¶Šå¥½ï¼‰
                reverse_trend_metrics = {'avg_resolution_time', 'response_latency', 'error_rate', 'first_response_time', 'avg_workflow_time'}
                if key in reverse_trend_metrics:
                    trend = self._determine_trend(previous_value, current_value)  # åå‘è¶¨å‹¢
                else:
                    trend = self._determine_trend(current_value, previous_value)
                
                # ç¢ºå®šç‹€æ…‹
                status = self._determine_performance_status(key, current_value)
                
                metrics[key] = MetricSummary(
                    current_value=current_value,
                    previous_value=previous_value,
                    change_percentage=change_percentage,
                    trend=trend,
                    status=status
                )
            
            # è¨ˆç®—ç¶œåˆæ€§èƒ½è©•åˆ†
            score_components = [
                min(100, max(0, 100 - (raw_metrics['avg_resolution_time'] - 2) * 10)),
                raw_metrics['resolution_rate'],
                raw_metrics['customer_satisfaction'] * 20,  # è½‰æ›ç‚ºç™¾åˆ†æ¯”
                raw_metrics['system_uptime'],
                min(100, max(0, 100 - raw_metrics['response_latency'] / 10)),
                raw_metrics['workflow_success_rate'],
            ]
            
            overall_score = sum(score_components) / len(score_components)
            previous_overall_score = overall_score * 0.95  # æ¨¡æ“¬å‰ä¸€æœŸé–“æ•¸æ“š
            
            metrics['overall_performance_score'] = MetricSummary(
                current_value=overall_score,
                previous_value=previous_overall_score,
                change_percentage=self._calculate_change_percentage(overall_score, previous_overall_score),
                trend=self._determine_trend(overall_score, previous_overall_score),
                status='good' if overall_score >= 80 else 'warning' if overall_score >= 60 else 'critical'
            )
            
            return metrics
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ€§èƒ½æŒ‡æ¨™å¤±æ•—: {e}")
            return {
                'avg_resolution_time': 0,
                'resolution_rate': 0,
                'customer_satisfaction': 0,
                'first_response_time': 0,
                'system_uptime': 0,
                'response_latency': 0,
                'error_rate': 0,
                'workflow_success_rate': 0,
                'avg_workflow_time': 0,
                'automation_coverage': 0,
                'overall_performance_score': 0
            }
    
    async def _generate_performance_insights(self, performance_data: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½æ´å¯Ÿå»ºè­°"""
        try:
            insights = []
            ticket_metrics = performance_data.get('ticket_metrics', {})
            system_metrics = performance_data.get('system_metrics', {})
            workflow_metrics = performance_data.get('workflow_metrics', {})
            
            # ç¥¨åˆ¸è™•ç†å»ºè­°
            resolution_rate = ticket_metrics.get('resolution_rate', 0)
            if resolution_rate < 80:
                insights.append("âš ï¸ ç¥¨åˆ¸è§£æ±ºç‡åä½ï¼Œå»ºè­°æª¢æŸ¥è™•ç†æµç¨‹å’Œå®¢æœåŸ¹è¨“")
            elif resolution_rate > 95:
                insights.append("âœ… ç¥¨åˆ¸è§£æ±ºç‡è¡¨ç¾å„ªç§€ï¼Œç¶­æŒç•¶å‰æœå‹™å“è³ª")
            
            avg_resolution_time = ticket_metrics.get('avg_resolution_hours', 0)
            if avg_resolution_time > 4:
                insights.append("â±ï¸ å¹³å‡è§£æ±ºæ™‚é–“è¼ƒé•·ï¼Œå»ºè­°å„ªåŒ–å·¥ä½œæµç¨‹æˆ–å¢åŠ äººåŠ›")
            elif avg_resolution_time < 1:
                insights.append("ğŸš€ å›æ‡‰æ™‚é–“å„ªç§€ï¼Œå®¢æˆ¶é«”é©—è‰¯å¥½")
            
            # ç³»çµ±æ€§èƒ½å»ºè­°
            uptime = system_metrics.get('uptime_percentage', 99.5)
            if uptime < 99:
                insights.append("ğŸ”§ ç³»çµ±æ­£å¸¸é‹è¡Œæ™‚é–“éœ€è¦æ”¹å–„ï¼Œå»ºè­°æª¢æŸ¥åŸºç¤è¨­æ–½")
            elif uptime > 99.9:
                insights.append("ğŸ’ª ç³»çµ±ç©©å®šæ€§æ¥µä½³ï¼ŒåŸºç¤è¨­æ–½é‹è¡Œè‰¯å¥½")
            
            response_latency = system_metrics.get('avg_response_ms', 150)
            if response_latency > 300:
                insights.append("ğŸ“¡ ç³»çµ±å›æ‡‰å»¶é²è¼ƒé«˜ï¼Œå»ºè­°å„ªåŒ–ç¶²è·¯æˆ–ä¼ºæœå™¨é…ç½®")
            elif response_latency < 100:
                insights.append("âš¡ ç³»çµ±å›æ‡‰é€Ÿåº¦å„ªç§€ï¼Œç”¨æˆ¶é«”é©—è‰¯å¥½")
            
            # å·¥ä½œæµç¨‹å»ºè­°
            success_rate = workflow_metrics.get('success_rate', 95)
            if success_rate < 90:
                insights.append("ğŸ”„ å·¥ä½œæµç¨‹æˆåŠŸç‡éœ€è¦æ”¹å–„ï¼Œå»ºè­°æª¢æŸ¥è‡ªå‹•åŒ–é‚è¼¯")
            elif success_rate > 98:
                insights.append("ğŸ¯ å·¥ä½œæµç¨‹é‹è¡Œç©©å®šï¼Œè‡ªå‹•åŒ–æ•ˆæœå„ªç§€")
            
            automation_rate = workflow_metrics.get('automation_rate', 75)
            if automation_rate < 50:
                insights.append("ğŸ¤– è‡ªå‹•åŒ–è¦†è“‹ç‡è¼ƒä½ï¼Œå»ºè­°å¢åŠ æ›´å¤šè‡ªå‹•åŒ–æµç¨‹")
            elif automation_rate > 90:
                insights.append("ğŸ† è‡ªå‹•åŒ–ç¨‹åº¦å¾ˆé«˜ï¼Œæœ‰æ•ˆæå‡å·¥ä½œæ•ˆç‡")
            
            # å¦‚æœæ²’æœ‰ç‰¹åˆ¥çš„å»ºè­°ï¼Œæä¾›é€šç”¨å»ºè­°
            if not insights:
                insights.append("ğŸ“Š ç³»çµ±æ•´é«”é‹è¡Œæ­£å¸¸ï¼ŒæŒçºŒç›£æ§å„é …æŒ‡æ¨™")
                insights.append("ğŸ’¡ å»ºè­°å®šæœŸæª¢è¦–æ€§èƒ½è¶¨å‹¢ï¼ŒæŒçºŒå„ªåŒ–æœå‹™å“è³ª")
            
            return insights[:5]  # æœ€å¤šè¿”å›5æ¢å»ºè­°
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ€§èƒ½æ´å¯Ÿå¤±æ•—: {e}")
            return ["âŒ ç„¡æ³•ç”Ÿæˆæ€§èƒ½å»ºè­°ï¼Œè«‹æª¢æŸ¥ç³»çµ±ç‹€æ…‹"]

# å…¨åŸŸå„€è¡¨æ¿ç®¡ç†å™¨å¯¦ä¾‹
dashboard_manager = DashboardManager()