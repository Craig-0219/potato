# shared/cache_manager.py - Redis å¤šå±¤å¿«å–ç®¡ç†ç³»çµ±
"""
ä¼æ¥­ç´š Redis å¿«å–ç®¡ç†ç³»çµ± v2.2.0
åŠŸèƒ½ç‰¹é»ï¼š
1. å¤šå±¤å¿«å–ç­–ç•¥ï¼ˆL1: è¨˜æ†¶é«”ï¼ŒL2: Redisï¼ŒL3: è³‡æ–™åº«ï¼‰
2. æ™ºèƒ½å¤±æ•ˆæ©Ÿåˆ¶
3. å¿«å–çµ±è¨ˆå’Œç›£æ§
4. è‡ªå‹•é ç†±å’Œæ¸…ç†
5. åˆ†æ•£å¼å¿«å–åŒæ­¥
"""

import asyncio
import hashlib
import json
import pickle
import time
from dataclasses import dataclass
from enum import Enum
from typing import Any, Callable, Dict, List

try:
    import redis.asyncio as redis

    REDIS_AVAILABLE = True
    REDIS_TYPE = "redis-py"
    aioredis = redis  # ç‚ºäº†å‘å¾Œç›¸å®¹
except ImportError:
    try:
        import aioredis

        REDIS_AVAILABLE = True
        REDIS_TYPE = "aioredis"
        redis = aioredis  # ç‚ºäº†å‘å¾Œç›¸å®¹
    except ImportError:
        REDIS_AVAILABLE = False
        REDIS_TYPE = None
        redis = None
        aioredis = None

from src.potato_shared.logger import logger


class CacheLevel(Enum):
    """å¿«å–å±¤ç´š"""

    L1_MEMORY = "l1_memory"  # æœ¬åœ°è¨˜æ†¶é«”å¿«å–
    L2_REDIS = "l2_redis"  # Redis å¿«å–
    L3_DATABASE = "l3_database"  # è³‡æ–™åº«ï¼ˆæœ€çµ‚æ•¸æ“šæºï¼‰


class CacheStrategy(Enum):
    """å¿«å–ç­–ç•¥"""

    WRITE_THROUGH = "write_through"  # å¯«å…¥æ™‚åŒæ™‚æ›´æ–°å¿«å–
    WRITE_BACK = "write_back"  # å¯«å…¥æ™‚åªæ›´æ–°å¿«å–ï¼Œå®šæœŸå›å¯«
    WRITE_AROUND = "write_around"  # å¯«å…¥æ™‚è·³éå¿«å–


@dataclass
class CacheConfig:
    """å¿«å–é…ç½®"""

    # L1 è¨˜æ†¶é«”å¿«å–é…ç½®
    l1_max_size: int = 1000  # æœ€å¤§æ¢ç›®æ•¸
    l1_ttl: int = 300  # å­˜æ´»æ™‚é–“ï¼ˆç§’ï¼‰

    # L2 Redis å¿«å–é…ç½®
    l2_ttl: int = 3600  # å­˜æ´»æ™‚é–“ï¼ˆç§’ï¼‰
    l2_max_memory: str = "100mb"  # æœ€å¤§è¨˜æ†¶é«”ä½¿ç”¨

    # å…¶ä»–é…ç½®
    enable_compression: bool = True  # å•Ÿç”¨å£“ç¸®
    enable_statistics: bool = True  # å•Ÿç”¨çµ±è¨ˆ
    auto_preload: bool = False  # è‡ªå‹•é è¼‰ç†±é»æ•¸æ“š

    # åˆ†æ•£å¼é…ç½®
    enable_distributed_invalidation: bool = True  # åˆ†æ•£å¼å¤±æ•ˆé€šçŸ¥


@dataclass
class CacheEntry:
    """å¿«å–æ¢ç›®"""

    key: str
    value: Any
    timestamp: float
    ttl: int
    access_count: int = 0
    last_access: float = 0


@dataclass
class CacheStatistics:
    """å¿«å–çµ±è¨ˆ"""

    l1_hits: int = 0
    l1_misses: int = 0
    l2_hits: int = 0
    l2_misses: int = 0
    total_requests: int = 0
    total_sets: int = 0
    total_deletes: int = 0

    def hit_rate(self) -> float:
        """ç¸½å‘½ä¸­ç‡"""
        total_hits = self.l1_hits + self.l2_hits
        if self.total_requests == 0:
            return 0.0
        return total_hits / self.total_requests

    def l1_hit_rate(self) -> float:
        """L1 å‘½ä¸­ç‡"""
        l1_total = self.l1_hits + self.l1_misses
        if l1_total == 0:
            return 0.0
        return self.l1_hits / l1_total

    def l2_hit_rate(self) -> float:
        """L2 å‘½ä¸­ç‡"""
        l2_total = self.l2_hits + self.l2_misses
        if l2_total == 0:
            return 0.0
        return self.l2_hits / l2_total


class MultiLevelCacheManager:
    """å¤šå±¤å¿«å–ç®¡ç†å™¨"""

    def __init__(self, redis_url: str = None, config: CacheConfig = None):
        self.redis_url = redis_url or "redis://localhost:6379"
        self.config = config or CacheConfig()
        self.redis = None
        self.is_connected = False

        # L1 è¨˜æ†¶é«”å¿«å– (LRU)
        self._l1_cache: Dict[str, CacheEntry] = {}
        self._l1_access_order: List[str] = []

        # çµ±è¨ˆæ•¸æ“š
        self.stats = CacheStatistics()

        # é–å°è±¡
        self._locks: Dict[str, asyncio.Lock] = {}
        self._global_lock = asyncio.Lock()

        # é è¼‰å’Œæ¸…ç†ä»»å‹™
        self._preload_task = None
        self._cleanup_task = None

        # åˆ†æ•£å¼å¤±æ•ˆé€šçŸ¥
        self._invalidation_subscriber = None

        logger.info(f"ğŸ”§ å¿«å–ç®¡ç†å™¨åˆå§‹åŒ– - Redis: {REDIS_AVAILABLE}")

    async def initialize(self):
        """åˆå§‹åŒ–å¿«å–ç³»çµ±"""
        try:
            # é€£æ¥ Redis
            if REDIS_AVAILABLE:
                await self._connect_redis()

            # å•Ÿå‹•èƒŒæ™¯ä»»å‹™
            await self._start_background_tasks()

            logger.info("âœ… å¤šå±¤å¿«å–ç³»çµ±åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ å¿«å–ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")
            raise

    async def _connect_redis(self):
        """é€£æ¥ Redis"""
        try:
            if REDIS_TYPE == "aioredis":
                self.redis = await aioredis.from_url(
                    self.redis_url,
                    decode_responses=True,
                    socket_connect_timeout=5,
                    socket_timeout=5,
                )
            elif REDIS_TYPE == "redis-py":
                self.redis = await aioredis.from_url(self.redis_url, decode_responses=True)

            # æ¸¬è©¦é€£æ¥
            await self.redis.ping()
            self.is_connected = True
            logger.info(f"âœ… Redis é€£æ¥æˆåŠŸ ({REDIS_TYPE})")

            # è¨­ç½® Redis é…ç½®
            await self._configure_redis()

        except Exception as e:
            logger.warning(f"âš ï¸ Redis é€£æ¥å¤±æ•—ï¼Œå°‡ä½¿ç”¨æœ¬æ©Ÿæ¨¡å¼: {e}")
            self.redis = None
            self.is_connected = False

    async def _configure_redis(self):
        """é…ç½® Redis"""
        if not self.redis:
            return

        try:
            # è¨­ç½®æœ€å¤§è¨˜æ†¶é«”
            await self.redis.config_set("maxmemory", self.config.l2_max_memory)
            await self.redis.config_set("maxmemory-policy", "allkeys-lru")

            # å•Ÿç”¨ RDB å¿«ç…§
            await self.redis.config_set("save", "900 1 300 10 60 10000")

            logger.info("âœ… Redis é…ç½®å®Œæˆ")

        except Exception as e:
            logger.warning(f"âš ï¸ Redis é…ç½®å¤±æ•—: {e}")

    async def _start_background_tasks(self):
        """å•Ÿå‹•èƒŒæ™¯ä»»å‹™"""
        # L1 å¿«å–æ¸…ç†ä»»å‹™
        self._cleanup_task = asyncio.create_task(self._cleanup_l1_cache())

        # çµ±è¨ˆæ•¸æ“šå®šæœŸå ±å‘Š
        if self.config.enable_statistics:
            asyncio.create_task(self._periodic_statistics_report())

        # åˆ†æ•£å¼å¤±æ•ˆé€šçŸ¥è¨‚é–±
        if self.config.enable_distributed_invalidation and self.redis:
            asyncio.create_task(self._start_invalidation_subscriber())

    async def get(self, key: str, default: Any = None) -> Any:
        """å¤šå±¤å¿«å–è®€å–"""
        self.stats.total_requests += 1
        start_time = time.time()

        try:
            # 1. å˜—è©¦ L1 è¨˜æ†¶é«”å¿«å–
            value = await self._get_from_l1(key)
            if value is not None:
                self.stats.l1_hits += 1
                logger.debug(f"ğŸ¯ L1 å¿«å–å‘½ä¸­: {key}")
                return value
            else:
                self.stats.l1_misses += 1

            # 2. å˜—è©¦ L2 Redis å¿«å–
            if self.redis:
                value = await self._get_from_l2(key)
                if value is not None:
                    self.stats.l2_hits += 1
                    logger.debug(f"ğŸ¯ L2 å¿«å–å‘½ä¸­: {key}")

                    # å›å¡« L1 å¿«å–
                    await self._set_to_l1(key, value, self.config.l1_ttl)
                    return value
                else:
                    self.stats.l2_misses += 1

            # 3. å¿«å–æœªå‘½ä¸­ï¼Œè¿”å›é è¨­å€¼
            logger.debug(f"ğŸ’¨ å¿«å–æœªå‘½ä¸­: {key}")
            return default

        except Exception as e:
            logger.error(f"âŒ è®€å–å¿«å–å¤±æ•— {key}: {e}")
            return default
        finally:
            duration = time.time() - start_time
            if duration > 0.1:  # å¦‚æœè¶…é100msï¼Œè¨˜éŒ„è­¦å‘Š
                logger.warning(f"âš ï¸ å¿«å–è®€å–è€—æ™‚éé•·: {key} - {duration:.3f}s")

    async def set(
        self,
        key: str,
        value: Any,
        ttl: int = None,
        strategy: CacheStrategy = CacheStrategy.WRITE_THROUGH,
    ) -> bool:
        """å¤šå±¤å¿«å–å¯«å…¥"""
        self.stats.total_sets += 1
        start_time = time.time()

        try:
            ttl = ttl or self.config.l1_ttl

            if strategy == CacheStrategy.WRITE_THROUGH:
                # åŒæ™‚å¯«å…¥æ‰€æœ‰å±¤ç´š
                success_l1 = await self._set_to_l1(key, value, ttl)
                success_l2 = True

                if self.redis:
                    success_l2 = await self._set_to_l2(key, value, ttl)

                # ç™¼é€åˆ†æ•£å¼å¤±æ•ˆé€šçŸ¥
                if self.config.enable_distributed_invalidation:
                    await self._notify_invalidation(key, "update")

                return success_l1 and success_l2

            elif strategy == CacheStrategy.WRITE_BACK:
                # åªå¯«å…¥ L1ï¼Œæ¨™è¨˜ç‚ºé«’æ•¸æ“š
                await self._set_to_l1(key, value, ttl, dirty=True)
                return True

            elif strategy == CacheStrategy.WRITE_AROUND:
                # åªå¯«å…¥ L2
                if self.redis:
                    return await self._set_to_l2(key, value, ttl)
                return False

        except Exception as e:
            logger.error(f"âŒ å¯«å…¥å¿«å–å¤±æ•— {key}: {e}")
            return False
        finally:
            duration = time.time() - start_time
            if duration > 0.05:  # å¦‚æœè¶…é50msï¼Œè¨˜éŒ„è­¦å‘Š
                logger.warning(f"âš ï¸ å¿«å–å¯«å…¥è€—æ™‚éé•·: {key} - {duration:.3f}s")

    async def delete(self, key: str) -> bool:
        """åˆªé™¤å¿«å–"""
        self.stats.total_deletes += 1

        try:
            # å¾æ‰€æœ‰å±¤ç´šåˆªé™¤
            success_l1 = await self._delete_from_l1(key)
            success_l2 = True

            if self.redis:
                success_l2 = await self._delete_from_l2(key)

            # ç™¼é€åˆ†æ•£å¼å¤±æ•ˆé€šçŸ¥
            if self.config.enable_distributed_invalidation:
                await self._notify_invalidation(key, "delete")

            return success_l1 or success_l2

        except Exception as e:
            logger.error(f"âŒ åˆªé™¤å¿«å–å¤±æ•— {key}: {e}")
            return False

    async def clear_all(self, pattern: str = "*") -> int:
        """æ¸…ç©ºå¿«å–"""
        count = 0

        try:
            # æ¸…ç©º L1
            if pattern == "*":
                count += len(self._l1_cache)
                self._l1_cache.clear()
                self._l1_access_order.clear()
            else:
                # æ”¯æ´æ¨¡å¼åŒ¹é…
                keys_to_delete = [
                    k for k in self._l1_cache.keys() if self._match_pattern(k, pattern)
                ]
                for key in keys_to_delete:
                    await self._delete_from_l1(key)
                count += len(keys_to_delete)

            # æ¸…ç©º L2
            if self.redis:
                if pattern == "*":
                    await self.redis.flushdb()
                else:
                    keys = await self.redis.keys(pattern)
                    if keys:
                        await self.redis.delete(*keys)
                        count += len(keys)

            logger.info(f"âœ… æ¸…ç©ºå¿«å–å®Œæˆï¼Œå…±æ¸…ç† {count} å€‹æ¢ç›®")
            return count

        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºå¿«å–å¤±æ•—: {e}")
            return 0

    async def get_statistics(self) -> Dict[str, Any]:
        """ç²å–å¿«å–çµ±è¨ˆ"""
        stats = {
            "requests": {
                "total": self.stats.total_requests,
                "hits": self.stats.l1_hits + self.stats.l2_hits,
                "misses": self.stats.l1_misses + self.stats.l2_misses,
                "hit_rate": f"{self.stats.hit_rate():.2%}",
            },
            "l1_memory": {
                "size": len(self._l1_cache),
                "max_size": self.config.l1_max_size,
                "usage": f"{len(self._l1_cache)/self.config.l1_max_size:.1%}",
                "hits": self.stats.l1_hits,
                "misses": self.stats.l1_misses,
                "hit_rate": f"{self.stats.l1_hit_rate():.2%}",
            },
            "l2_redis": {
                "connected": self.is_connected,
                "hits": self.stats.l2_hits,
                "misses": self.stats.l2_misses,
                "hit_rate": f"{self.stats.l2_hit_rate():.2%}",
            },
            "operations": {
                "sets": self.stats.total_sets,
                "deletes": self.stats.total_deletes,
            },
        }

        # æ·»åŠ  Redis è©³ç´°è³‡è¨Š
        if self.redis:
            try:
                info = await self.redis.info()
                stats["l2_redis"].update(
                    {
                        "memory_used": info.get("used_memory_human", "N/A"),
                        "keys": info.get("db0", {}).get("keys", 0),
                        "connections": info.get("connected_clients", 0),
                    }
                )
            except Exception as e:
                logger.warning(f"âš ï¸ ç„¡æ³•ç²å– Redis è³‡è¨Š: {e}")

        return stats

    # ========== L1 è¨˜æ†¶é«”å¿«å–æ“ä½œ ==========

    async def _get_from_l1(self, key: str) -> Any:
        """å¾ L1 å¿«å–è®€å–"""
        async with self._global_lock:
            entry = self._l1_cache.get(key)
            if not entry:
                return None

            # æª¢æŸ¥æ˜¯å¦éæœŸ
            if time.time() - entry.timestamp > entry.ttl:
                del self._l1_cache[key]
                if key in self._l1_access_order:
                    self._l1_access_order.remove(key)
                return None

            # æ›´æ–°å­˜å–ç´€éŒ„
            entry.access_count += 1
            entry.last_access = time.time()

            # æ›´æ–° LRU é †åº
            if key in self._l1_access_order:
                self._l1_access_order.remove(key)
            self._l1_access_order.append(key)

            return entry.value

    async def _set_to_l1(self, key: str, value: Any, ttl: int, dirty: bool = False) -> bool:
        """å¯«å…¥ L1 å¿«å–"""
        async with self._global_lock:
            # æª¢æŸ¥å®¹é‡é™åˆ¶
            if len(self._l1_cache) >= self.config.l1_max_size and key not in self._l1_cache:
                # LRU æ·˜æ±°
                if self._l1_access_order:
                    oldest_key = self._l1_access_order.pop(0)
                    del self._l1_cache[oldest_key]

            # å‰µå»ºå¿«å–æ¢ç›®
            entry = CacheEntry(
                key=key,
                value=value,
                timestamp=time.time(),
                ttl=ttl,
                access_count=1,
                last_access=time.time(),
            )

            self._l1_cache[key] = entry

            # æ›´æ–°å­˜å–é †åº
            if key in self._l1_access_order:
                self._l1_access_order.remove(key)
            self._l1_access_order.append(key)

            return True

    async def _delete_from_l1(self, key: str) -> bool:
        """å¾ L1 å¿«å–åˆªé™¤"""
        async with self._global_lock:
            if key in self._l1_cache:
                del self._l1_cache[key]
                if key in self._l1_access_order:
                    self._l1_access_order.remove(key)
                return True
            return False

    # ========== L2 Redis å¿«å–æ“ä½œ ==========

    async def _get_from_l2(self, key: str) -> Any:
        """å¾ L2 å¿«å–è®€å–"""
        if not self.redis:
            return None

        try:
            data = await self.redis.get(f"cache:{key}")
            if data:
                if self.config.enable_compression:
                    # ä½¿ç”¨ JSON ä»£æ›¿ pickle ä»¥æé«˜å®‰å…¨æ€§
                    import base64
                    import gzip

                    try:
                        compressed_data = base64.b64decode(data.encode("latin-1"))
                        decompressed_data = gzip.decompress(compressed_data)
                        return json.loads(decompressed_data.decode("utf-8"))
                    except Exception:
                        # å›é€€åˆ° JSONï¼ˆç‚ºäº†å…¼å®¹æ€§ï¼‰
                        return json.loads(data)
                else:
                    return json.loads(data)
            return None
        except Exception as e:
            logger.error(f"âŒ L2 è®€å–å¤±æ•— {key}: {e}")
            return None

    async def _set_to_l2(self, key: str, value: Any, ttl: int) -> bool:
        """å¯«å…¥ L2 å¿«å–"""
        if not self.redis:
            return False

        try:
            if self.config.enable_compression:
                data = pickle.dumps(value).decode("latin-1")
            else:
                data = json.dumps(value, ensure_ascii=False)

            await self.redis.setex(f"cache:{key}", ttl, data)
            return True
        except Exception as e:
            logger.error(f"âŒ L2 å¯«å…¥å¤±æ•— {key}: {e}")
            return False

    async def _delete_from_l2(self, key: str) -> bool:
        """å¾ L2 å¿«å–åˆªé™¤"""
        if not self.redis:
            return False

        try:
            result = await self.redis.delete(f"cache:{key}")
            return result > 0
        except Exception as e:
            logger.error(f"âŒ L2 åˆªé™¤å¤±æ•— {key}: {e}")
            return False

    # ========== èƒŒæ™¯ä»»å‹™ ==========

    async def _cleanup_l1_cache(self):
        """L1 å¿«å–æ¸…ç†ä»»å‹™"""
        while True:
            try:
                await asyncio.sleep(60)  # æ¯åˆ†é˜åŸ·è¡Œä¸€æ¬¡

                current_time = time.time()
                expired_keys = []

                async with self._global_lock:
                    for key, entry in self._l1_cache.items():
                        if current_time - entry.timestamp > entry.ttl:
                            expired_keys.append(key)

                # æ¸…ç†éæœŸæ¢ç›®
                for key in expired_keys:
                    await self._delete_from_l1(key)

                if expired_keys:
                    logger.debug(f"ğŸ§¹ L1 å¿«å–æ¸…ç†å®Œæˆï¼Œç§»é™¤ {len(expired_keys)} å€‹éæœŸæ¢ç›®")

            except Exception as e:
                logger.error(f"âŒ L1 å¿«å–æ¸…ç†å¤±æ•—: {e}")

    async def _periodic_statistics_report(self):
        """å®šæœŸçµ±è¨ˆå ±å‘Š"""
        while True:
            try:
                await asyncio.sleep(300)  # æ¯5åˆ†é˜å ±å‘Šä¸€æ¬¡

                stats = await self.get_statistics()
                logger.info(
                    f"ğŸ“Š å¿«å–çµ±è¨ˆ - ç¸½å‘½ä¸­ç‡: {stats['requests']['hit_rate']}, "
                    f"L1: {stats['l1_memory']['hit_rate']}, "
                    f"L2: {stats['l2_redis']['hit_rate']}"
                )

            except Exception as e:
                logger.error(f"âŒ çµ±è¨ˆå ±å‘Šå¤±æ•—: {e}")

    async def _notify_invalidation(self, key: str, action: str):
        """ç™¼é€åˆ†æ•£å¼å¤±æ•ˆé€šçŸ¥"""
        if not self.redis:
            return

        try:
            message = json.dumps(
                {
                    "key": key,
                    "action": action,
                    "timestamp": time.time(),
                    "source": "cache_manager",
                }
            )

            await self.redis.publish("cache:invalidation", message)

        except Exception as e:
            logger.error(f"âŒ ç™¼é€å¤±æ•ˆé€šçŸ¥å¤±æ•—: {e}")

    async def _start_invalidation_subscriber(self):
        """å•Ÿå‹•åˆ†æ•£å¼å¤±æ•ˆé€šçŸ¥è¨‚é–±"""
        if not self.redis:
            return

        try:
            pubsub = self.redis.pubsub()
            await pubsub.subscribe("cache:invalidation")

            async for message in pubsub.listen():
                if message["type"] == "message":
                    try:
                        data = json.loads(message["data"])
                        key = data["key"]
                        action = data["action"]

                        # è™•ç†å¤±æ•ˆé€šçŸ¥
                        if action == "delete":
                            await self._delete_from_l1(key)
                        elif action == "update":
                            # æ›´æ–°é€šçŸ¥ï¼šå¾ L1 ç§»é™¤è®“å…¶é‡æ–°è¼‰å…¥
                            await self._delete_from_l1(key)

                    except Exception as e:
                        logger.error(f"âŒ è™•ç†å¤±æ•ˆé€šçŸ¥å¤±æ•—: {e}")

        except Exception as e:
            logger.error(f"âŒ å¤±æ•ˆé€šçŸ¥è¨‚é–±å¤±æ•—: {e}")

    @staticmethod
    def _match_pattern(text: str, pattern: str) -> bool:
        """ç°¡å–®çš„æ¨¡å¼åŒ¹é…"""
        import fnmatch

        return fnmatch.fnmatch(text, pattern)

    async def close(self):
        """é—œé–‰å¿«å–ç®¡ç†å™¨"""
        try:
            # å–æ¶ˆèƒŒæ™¯ä»»å‹™
            if self._cleanup_task:
                self._cleanup_task.cancel()
            if self._preload_task:
                self._preload_task.cancel()

            # é—œé–‰ Redis é€£æ¥
            if self.redis:
                await self.redis.close()

            logger.info("âœ… å¿«å–ç®¡ç†å™¨å·²é—œé–‰")

        except Exception as e:
            logger.error(f"âŒ é—œé–‰å¿«å–ç®¡ç†å™¨å¤±æ•—: {e}")


# ========== å…¨åŸŸå¯¦ä¾‹å’Œä¾¿æ·å‡½æ•¸ ==========

# å…¨åŸŸå¿«å–ç®¡ç†å™¨å¯¦ä¾‹
cache_manager = MultiLevelCacheManager()


async def init_cache(redis_url: str = None, config: CacheConfig = None):
    """åˆå§‹åŒ–å…¨åŸŸå¿«å–ç®¡ç†å™¨"""
    global cache_manager
    cache_manager = MultiLevelCacheManager(redis_url, config)
    await cache_manager.initialize()


async def get_cache_manager() -> MultiLevelCacheManager:
    """ç²å–å¿«å–ç®¡ç†å™¨å¯¦ä¾‹"""
    return cache_manager


# ä¾¿æ·çš„å¿«å–è£é£¾å™¨
def cached(
    key_prefix: str = "",
    ttl: int = 300,
    strategy: CacheStrategy = CacheStrategy.WRITE_THROUGH,
):
    """å¿«å–è£é£¾å™¨"""

    def decorator(func: Callable):
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆå¿«å–éµ
            key_parts = [key_prefix, func.__name__]

            # æ·»åŠ åƒæ•¸åˆ°éµä¸­
            if args:
                key_parts.extend(str(arg) for arg in args[:3])  # åªå–å‰3å€‹åƒæ•¸
            if kwargs:
                key_parts.extend(f"{k}={v}" for k, v in list(kwargs.items())[:3])

            cache_key = ":".join(filter(None, key_parts))
            cache_key = hashlib.sha256(cache_key.encode()).hexdigest()[:16]  # ä½¿ç”¨ SHA256 ä»£æ›¿ MD5

            # å˜—è©¦å¾å¿«å–è®€å–
            result = await cache_manager.get(cache_key)
            if result is not None:
                return result

            # åŸ·è¡Œå‡½æ•¸ä¸¦å¿«å–çµæœ
            result = await func(*args, **kwargs)
            if result is not None:
                await cache_manager.set(cache_key, result, ttl, strategy)

            return result

        return wrapper

    return decorator


# æ‰¹é‡æ“ä½œæ”¯æ´
async def mget(keys: List[str]) -> Dict[str, Any]:
    """æ‰¹é‡ç²å–"""
    results = {}
    for key in keys:
        results[key] = await cache_manager.get(key)
    return results


async def mset(data: Dict[str, Any], ttl: int = 300) -> int:
    """æ‰¹é‡è¨­ç½®"""
    success_count = 0
    for key, value in data.items():
        if await cache_manager.set(key, value, ttl):
            success_count += 1
    return success_count
