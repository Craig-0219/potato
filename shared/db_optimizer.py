# shared/db_optimizer.py - è³‡æ–™åº«æŸ¥è©¢å„ªåŒ–ç®¡ç†å™¨
"""
è³‡æ–™åº«æŸ¥è©¢å„ªåŒ–ç®¡ç†å™¨ v2.2.0
åŠŸèƒ½ç‰¹é»ï¼š
1. æŸ¥è©¢åŸ·è¡Œè¨ˆåŠƒåˆ†æ
2. è‡ªå‹•ç´¢å¼•å»ºè­°å’Œç®¡ç†
3. æ…¢æŸ¥è©¢æª¢æ¸¬å’Œå„ªåŒ–
4. è³‡æ–™åº«æ€§èƒ½ç›£æ§
5. æŸ¥è©¢å¿«å–ç®¡ç†
6. è³‡æ–™åº«çµ±è¨ˆæ›´æ–°
"""

import hashlib
import json
import re
import time
from dataclasses import dataclass
from datetime import datetime, timezone
from enum import Enum
from typing import Any, Dict, List, Tuple

from bot.db.pool import db_pool
from shared.logger import logger


class QueryType(Enum):
    """æŸ¥è©¢é¡å‹"""

    SELECT = "SELECT"
    INSERT = "INSERT"
    UPDATE = "UPDATE"
    DELETE = "DELETE"
    CREATE = "CREATE"
    ALTER = "ALTER"


class OptimizationLevel(Enum):
    """å„ªåŒ–ç­‰ç´š"""

    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


@dataclass
class QueryAnalysis:
    """æŸ¥è©¢åˆ†æçµæœ"""

    query_hash: str
    original_query: str
    query_type: QueryType
    execution_time: float
    rows_examined: int
    rows_sent: int
    tables_used: List[str]
    indexes_used: List[str]
    optimization_level: OptimizationLevel
    suggestions: List[str]
    explain_plan: Dict[str, Any]
    timestamp: datetime


@dataclass
class IndexRecommendation:
    """ç´¢å¼•å»ºè­°"""

    table_name: str
    columns: List[str]
    index_type: str  # BTREE, HASH, FULLTEXT
    reason: str
    estimated_benefit: float
    create_sql: str


@dataclass
class DatabaseMetrics:
    """è³‡æ–™åº«æ€§èƒ½æŒ‡æ¨™"""

    query_cache_hit_rate: float
    slow_query_count: int
    connections_used: int
    max_connections: int
    innodb_buffer_pool_hit_rate: float
    table_scan_rate: float
    temp_table_rate: float
    key_read_hit_rate: float
    timestamp: datetime


class DatabaseOptimizer:
    """è³‡æ–™åº«å„ªåŒ–ç®¡ç†å™¨"""

    def __init__(self):
        self.slow_query_threshold = 1.0  # 1ç§’ç‚ºæ…¢æŸ¥è©¢é–¾å€¼
        self.query_cache = {}
        self.optimization_history = []
        self.metrics_cache = None
        self.metrics_cache_time = None

        # ç´¢å¼•å„ªåŒ–é…ç½®
        self.index_recommendations = []
        self.auto_create_indexes = False  # é è¨­ä¸è‡ªå‹•å‰µå»ºç´¢å¼•

        logger.info("ğŸ”§ è³‡æ–™åº«å„ªåŒ–ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    async def initialize(self):
        """åˆå§‹åŒ–å„ªåŒ–å™¨"""
        try:
            # æª¢æŸ¥ä¸¦å»ºç«‹å„ªåŒ–ç›¸é—œçš„è¡¨æ ¼
            await self._create_optimization_tables()

            # è¼‰å…¥ç¾æœ‰çš„å„ªåŒ–è¨˜éŒ„
            await self._load_optimization_history()

            # åˆ†æç¾æœ‰è¡¨çµæ§‹
            await self._analyze_existing_tables()

            logger.info("âœ… è³‡æ–™åº«å„ªåŒ–å™¨åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ è³‡æ–™åº«å„ªåŒ–å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            raise

    async def _create_optimization_tables(self):
        """å»ºç«‹å„ªåŒ–ç›¸é—œçš„è¡¨æ ¼"""
        try:
            async with db_pool.connection() as conn:
                async with conn.cursor() as cursor:
                    # æŸ¥è©¢åˆ†æè¨˜éŒ„è¡¨
                    await cursor.execute(
                        """
                        CREATE TABLE IF NOT EXISTS query_analysis_log (
                            id INT AUTO_INCREMENT PRIMARY KEY,
                            query_hash VARCHAR(64) NOT NULL,
                            query_text TEXT NOT NULL,
                            query_type VARCHAR(20) NOT NULL,
                            execution_time DECIMAL(10,4) NOT NULL,
                            rows_examined INT NOT NULL DEFAULT 0,
                            rows_sent INT NOT NULL DEFAULT 0,
                            tables_used JSON,
                            indexes_used JSON,
                            optimization_level VARCHAR(20) NOT NULL,
                            suggestions JSON,
                            explain_plan JSON,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            INDEX idx_query_hash (query_hash),
                            INDEX idx_execution_time (execution_time),
                            INDEX idx_created_at (created_at)
                        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
                    """
                    )

                    # ç´¢å¼•å»ºè­°è¨˜éŒ„è¡¨
                    await cursor.execute(
                        """
                        CREATE TABLE IF NOT EXISTS index_recommendations (
                            id INT AUTO_INCREMENT PRIMARY KEY,
                            table_name VARCHAR(100) NOT NULL,
                            columns JSON NOT NULL,
                            index_type VARCHAR(20) NOT NULL DEFAULT 'BTREE',
                            reason TEXT NOT NULL,
                            estimated_benefit DECIMAL(5,2) NOT NULL DEFAULT 0,
                            create_sql TEXT NOT NULL,
                            status ENUM('pending', 'applied', 'rejected') DEFAULT 'pending',
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            applied_at TIMESTAMP NULL,
                            INDEX idx_table_name (table_name),
                            INDEX idx_status (status),
                            INDEX idx_created_at (created_at)
                        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
                    """
                    )

                    # æ€§èƒ½æŒ‡æ¨™è¨˜éŒ„è¡¨
                    await cursor.execute(
                        """
                        CREATE TABLE IF NOT EXISTS db_performance_metrics (
                            id INT AUTO_INCREMENT PRIMARY KEY,
                            query_cache_hit_rate DECIMAL(5,2),
                            slow_query_count INT,
                            connections_used INT,
                            max_connections INT,
                            innodb_buffer_pool_hit_rate DECIMAL(5,2),
                            table_scan_rate DECIMAL(5,2),
                            temp_table_rate DECIMAL(5,2),
                            key_read_hit_rate DECIMAL(5,2),
                            recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            INDEX idx_recorded_at (recorded_at)
                        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
                    """
                    )

                    await conn.commit()
                    logger.info("âœ… å„ªåŒ–ç›¸é—œè¡¨æ ¼å»ºç«‹å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ å»ºç«‹å„ªåŒ–è¡¨æ ¼å¤±æ•—: {e}")
            raise

    # ========== æŸ¥è©¢åˆ†æå’Œç›£æ§ ==========

    async def analyze_query(self, query: str, params: tuple = None) -> QueryAnalysis:
        """åˆ†æå–®å€‹æŸ¥è©¢"""
        start_time = time.time()
        query_hash = hashlib.sha256(query.encode()).hexdigest()[:32]  # ä½¿ç”¨ SHA256

        try:
            # åŸ·è¡Œ EXPLAIN åˆ†æ
            explain_result = await self._explain_query(query, params)

            # åˆ†ææŸ¥è©¢é¡å‹
            query_type = self._detect_query_type(query)

            # åŸ·è¡ŒæŸ¥è©¢ä¸¦æ¸¬é‡æ€§èƒ½
            execution_time, rows_examined, rows_sent = await self._execute_and_measure(
                query, params
            )

            # æå–è¡¨æ ¼å’Œç´¢å¼•ä¿¡æ¯
            tables_used = self._extract_tables_from_explain(explain_result)
            indexes_used = self._extract_indexes_from_explain(explain_result)

            # ç”Ÿæˆå„ªåŒ–å»ºè­°
            optimization_level, suggestions = (
                await self._generate_optimization_suggestions(
                    explain_result, execution_time, query_type, query
                )
            )

            # å‰µå»ºåˆ†æçµæœ
            analysis = QueryAnalysis(
                query_hash=query_hash,
                original_query=query,
                query_type=query_type,
                execution_time=execution_time,
                rows_examined=rows_examined,
                rows_sent=rows_sent,
                tables_used=tables_used,
                indexes_used=indexes_used,
                optimization_level=optimization_level,
                suggestions=suggestions,
                explain_plan=explain_result,
                timestamp=datetime.now(timezone.utc),
            )

            # è¨˜éŒ„åˆ†æçµæœ
            await self._log_query_analysis(analysis)

            # å¦‚æœæ˜¯æ…¢æŸ¥è©¢ï¼Œè¨˜éŒ„è­¦å‘Š
            if execution_time > self.slow_query_threshold:
                logger.warning(
                    f"ğŸŒ æ…¢æŸ¥è©¢æª¢æ¸¬: {execution_time:.3f}s - {query[:100]}..."
                )

            return analysis

        except Exception as e:
            logger.error(f"âŒ æŸ¥è©¢åˆ†æå¤±æ•—: {e}")
            # è¿”å›åŸºæœ¬åˆ†æçµæœ
            return QueryAnalysis(
                query_hash=query_hash,
                original_query=query,
                query_type=self._detect_query_type(query),
                execution_time=time.time() - start_time,
                rows_examined=0,
                rows_sent=0,
                tables_used=[],
                indexes_used=[],
                optimization_level=OptimizationLevel.LOW,
                suggestions=["æŸ¥è©¢åˆ†æå¤±æ•—ï¼Œè«‹æª¢æŸ¥èªæ³•"],
                explain_plan={},
                timestamp=datetime.now(timezone.utc),
            )

    async def _explain_query(self, query: str, params: tuple = None) -> Dict[str, Any]:
        """åŸ·è¡Œ EXPLAIN åˆ†æ"""
        try:
            async with db_pool.connection() as conn:
                async with conn.cursor() as cursor:
                    explain_query = f"EXPLAIN FORMAT=JSON {query}"
                    await cursor.execute(explain_query, params)
                    result = await cursor.fetchone()

                    if result and result[0]:
                        return json.loads(result[0])
                    return {}

        except Exception as e:
            logger.error(f"âŒ EXPLAIN åˆ†æå¤±æ•—: {e}")
            return {}

    async def _execute_and_measure(
        self, query: str, params: tuple = None
    ) -> Tuple[float, int, int]:
        """åŸ·è¡ŒæŸ¥è©¢ä¸¦æ¸¬é‡æ€§èƒ½"""
        start_time = time.time()
        rows_examined = 0
        rows_sent = 0

        try:
            async with db_pool.connection() as conn:
                async with conn.cursor() as cursor:
                    await cursor.execute(query, params)

                    # ç²å–åŸ·è¡Œçµ±è¨ˆ
                    await cursor.execute("SHOW SESSION STATUS LIKE 'Handler_read%'")
                    handler_stats = await cursor.fetchall()

                    for stat_name, stat_value in handler_stats:
                        if "read" in stat_name.lower():
                            rows_examined += int(stat_value)

                    rows_sent = cursor.rowcount if cursor.rowcount > 0 else 0

            execution_time = time.time() - start_time
            return execution_time, rows_examined, rows_sent

        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"âŒ æŸ¥è©¢åŸ·è¡Œæ¸¬é‡å¤±æ•—: {e}")
            return execution_time, 0, 0

    def _detect_query_type(self, query: str) -> QueryType:
        """æª¢æ¸¬æŸ¥è©¢é¡å‹"""
        query_upper = query.strip().upper()

        if query_upper.startswith("SELECT"):
            return QueryType.SELECT
        elif query_upper.startswith("INSERT"):
            return QueryType.INSERT
        elif query_upper.startswith("UPDATE"):
            return QueryType.UPDATE
        elif query_upper.startswith("DELETE"):
            return QueryType.DELETE
        elif query_upper.startswith("CREATE"):
            return QueryType.CREATE
        elif query_upper.startswith("ALTER"):
            return QueryType.ALTER
        else:
            return QueryType.SELECT  # é è¨­ç‚º SELECT

    def _extract_tables_from_explain(self, explain_result: Dict) -> List[str]:
        """å¾ EXPLAIN çµæœä¸­æå–è¡¨æ ¼åç¨±"""
        tables = []

        try:
            query_block = explain_result.get("query_block", {})

            # è™•ç†ç°¡å–®æŸ¥è©¢
            if "table" in query_block:
                table_info = query_block["table"]
                if isinstance(table_info, dict) and "table_name" in table_info:
                    tables.append(table_info["table_name"])

            # è™•ç†è¤‡é›œæŸ¥è©¢ï¼ˆåµŒå¥—ã€è¯æ¥ç­‰ï¼‰
            if "nested_loop" in query_block:
                nested_tables = self._extract_nested_tables(query_block["nested_loop"])
                tables.extend(nested_tables)

        except Exception as e:
            logger.debug(f"æå–è¡¨æ ¼åç¨±å¤±æ•—: {e}")

        return list(set(tables))  # å»é‡

    def _extract_indexes_from_explain(self, explain_result: Dict) -> List[str]:
        """å¾ EXPLAIN çµæœä¸­æå–ä½¿ç”¨çš„ç´¢å¼•"""
        indexes = []

        try:

            def extract_from_block(block):
                if isinstance(block, dict):
                    if "key" in block and block["key"] != "NULL":
                        indexes.append(block["key"])

                    # éæ­¸è™•ç†å­å¡Š
                    for key, value in block.items():
                        if isinstance(value, (dict, list)):
                            extract_from_block(value)
                elif isinstance(block, list):
                    for item in block:
                        extract_from_block(item)

            extract_from_block(explain_result)

        except Exception as e:
            logger.debug(f"æå–ç´¢å¼•åç¨±å¤±æ•—: {e}")

        return list(set(indexes))  # å»é‡

    def _extract_nested_tables(self, nested_loop: List) -> List[str]:
        """å¾åµŒå¥—å¾ªç’°ä¸­æå–è¡¨æ ¼"""
        tables = []

        for item in nested_loop:
            if isinstance(item, dict) and "table" in item:
                table_info = item["table"]
                if "table_name" in table_info:
                    tables.append(table_info["table_name"])

        return tables

    async def _generate_optimization_suggestions(
        self,
        explain_result: Dict,
        execution_time: float,
        query_type: QueryType,
        query: str,
    ) -> Tuple[OptimizationLevel, List[str]]:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        suggestions = []
        optimization_level = OptimizationLevel.LOW

        try:
            # åŸºæ–¼åŸ·è¡Œæ™‚é–“åˆ¤æ–·å„ªåŒ–ç­‰ç´š
            if execution_time > 5.0:
                optimization_level = OptimizationLevel.CRITICAL
            elif execution_time > 2.0:
                optimization_level = OptimizationLevel.HIGH
            elif execution_time > 1.0:
                optimization_level = OptimizationLevel.MEDIUM

            # åˆ†æ EXPLAIN çµæœ
            query_block = explain_result.get("query_block", {})

            # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨äº†ç´¢å¼•
            if self._has_full_table_scan(query_block):
                suggestions.append("æª¢æ¸¬åˆ°å…¨è¡¨æƒæï¼Œå»ºè­°æ·»åŠ é©ç•¶çš„ç´¢å¼•")
                optimization_level = max(optimization_level, OptimizationLevel.HIGH)

            # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨äº†è‡¨æ™‚è¡¨
            if self._uses_temporary_table(query_block):
                suggestions.append("æŸ¥è©¢ä½¿ç”¨äº†è‡¨æ™‚è¡¨ï¼Œè€ƒæ…®å„ªåŒ–æ’åºæˆ–åˆ†çµ„æ¢ä»¶")
                optimization_level = max(optimization_level, OptimizationLevel.MEDIUM)

            # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨äº†æª”æ¡ˆæ’åº
            if self._uses_filesort(query_block):
                suggestions.append("æª¢æ¸¬åˆ°æª”æ¡ˆæ’åºï¼Œå»ºè­°ç‚ºæ’åºæ¬„ä½æ·»åŠ ç´¢å¼•")
                optimization_level = max(optimization_level, OptimizationLevel.MEDIUM)

            # æª¢æŸ¥æŸ¥è©¢çµæ§‹
            if query_type == QueryType.SELECT:
                if re.search(r'\bLIKE\s+[\'"]%.*%[\'"]', query, re.IGNORECASE):
                    suggestions.append(
                        "é¿å…ä½¿ç”¨å‰å°è¬ç”¨å­—å…ƒçš„ LIKE æŸ¥è©¢ï¼Œè€ƒæ…®ä½¿ç”¨å…¨æ–‡æœç´¢"
                    )

                if re.search(r"\bORDER BY\b.*\bRAND\(\)", query, re.IGNORECASE):
                    suggestions.append("é¿å…ä½¿ç”¨ ORDER BY RAND()ï¼Œè€ƒæ…®å…¶ä»–éš¨æ©ŸåŒ–æ–¹æ¡ˆ")

            # é è¨­å»ºè­°
            if not suggestions:
                if execution_time > self.slow_query_threshold:
                    suggestions.append("æŸ¥è©¢åŸ·è¡Œæ™‚é–“è¼ƒé•·ï¼Œå»ºè­°æª¢æŸ¥è¡¨çµæ§‹å’Œç´¢å¼•ä½¿ç”¨")
                else:
                    suggestions.append("æŸ¥è©¢æ€§èƒ½è‰¯å¥½")

        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå„ªåŒ–å»ºè­°å¤±æ•—: {e}")
            suggestions = ["åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼Œå»ºè­°æ‰‹å‹•æª¢æŸ¥æŸ¥è©¢"]

        return optimization_level, suggestions

    def _has_full_table_scan(self, query_block: Dict) -> bool:
        """æª¢æŸ¥æ˜¯å¦æœ‰å…¨è¡¨æƒæ"""
        try:

            def check_block(block):
                if isinstance(block, dict):
                    if "access_type" in block and block["access_type"] == "ALL":
                        return True

                    for value in block.values():
                        if isinstance(value, (dict, list)) and check_block(value):
                            return True
                elif isinstance(block, list):
                    for item in block:
                        if check_block(item):
                            return True
                return False

            return check_block(query_block)
        except:
            return False

    def _uses_temporary_table(self, query_block: Dict) -> bool:
        """æª¢æŸ¥æ˜¯å¦ä½¿ç”¨è‡¨æ™‚è¡¨"""
        try:
            return "using_temporary_table" in json.dumps(query_block).lower()
        except:
            return False

    def _uses_filesort(self, query_block: Dict) -> bool:
        """æª¢æŸ¥æ˜¯å¦ä½¿ç”¨æª”æ¡ˆæ’åº"""
        try:
            return "using_filesort" in json.dumps(query_block).lower()
        except:
            return False

    async def _log_query_analysis(self, analysis: QueryAnalysis):
        """è¨˜éŒ„æŸ¥è©¢åˆ†æçµæœ"""
        try:
            async with db_pool.connection() as conn:
                async with conn.cursor() as cursor:
                    await cursor.execute(
                        """
                        INSERT INTO query_analysis_log
                        (query_hash, query_text, query_type, execution_time,
                         rows_examined, rows_sent, tables_used, indexes_used,
                         optimization_level, suggestions, explain_plan)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """,
                        (
                            analysis.query_hash,
                            analysis.original_query,
                            analysis.query_type.value,
                            analysis.execution_time,
                            analysis.rows_examined,
                            analysis.rows_sent,
                            json.dumps(analysis.tables_used),
                            json.dumps(analysis.indexes_used),
                            analysis.optimization_level.value,
                            json.dumps(analysis.suggestions),
                            json.dumps(analysis.explain_plan),
                        ),
                    )
                    await conn.commit()

        except Exception as e:
            logger.error(f"âŒ è¨˜éŒ„æŸ¥è©¢åˆ†æå¤±æ•—: {e}")

    # ========== ç´¢å¼•ç®¡ç† ==========

    async def analyze_index_usage(
        self, table_name: str = None
    ) -> List[IndexRecommendation]:
        """åˆ†æç´¢å¼•ä½¿ç”¨ä¸¦ç”Ÿæˆå»ºè­°"""
        try:
            recommendations = []

            # ç²å–æ‰€æœ‰è¡¨æ ¼æˆ–æŒ‡å®šè¡¨æ ¼
            tables = await self._get_tables(table_name)

            for table in tables:
                # åˆ†æç¾æœ‰ç´¢å¼•ä½¿ç”¨æƒ…æ³
                index_stats = await self._get_index_statistics(table)

                # åˆ†ææŸ¥è©¢æ¨¡å¼
                query_patterns = await self._get_query_patterns(table)

                # ç”Ÿæˆç´¢å¼•å»ºè­°
                table_recommendations = await self._generate_index_recommendations(
                    table, index_stats, query_patterns
                )

                recommendations.extend(table_recommendations)

            return recommendations

        except Exception as e:
            logger.error(f"âŒ ç´¢å¼•åˆ†æå¤±æ•—: {e}")
            return []

    async def _get_tables(self, table_name: str = None) -> List[str]:
        """ç²å–è¡¨æ ¼æ¸…å–®"""
        try:
            async with db_pool.connection() as conn:
                async with conn.cursor() as cursor:
                    if table_name:
                        await cursor.execute(
                            """
                            SELECT table_name FROM information_schema.tables
                            WHERE table_schema = DATABASE() AND table_name = %s
                        """,
                            (table_name,),
                        )
                    else:
                        await cursor.execute(
                            """
                            SELECT table_name FROM information_schema.tables
                            WHERE table_schema = DATABASE() AND table_type = 'BASE TABLE'
                        """
                        )

                    return [row[0] for row in await cursor.fetchall()]

        except Exception as e:
            logger.error(f"âŒ ç²å–è¡¨æ ¼æ¸…å–®å¤±æ•—: {e}")
            return []

    async def _get_index_statistics(self, table_name: str) -> Dict[str, Any]:
        """ç²å–ç´¢å¼•çµ±è¨ˆè³‡è¨Š"""
        try:
            async with db_pool.connection() as conn:
                async with conn.cursor() as cursor:
                    # ç²å–ç´¢å¼•è³‡è¨Š
                    await cursor.execute(
                        """
                        SELECT
                            index_name,
                            column_name,
                            cardinality,
                            sub_part,
                            nullable,
                            index_type
                        FROM information_schema.statistics
                        WHERE table_schema = DATABASE() AND table_name = %s
                        ORDER BY index_name, seq_in_index
                    """,
                        (table_name,),
                    )

                    index_info = await cursor.fetchall()

                    # ç²å–ç´¢å¼•ä½¿ç”¨çµ±è¨ˆ
                    await cursor.execute(
                        """
                        SELECT
                            index_name,
                            COUNT(*) as usage_count
                        FROM information_schema.statistics s
                        WHERE table_schema = DATABASE() AND table_name = %s
                        GROUP BY index_name
                    """,
                        (table_name,),
                    )

                    usage_stats = dict(await cursor.fetchall())

                    return {"index_info": index_info, "usage_stats": usage_stats}

        except Exception as e:
            logger.error(f"âŒ ç²å–ç´¢å¼•çµ±è¨ˆå¤±æ•— {table_name}: {e}")
            return {}

    async def _get_query_patterns(self, table_name: str) -> List[Dict[str, Any]]:
        """ç²å–æŸ¥è©¢æ¨¡å¼"""
        try:
            async with db_pool.connection() as conn:
                async with conn.cursor() as cursor:
                    # å¾æŸ¥è©¢åˆ†æè¨˜éŒ„ä¸­ç²å–æ¨¡å¼
                    await cursor.execute(
                        """
                        SELECT
                            query_text,
                            COUNT(*) as frequency,
                            AVG(execution_time) as avg_execution_time,
                            MAX(execution_time) as max_execution_time
                        FROM query_analysis_log
                        WHERE JSON_CONTAINS(tables_used, %s)
                        AND created_at > DATE_SUB(NOW(), INTERVAL 7 DAY)
                        GROUP BY query_hash
                        ORDER BY frequency DESC, avg_execution_time DESC
                        LIMIT 50
                    """,
                        (json.dumps([table_name]),),
                    )

                    patterns = []
                    for row in await cursor.fetchall():
                        patterns.append(
                            {
                                "query": row[0],
                                "frequency": row[1],
                                "avg_time": row[2],
                                "max_time": row[3],
                            }
                        )

                    return patterns

        except Exception as e:
            logger.error(f"âŒ ç²å–æŸ¥è©¢æ¨¡å¼å¤±æ•— {table_name}: {e}")
            return []

    async def _generate_index_recommendations(
        self, table_name: str, index_stats: Dict, query_patterns: List[Dict]
    ) -> List[IndexRecommendation]:
        """ç”Ÿæˆç´¢å¼•å»ºè­°"""
        recommendations = []

        try:
            # åˆ†æ WHERE æ¢ä»¶ä¸­å¸¸ç”¨çš„æ¬„ä½
            where_columns = self._extract_where_columns(query_patterns)

            # åˆ†æ ORDER BY ä¸­å¸¸ç”¨çš„æ¬„ä½
            self._extract_order_columns(query_patterns)

            # åˆ†æ JOIN æ¢ä»¶ä¸­çš„æ¬„ä½
            self._extract_join_columns(query_patterns)

            # ç²å–ç¾æœ‰ç´¢å¼•
            existing_indexes = self._get_existing_indexes(index_stats)

            # ç”Ÿæˆ WHERE æ¢ä»¶ç´¢å¼•å»ºè­°
            for column, frequency in where_columns.items():
                if not self._has_index_on_column(existing_indexes, column):
                    recommendation = IndexRecommendation(
                        table_name=table_name,
                        columns=[column],
                        index_type="BTREE",
                        reason=f"WHERE æ¢ä»¶ä¸­é«˜é »ä½¿ç”¨ ({frequency} æ¬¡)",
                        estimated_benefit=min(frequency * 0.1, 10.0),
                        create_sql=f"CREATE INDEX idx_{table_name}_{column} ON {table_name} ({column})",
                    )
                    recommendations.append(recommendation)

            # ç”Ÿæˆè¤‡åˆç´¢å¼•å»ºè­°
            composite_candidates = self._identify_composite_index_candidates(
                query_patterns
            )
            for columns in composite_candidates:
                if not self._has_composite_index(existing_indexes, columns):
                    columns_str = "_".join(columns)
                    recommendation = IndexRecommendation(
                        table_name=table_name,
                        columns=columns,
                        index_type="BTREE",
                        reason="è¤‡åˆæŸ¥è©¢æ¢ä»¶å„ªåŒ–",
                        estimated_benefit=5.0,
                        create_sql=f"CREATE INDEX idx_{table_name}_{columns_str} ON {table_name} ({', '.join(columns)})",
                    )
                    recommendations.append(recommendation)

        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆç´¢å¼•å»ºè­°å¤±æ•— {table_name}: {e}")

        return recommendations

    def _extract_where_columns(self, query_patterns: List[Dict]) -> Dict[str, int]:
        """æå– WHERE æ¢ä»¶ä¸­çš„æ¬„ä½"""
        columns = {}

        for pattern in query_patterns:
            query = pattern["query"].upper()
            frequency = pattern["frequency"]

            # ç°¡å–®çš„æ­£å‰‡åŒ¹é… WHERE æ¢ä»¶
            where_matches = re.findall(r"WHERE\s+.*?(\w+)\s*[=<>!]", query)
            and_matches = re.findall(r"AND\s+(\w+)\s*[=<>!]", query)

            for match in where_matches + and_matches:
                columns[match] = columns.get(match, 0) + frequency

        return columns

    def _extract_order_columns(self, query_patterns: List[Dict]) -> Dict[str, int]:
        """æå– ORDER BY ä¸­çš„æ¬„ä½"""
        columns = {}

        for pattern in query_patterns:
            query = pattern["query"].upper()
            frequency = pattern["frequency"]

            order_matches = re.findall(r"ORDER\s+BY\s+(\w+)", query)

            for match in order_matches:
                columns[match] = columns.get(match, 0) + frequency

        return columns

    def _extract_join_columns(self, query_patterns: List[Dict]) -> Dict[str, int]:
        """æå– JOIN æ¢ä»¶ä¸­çš„æ¬„ä½"""
        columns = {}

        for pattern in query_patterns:
            query = pattern["query"].upper()
            frequency = pattern["frequency"]

            join_matches = re.findall(r"JOIN\s+\w+\s+ON\s+.*?(\w+)\s*=", query)

            for match in join_matches:
                columns[match] = columns.get(match, 0) + frequency

        return columns

    def _get_existing_indexes(self, index_stats: Dict) -> List[Dict]:
        """ç²å–ç¾æœ‰ç´¢å¼•è³‡è¨Š"""
        indexes = []

        try:
            index_info = index_stats.get("index_info", [])

            current_index = None
            for info in index_info:
                index_name, column_name = info[0], info[1]

                if current_index is None or current_index["name"] != index_name:
                    if current_index:
                        indexes.append(current_index)

                    current_index = {"name": index_name, "columns": [column_name]}
                else:
                    current_index["columns"].append(column_name)

            if current_index:
                indexes.append(current_index)

        except Exception as e:
            logger.error(f"âŒ è§£æç¾æœ‰ç´¢å¼•å¤±æ•—: {e}")

        return indexes

    def _has_index_on_column(self, existing_indexes: List[Dict], column: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨è©²æ¬„ä½çš„ç´¢å¼•"""
        for index in existing_indexes:
            if column in index["columns"]:
                return True
        return False

    def _has_composite_index(
        self, existing_indexes: List[Dict], columns: List[str]
    ) -> bool:
        """æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨è¤‡åˆç´¢å¼•"""
        for index in existing_indexes:
            if set(columns).issubset(set(index["columns"])):
                return True
        return False

    def _identify_composite_index_candidates(
        self, query_patterns: List[Dict]
    ) -> List[List[str]]:
        """è­˜åˆ¥è¤‡åˆç´¢å¼•å€™é¸"""
        candidates = []

        for pattern in query_patterns:
            query = pattern["query"].upper()

            # æ‰¾å‡ºåŒæ™‚å‡ºç¾åœ¨ WHERE æ¢ä»¶ä¸­çš„æ¬„ä½
            where_pattern = r"WHERE\s+(.*?)(?:ORDER\s+BY|GROUP\s+BY|HAVING|LIMIT|$)"
            where_match = re.search(where_pattern, query)

            if where_match:
                where_clause = where_match.group(1)
                columns = re.findall(r"(\w+)\s*[=<>!]", where_clause)

                if len(columns) > 1:
                    candidates.append(columns[:3])  # æœ€å¤š3å€‹æ¬„ä½çš„è¤‡åˆç´¢å¼•

        return candidates

    # ========== æ€§èƒ½ç›£æ§ ==========

    async def collect_database_metrics(self) -> DatabaseMetrics:
        """æ”¶é›†è³‡æ–™åº«æ€§èƒ½æŒ‡æ¨™"""
        try:
            async with db_pool.connection() as conn:
                async with conn.cursor() as cursor:
                    # ç²å–å„ç¨®æ€§èƒ½æŒ‡æ¨™
                    metrics_queries = {
                        "query_cache_hit_rate": """
                            SELECT
                                ROUND(
                                    100 * (1 - Qcache_hits / (Qcache_hits + Qcache_inserts)), 2
                                ) as hit_rate
                            FROM
                                (SELECT
                                    VARIABLE_VALUE as Qcache_hits
                                 FROM information_schema.GLOBAL_STATUS
                                 WHERE VARIABLE_NAME = 'Qcache_hits') qh,
                                (SELECT
                                    VARIABLE_VALUE as Qcache_inserts
                                 FROM information_schema.GLOBAL_STATUS
                                 WHERE VARIABLE_NAME = 'Qcache_inserts') qi
                        """,
                        "slow_queries": """
                            SELECT VARIABLE_VALUE
                            FROM information_schema.GLOBAL_STATUS
                            WHERE VARIABLE_NAME = 'Slow_queries'
                        """,
                        "connections": """
                            SELECT
                                (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Threads_connected') as used,
                                (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_VARIABLES WHERE VARIABLE_NAME = 'max_connections') as max_conn
                        """,
                        "innodb_buffer_pool": """
                            SELECT
                                ROUND(
                                    100 * (
                                        (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Innodb_buffer_pool_read_requests') -
                                        (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Innodb_buffer_pool_reads')
                                    ) /
                                    (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Innodb_buffer_pool_read_requests'), 2
                                ) as buffer_hit_rate
                        """,
                    }

                    metrics_data = {}

                    for metric_name, query in metrics_queries.items():
                        try:
                            await cursor.execute(query)
                            result = await cursor.fetchone()

                            if metric_name == "connections":
                                metrics_data["connections_used"] = (
                                    int(result[0]) if result else 0
                                )
                                metrics_data["max_connections"] = (
                                    int(result[1]) if result else 0
                                )
                            else:
                                metrics_data[metric_name] = (
                                    float(result[0]) if result and result[0] else 0.0
                                )

                        except Exception as e:
                            logger.warning(f"âš ï¸ æ”¶é›†æŒ‡æ¨™å¤±æ•— {metric_name}: {e}")
                            metrics_data[metric_name] = 0.0

                    # å‰µå»ºæŒ‡æ¨™ç‰©ä»¶
                    metrics = DatabaseMetrics(
                        query_cache_hit_rate=metrics_data.get(
                            "query_cache_hit_rate", 0.0
                        ),
                        slow_query_count=int(metrics_data.get("slow_queries", 0)),
                        connections_used=metrics_data.get("connections_used", 0),
                        max_connections=metrics_data.get("max_connections", 0),
                        innodb_buffer_pool_hit_rate=metrics_data.get(
                            "innodb_buffer_pool", 0.0
                        ),
                        table_scan_rate=0.0,  # éœ€è¦é¡å¤–è¨ˆç®—
                        temp_table_rate=0.0,  # éœ€è¦é¡å¤–è¨ˆç®—
                        key_read_hit_rate=0.0,  # éœ€è¦é¡å¤–è¨ˆç®—
                        timestamp=datetime.now(timezone.utc),
                    )

                    # å¿«å–æŒ‡æ¨™
                    self.metrics_cache = metrics
                    self.metrics_cache_time = datetime.now(timezone.utc)

                    return metrics

        except Exception as e:
            logger.error(f"âŒ æ”¶é›†è³‡æ–™åº«æŒ‡æ¨™å¤±æ•—: {e}")
            return DatabaseMetrics(
                query_cache_hit_rate=0.0,
                slow_query_count=0,
                connections_used=0,
                max_connections=0,
                innodb_buffer_pool_hit_rate=0.0,
                table_scan_rate=0.0,
                temp_table_rate=0.0,
                key_read_hit_rate=0.0,
                timestamp=datetime.now(timezone.utc),
            )

    async def get_optimization_summary(self, days: int = 7) -> Dict[str, Any]:
        """ç²å–å„ªåŒ–æ‘˜è¦å ±å‘Š"""
        try:
            async with db_pool.connection() as conn:
                async with conn.cursor() as cursor:
                    # æ…¢æŸ¥è©¢çµ±è¨ˆ
                    await cursor.execute(
                        """
                        SELECT
                            COUNT(*) as total_slow_queries,
                            AVG(execution_time) as avg_slow_time,
                            MAX(execution_time) as max_slow_time,
                            COUNT(DISTINCT query_hash) as unique_slow_queries
                        FROM query_analysis_log
                        WHERE execution_time > %s
                        AND created_at > DATE_SUB(NOW(), INTERVAL %s DAY)
                    """,
                        (self.slow_query_threshold, days),
                    )

                    slow_query_stats = await cursor.fetchone()

                    # å„ªåŒ–ç­‰ç´šåˆ†ä½ˆ
                    await cursor.execute(
                        """
                        SELECT
                            optimization_level,
                            COUNT(*) as count
                        FROM query_analysis_log
                        WHERE created_at > DATE_SUB(NOW(), INTERVAL %s DAY)
                        GROUP BY optimization_level
                    """,
                        (days,),
                    )

                    optimization_levels = dict(await cursor.fetchall())

                    # æœ€é »ç¹çš„æŸ¥è©¢
                    await cursor.execute(
                        """
                        SELECT
                            query_text,
                            COUNT(*) as frequency,
                            AVG(execution_time) as avg_time,
                            optimization_level
                        FROM query_analysis_log
                        WHERE created_at > DATE_SUB(NOW(), INTERVAL %s DAY)
                        GROUP BY query_hash
                        ORDER BY frequency DESC
                        LIMIT 10
                    """,
                        (days,),
                    )

                    frequent_queries = await cursor.fetchall()

                    # ç´¢å¼•å»ºè­°çµ±è¨ˆ
                    await cursor.execute(
                        """
                        SELECT
                            status,
                            COUNT(*) as count,
                            AVG(estimated_benefit) as avg_benefit
                        FROM index_recommendations
                        WHERE created_at > DATE_SUB(NOW(), INTERVAL %s DAY)
                        GROUP BY status
                    """,
                        (days,),
                    )

                    index_stats = await cursor.fetchall()

                    return {
                        "period_days": days,
                        "slow_queries": {
                            "total": slow_query_stats[0] if slow_query_stats else 0,
                            "avg_time": (
                                float(slow_query_stats[1])
                                if slow_query_stats and slow_query_stats[1]
                                else 0
                            ),
                            "max_time": (
                                float(slow_query_stats[2])
                                if slow_query_stats and slow_query_stats[2]
                                else 0
                            ),
                            "unique_count": (
                                slow_query_stats[3] if slow_query_stats else 0
                            ),
                        },
                        "optimization_levels": optimization_levels,
                        "frequent_queries": [
                            {
                                "query": (
                                    query[:100] + "..." if len(query) > 100 else query
                                ),
                                "frequency": freq,
                                "avg_time": float(avg_time),
                                "level": level,
                            }
                            for query, freq, avg_time, level in frequent_queries
                        ],
                        "index_recommendations": dict(index_stats),
                        "timestamp": datetime.now(timezone.utc).isoformat(),
                    }

        except Exception as e:
            logger.error(f"âŒ ç²å–å„ªåŒ–æ‘˜è¦å¤±æ•—: {e}")
            return {}

    async def _load_optimization_history(self):
        """è¼‰å…¥å„ªåŒ–æ­·å²"""
        # å¯¦ç¾è¼‰å…¥æ­·å²è¨˜éŒ„çš„é‚è¼¯

    async def _analyze_existing_tables(self):
        """åˆ†æç¾æœ‰è¡¨çµæ§‹"""
        # å¯¦ç¾åˆ†æè¡¨çµæ§‹çš„é‚è¼¯


# ========== å…¨åŸŸå¯¦ä¾‹ ==========

# å‰µå»ºå…¨åŸŸè³‡æ–™åº«å„ªåŒ–å™¨å¯¦ä¾‹
db_optimizer = DatabaseOptimizer()


async def get_db_optimizer() -> DatabaseOptimizer:
    """ç²å–è³‡æ–™åº«å„ªåŒ–å™¨å¯¦ä¾‹"""
    return db_optimizer


# è£é£¾å™¨ï¼šè‡ªå‹•åˆ†ææŸ¥è©¢æ€§èƒ½
def query_analyzed(func):
    """æŸ¥è©¢åˆ†æè£é£¾å™¨"""

    async def wrapper(*args, **kwargs):
        start_time = time.time()

        try:
            result = await func(*args, **kwargs)

            # å¦‚æœåŸ·è¡Œæ™‚é–“è¶…éé–¾å€¼ï¼Œè¨˜éŒ„åˆ†æ
            execution_time = time.time() - start_time
            if execution_time > db_optimizer.slow_query_threshold:
                # é€™è£¡éœ€è¦ç²å–å¯¦éš›çš„æŸ¥è©¢èªå¥ï¼Œå¯èƒ½éœ€è¦ä¿®æ”¹å‡½æ•¸ç°½å
                logger.warning(f"æ…¢æŸ¥è©¢æª¢æ¸¬: {func.__name__} - {execution_time:.3f}s")

            return result

        except Exception as e:
            logger.error(f"æŸ¥è©¢åŸ·è¡Œå¤±æ•— {func.__name__}: {e}")
            raise

    return wrapper
