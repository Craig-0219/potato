# shared/db_optimizer.py - Ë≥áÊñôÂ∫´Êü•Ë©¢ÂÑ™ÂåñÁÆ°ÁêÜÂô®
"""
Ë≥áÊñôÂ∫´Êü•Ë©¢ÂÑ™ÂåñÁÆ°ÁêÜÂô® v2.2.0
ÂäüËÉΩÁâπÈªûÔºö
1. Êü•Ë©¢Âü∑Ë°åË®àÂäÉÂàÜÊûê
2. Ëá™ÂãïÁ¥¢ÂºïÂª∫Ë≠∞ÂíåÁÆ°ÁêÜ
3. ÊÖ¢Êü•Ë©¢Ê™¢Ê∏¨ÂíåÂÑ™Âåñ
4. Ë≥áÊñôÂ∫´ÊÄßËÉΩÁõ£Êéß
5. Êü•Ë©¢Âø´ÂèñÁÆ°ÁêÜ
6. Ë≥áÊñôÂ∫´Áµ±Ë®àÊõ¥Êñ∞
"""

import hashlib
import json
import re
import time
from dataclasses import dataclass
from datetime import datetime, timezone
from enum import Enum
from typing import Any, Dict, List, Tuple

from bot.db.pool import db_pool
from shared.logger import logger


class QueryType(Enum):
    """Êü•Ë©¢È°ûÂûã"""

    SELECT = "SELECT"
    INSERT = "INSERT"
    UPDATE = "UPDATE"
    DELETE = "DELETE"
    CREATE = "CREATE"
    ALTER = "ALTER"


class OptimizationLevel(Enum):
    """ÂÑ™ÂåñÁ≠âÁ¥ö"""

    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


@dataclass
class QueryAnalysis:
    """Êü•Ë©¢ÂàÜÊûêÁµêÊûú"""

    query_hash: str
    original_query: str
    query_type: QueryType
    execution_time: float
    rows_examined: int
    rows_sent: int
    tables_used: List[str]
    indexes_used: List[str]
    optimization_level: OptimizationLevel
    suggestions: List[str]
    explain_plan: Dict[str, Any]
    timestamp: datetime


@dataclass
class IndexRecommendation:
    """Á¥¢ÂºïÂª∫Ë≠∞"""

    table_name: str
    columns: List[str]
    index_type: str  # BTREE, HASH, FULLTEXT
    reason: str
    estimated_benefit: float
    create_sql: str


@dataclass
class DatabaseMetrics:
    """Ë≥áÊñôÂ∫´ÊÄßËÉΩÊåáÊ®ô"""

    query_cache_hit_rate: float
    slow_query_count: int
    connections_used: int
    max_connections: int
    innodb_buffer_pool_hit_rate: float
    table_scan_rate: float
    temp_table_rate: float
    key_read_hit_rate: float
    timestamp: datetime


class DatabaseOptimizer:
    """Ë≥áÊñôÂ∫´ÂÑ™ÂåñÁÆ°ÁêÜÂô®"""

    def __init__(self):
        self.slow_query_threshold = 1.0  # 1ÁßíÁÇ∫ÊÖ¢Êü•Ë©¢ÈñæÂÄº
        self.query_cache = {}
        self.optimization_history = []
        self.metrics_cache = None
        self.metrics_cache_time = None

        # Á¥¢ÂºïÂÑ™ÂåñÈÖçÁΩÆ
        self.index_recommendations = []
        self.auto_create_indexes = False  # È†êË®≠‰∏çËá™ÂãïÂâµÂª∫Á¥¢Âºï

        logger.info("üîß Ë≥áÊñôÂ∫´ÂÑ™ÂåñÁÆ°ÁêÜÂô®ÂàùÂßãÂåñÂÆåÊàê")

    async def initialize(self):
        """ÂàùÂßãÂåñÂÑ™ÂåñÂô®"""
        try:
            # Ê™¢Êü•‰∏¶Âª∫Á´ãÂÑ™ÂåñÁõ∏ÈóúÁöÑË°®Ê†º
            await self._create_optimization_tables()

            # ËºâÂÖ•ÁèæÊúâÁöÑÂÑ™ÂåñË®òÈåÑ
            await self._load_optimization_history()

            # ÂàÜÊûêÁèæÊúâË°®ÁµêÊßã
            await self._analyze_existing_tables()

            logger.info("‚úÖ Ë≥áÊñôÂ∫´ÂÑ™ÂåñÂô®ÂàùÂßãÂåñÂÆåÊàê")

        except Exception as e:
            logger.error(f"‚ùå Ë≥áÊñôÂ∫´ÂÑ™ÂåñÂô®ÂàùÂßãÂåñÂ§±Êïó: {e}")
            raise

    async def _create_optimization_tables(self):
        """Âª∫Á´ãÂÑ™ÂåñÁõ∏ÈóúÁöÑË°®Ê†º"""
        try:
            async with db_pool.connection() as conn:
                async with conn.cursor() as cursor:
                    # Êü•Ë©¢ÂàÜÊûêË®òÈåÑË°®
                    await cursor.execute(
                        """
                        CREATE TABLE IF NOT EXISTS query_analysis_log (
                            id INT AUTO_INCREMENT PRIMARY KEY,
                            query_hash VARCHAR(64) NOT NULL,
                            query_text TEXT NOT NULL,
                            query_type VARCHAR(20) NOT NULL,
                            execution_time DECIMAL(10,4) NOT NULL,
                            rows_examined INT NOT NULL DEFAULT 0,
                            rows_sent INT NOT NULL DEFAULT 0,
                            tables_used JSON,
                            indexes_used JSON,
                            optimization_level VARCHAR(20) NOT NULL,
                            suggestions JSON,
                            explain_plan JSON,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            INDEX idx_query_hash (query_hash),
                            INDEX idx_execution_time (execution_time),
                            INDEX idx_created_at (created_at)
                        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
                    """
                    )

                    # Á¥¢ÂºïÂª∫Ë≠∞Ë®òÈåÑË°®
                    await cursor.execute(
                        """
                        CREATE TABLE IF NOT EXISTS index_recommendations (
                            id INT AUTO_INCREMENT PRIMARY KEY,
                            table_name VARCHAR(100) NOT NULL,
                            columns JSON NOT NULL,
                            index_type VARCHAR(20) NOT NULL DEFAULT 'BTREE',
                            reason TEXT NOT NULL,
                            estimated_benefit DECIMAL(5,2) NOT NULL DEFAULT 0,
                            create_sql TEXT NOT NULL,
                            status ENUM('pending', 'applied', 'rejected') DEFAULT 'pending',
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            applied_at TIMESTAMP NULL,
                            INDEX idx_table_name (table_name),
                            INDEX idx_status (status),
                            INDEX idx_created_at (created_at)
                        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
                    """
                    )

                    # ÊÄßËÉΩÊåáÊ®ôË®òÈåÑË°®
                    await cursor.execute(
                        """
                        CREATE TABLE IF NOT EXISTS db_performance_metrics (
                            id INT AUTO_INCREMENT PRIMARY KEY,
                            query_cache_hit_rate DECIMAL(5,2),
                            slow_query_count INT,
                            connections_used INT,
                            max_connections INT,
                            innodb_buffer_pool_hit_rate DECIMAL(5,2),
                            table_scan_rate DECIMAL(5,2),
                            temp_table_rate DECIMAL(5,2),
                            key_read_hit_rate DECIMAL(5,2),
                            recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            INDEX idx_recorded_at (recorded_at)
                        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
                    """
                    )

                    await conn.commit()
                    logger.info("‚úÖ ÂÑ™ÂåñÁõ∏ÈóúË°®Ê†ºÂª∫Á´ãÂÆåÊàê")

        except Exception as e:
            logger.error(f"‚ùå Âª∫Á´ãÂÑ™ÂåñË°®Ê†ºÂ§±Êïó: {e}")
            raise

    # ========== Êü•Ë©¢ÂàÜÊûêÂíåÁõ£Êéß ==========

    async def analyze_query(self, query: str, params: tuple = None) -> QueryAnalysis:
        """ÂàÜÊûêÂñÆÂÄãÊü•Ë©¢"""
        start_time = time.time()
        query_hash = hashlib.sha256(query.encode()).hexdigest()[:32]  # ‰ΩøÁî® SHA256

        try:
            # Âü∑Ë°å EXPLAIN ÂàÜÊûê
            explain_result = await self._explain_query(query, params)

            # ÂàÜÊûêÊü•Ë©¢È°ûÂûã
            query_type = self._detect_query_type(query)

            # Âü∑Ë°åÊü•Ë©¢‰∏¶Ê∏¨ÈáèÊÄßËÉΩ
            execution_time, rows_examined, rows_sent = await self._execute_and_measure(
                query, params
            )

            # ÊèêÂèñË°®Ê†ºÂíåÁ¥¢Âºï‰ø°ÊÅØ
            tables_used = self._extract_tables_from_explain(explain_result)
            indexes_used = self._extract_indexes_from_explain(explain_result)

            # ÁîüÊàêÂÑ™ÂåñÂª∫Ë≠∞
            optimization_level, suggestions = (
                await self._generate_optimization_suggestions(
                    explain_result, execution_time, query_type, query
                )
            )

            # ÂâµÂª∫ÂàÜÊûêÁµêÊûú
            analysis = QueryAnalysis(
                query_hash=query_hash,
                original_query=query,
                query_type=query_type,
                execution_time=execution_time,
                rows_examined=rows_examined,
                rows_sent=rows_sent,
                tables_used=tables_used,
                indexes_used=indexes_used,
                optimization_level=optimization_level,
                suggestions=suggestions,
                explain_plan=explain_result,
                timestamp=datetime.now(timezone.utc),
            )

            # Ë®òÈåÑÂàÜÊûêÁµêÊûú
            await self._log_query_analysis(analysis)

            # Â¶ÇÊûúÊòØÊÖ¢Êü•Ë©¢ÔºåË®òÈåÑË≠¶Âëä
            if execution_time > self.slow_query_threshold:
                logger.warning(
                    f"üêå ÊÖ¢Êü•Ë©¢Ê™¢Ê∏¨: {execution_time:.3f}s - {query[:100]}..."
                )

            return analysis

        except Exception as e:
            logger.error(f"‚ùå Êü•Ë©¢ÂàÜÊûêÂ§±Êïó: {e}")
            # ËøîÂõûÂü∫Êú¨ÂàÜÊûêÁµêÊûú
            return QueryAnalysis(
                query_hash=query_hash,
                original_query=query,
                query_type=self._detect_query_type(query),
                execution_time=time.time() - start_time,
                rows_examined=0,
                rows_sent=0,
                tables_used=[],
                indexes_used=[],
                optimization_level=OptimizationLevel.LOW,
                suggestions=["Êü•Ë©¢ÂàÜÊûêÂ§±ÊïóÔºåË´ãÊ™¢Êü•Ë™ûÊ≥ï"],
                explain_plan={},
                timestamp=datetime.now(timezone.utc),
            )

    async def _explain_query(self, query: str, params: tuple = None) -> Dict[str, Any]:
        """Âü∑Ë°å EXPLAIN ÂàÜÊûê"""
        try:
            async with db_pool.connection() as conn:
                async with conn.cursor() as cursor:
                    explain_query = f"EXPLAIN FORMAT=JSON {query}"
                    await cursor.execute(explain_query, params)
                    result = await cursor.fetchone()

                    if result and result[0]:
                        return json.loads(result[0])
                    return {}

        except Exception as e:
            logger.error(f"‚ùå EXPLAIN ÂàÜÊûêÂ§±Êïó: {e}")
            return {}

    async def _execute_and_measure(
        self, query: str, params: tuple = None
    ) -> Tuple[float, int, int]:
        """Âü∑Ë°åÊü•Ë©¢‰∏¶Ê∏¨ÈáèÊÄßËÉΩ"""
        start_time = time.time()
        rows_examined = 0
        rows_sent = 0

        try:
            async with db_pool.connection() as conn:
                async with conn.cursor() as cursor:
                    await cursor.execute(query, params)

                    # Áç≤ÂèñÂü∑Ë°åÁµ±Ë®à
                    await cursor.execute("SHOW SESSION STATUS LIKE 'Handler_read%'")
                    handler_stats = await cursor.fetchall()

                    for stat_name, stat_value in handler_stats:
                        if "read" in stat_name.lower():
                            rows_examined += int(stat_value)

                    rows_sent = cursor.rowcount if cursor.rowcount > 0 else 0

            execution_time = time.time() - start_time
            return execution_time, rows_examined, rows_sent

        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"‚ùå Êü•Ë©¢Âü∑Ë°åÊ∏¨ÈáèÂ§±Êïó: {e}")
            return execution_time, 0, 0

    def _detect_query_type(self, query: str) -> QueryType:
        """Ê™¢Ê∏¨Êü•Ë©¢È°ûÂûã"""
        query_upper = query.strip().upper()

        if query_upper.startswith("SELECT"):
            return QueryType.SELECT
        elif query_upper.startswith("INSERT"):
            return QueryType.INSERT
        elif query_upper.startswith("UPDATE"):
            return QueryType.UPDATE
        elif query_upper.startswith("DELETE"):
            return QueryType.DELETE
        elif query_upper.startswith("CREATE"):
            return QueryType.CREATE
        elif query_upper.startswith("ALTER"):
            return QueryType.ALTER
        else:
            return QueryType.SELECT  # È†êË®≠ÁÇ∫ SELECT

    def _extract_tables_from_explain(self, explain_result: Dict) -> List[str]:
        """Âæû EXPLAIN ÁµêÊûú‰∏≠ÊèêÂèñË°®Ê†ºÂêçÁ®±"""
        tables = []

        try:
            query_block = explain_result.get("query_block", {})

            # ËôïÁêÜÁ∞°ÂñÆÊü•Ë©¢
            if "table" in query_block:
                table_info = query_block["table"]
                if isinstance(table_info, dict) and "table_name" in table_info:
                    tables.append(table_info["table_name"])

            # ËôïÁêÜË§áÈõúÊü•Ë©¢ÔºàÂµåÂ•ó„ÄÅËÅØÊé•Á≠âÔºâ
            if "nested_loop" in query_block:
                nested_tables = self._extract_nested_tables(query_block["nested_loop"])
                tables.extend(nested_tables)

        except Exception as e:
            logger.debug(f"ÊèêÂèñË°®Ê†ºÂêçÁ®±Â§±Êïó: {e}")

        return list(set(tables))  # ÂéªÈáç

    def _extract_indexes_from_explain(self, explain_result: Dict) -> List[str]:
        """Âæû EXPLAIN ÁµêÊûú‰∏≠ÊèêÂèñ‰ΩøÁî®ÁöÑÁ¥¢Âºï"""
        indexes = []

        try:

            def extract_from_block(block):
                if isinstance(block, dict):
                    if "key" in block and block["key"] != "NULL":
                        indexes.append(block["key"])

                    # ÈÅûÊ≠∏ËôïÁêÜÂ≠êÂ°ä
                    for key, value in block.items():
                        if isinstance(value, (dict, list)):
                            extract_from_block(value)
                elif isinstance(block, list):
                    for item in block:
                        extract_from_block(item)

            extract_from_block(explain_result)

        except Exception as e:
            logger.debug(f"ÊèêÂèñÁ¥¢ÂºïÂêçÁ®±Â§±Êïó: {e}")

        return list(set(indexes))  # ÂéªÈáç

    def _extract_nested_tables(self, nested_loop: List) -> List[str]:
        """ÂæûÂµåÂ•óÂæ™Áí∞‰∏≠ÊèêÂèñË°®Ê†º"""
        tables = []

        for item in nested_loop:
            if isinstance(item, dict) and "table" in item:
                table_info = item["table"]
                if "table_name" in table_info:
                    tables.append(table_info["table_name"])

        return tables

    async def _generate_optimization_suggestions(
        self,
        explain_result: Dict,
        execution_time: float,
        query_type: QueryType,
        query: str,
    ) -> Tuple[OptimizationLevel, List[str]]:
        """ÁîüÊàêÂÑ™ÂåñÂª∫Ë≠∞"""
        suggestions = []
        optimization_level = OptimizationLevel.LOW

        try:
            # Âü∫ÊñºÂü∑Ë°åÊôÇÈñìÂà§Êñ∑ÂÑ™ÂåñÁ≠âÁ¥ö
            if execution_time > 5.0:
                optimization_level = OptimizationLevel.CRITICAL
            elif execution_time > 2.0:
                optimization_level = OptimizationLevel.HIGH
            elif execution_time > 1.0:
                optimization_level = OptimizationLevel.MEDIUM

            # ÂàÜÊûê EXPLAIN ÁµêÊûú
            query_block = explain_result.get("query_block", {})

            # Ê™¢Êü•ÊòØÂê¶‰ΩøÁî®‰∫ÜÁ¥¢Âºï
            if self._has_full_table_scan(query_block):
                suggestions.append("Ê™¢Ê∏¨Âà∞ÂÖ®Ë°®ÊéÉÊèèÔºåÂª∫Ë≠∞Ê∑ªÂä†ÈÅ©Áï∂ÁöÑÁ¥¢Âºï")
                optimization_level = max(optimization_level, OptimizationLevel.HIGH)

            # Ê™¢Êü•ÊòØÂê¶‰ΩøÁî®‰∫ÜËá®ÊôÇË°®
            if self._uses_temporary_table(query_block):
                suggestions.append("Êü•Ë©¢‰ΩøÁî®‰∫ÜËá®ÊôÇË°®ÔºåËÄÉÊÖÆÂÑ™ÂåñÊéíÂ∫èÊàñÂàÜÁµÑÊ¢ù‰ª∂")
                optimization_level = max(optimization_level, OptimizationLevel.MEDIUM)

            # Ê™¢Êü•ÊòØÂê¶‰ΩøÁî®‰∫ÜÊ™îÊ°àÊéíÂ∫è
            if self._uses_filesort(query_block):
                suggestions.append("Ê™¢Ê∏¨Âà∞Ê™îÊ°àÊéíÂ∫èÔºåÂª∫Ë≠∞ÁÇ∫ÊéíÂ∫èÊ¨Ñ‰ΩçÊ∑ªÂä†Á¥¢Âºï")
                optimization_level = max(optimization_level, OptimizationLevel.MEDIUM)

            # Ê™¢Êü•Êü•Ë©¢ÁµêÊßã
            if query_type == QueryType.SELECT:
                if re.search(r'\bLIKE\s+[\'"]%.*%[\'"]', query, re.IGNORECASE):
                    suggestions.append(
                        "ÈÅøÂÖç‰ΩøÁî®ÂâçÂ∞éËê¨Áî®Â≠óÂÖÉÁöÑ LIKE Êü•Ë©¢ÔºåËÄÉÊÖÆ‰ΩøÁî®ÂÖ®ÊñáÊêúÁ¥¢"
                    )

                if re.search(r"\bORDER BY\b.*\bRAND\(\)", query, re.IGNORECASE):
                    suggestions.append("ÈÅøÂÖç‰ΩøÁî® ORDER BY RAND()ÔºåËÄÉÊÖÆÂÖ∂‰ªñÈö®Ê©üÂåñÊñπÊ°à")

            # È†êË®≠Âª∫Ë≠∞
            if not suggestions:
                if execution_time > self.slow_query_threshold:
                    suggestions.append("Êü•Ë©¢Âü∑Ë°åÊôÇÈñìËºÉÈï∑ÔºåÂª∫Ë≠∞Ê™¢Êü•Ë°®ÁµêÊßãÂíåÁ¥¢Âºï‰ΩøÁî®")
                else:
                    suggestions.append("Êü•Ë©¢ÊÄßËÉΩËâØÂ•Ω")

        except Exception as e:
            logger.error(f"‚ùå ÁîüÊàêÂÑ™ÂåñÂª∫Ë≠∞Â§±Êïó: {e}")
            suggestions = ["ÂàÜÊûêÈÅéÁ®ã‰∏≠ÁôºÁîüÈåØË™§ÔºåÂª∫Ë≠∞ÊâãÂãïÊ™¢Êü•Êü•Ë©¢"]

        return optimization_level, suggestions

    def _has_full_table_scan(self, query_block: Dict) -> bool:
        """Ê™¢Êü•ÊòØÂê¶ÊúâÂÖ®Ë°®ÊéÉÊèè"""
        try:

            def check_block(block):
                if isinstance(block, dict):
                    if "access_type" in block and block["access_type"] == "ALL":
                        return True

                    for value in block.values():
                        if isinstance(value, (dict, list)) and check_block(value):
                            return True
                elif isinstance(block, list):
                    for item in block:
                        if check_block(item):
                            return True
                return False

            return check_block(query_block)
        except:
            return False

    def _uses_temporary_table(self, query_block: Dict) -> bool:
        """Ê™¢Êü•ÊòØÂê¶‰ΩøÁî®Ëá®ÊôÇË°®"""
        try:
            return "using_temporary_table" in json.dumps(query_block).lower()
        except:
            return False

    def _uses_filesort(self, query_block: Dict) -> bool:
        """Ê™¢Êü•ÊòØÂê¶‰ΩøÁî®Ê™îÊ°àÊéíÂ∫è"""
        try:
            return "using_filesort" in json.dumps(query_block).lower()
        except:
            return False

    async def _log_query_analysis(self, analysis: QueryAnalysis):
        """Ë®òÈåÑÊü•Ë©¢ÂàÜÊûêÁµêÊûú"""
        try:
            async with db_pool.connection() as conn:
                async with conn.cursor() as cursor:
                    await cursor.execute(
                        """
                        INSERT INTO query_analysis_log
                        (query_hash, query_text, query_type, execution_time,
                         rows_examined, rows_sent, tables_used, indexes_used,
                         optimization_level, suggestions, explain_plan)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """,
                        (
                            analysis.query_hash,
                            analysis.original_query,
                            analysis.query_type.value,
                            analysis.execution_time,
                            analysis.rows_examined,
                            analysis.rows_sent,
                            json.dumps(analysis.tables_used),
                            json.dumps(analysis.indexes_used),
                            analysis.optimization_level.value,
                            json.dumps(analysis.suggestions),
                            json.dumps(analysis.explain_plan),
                        ),
                    )
                    await conn.commit()

        except Exception as e:
            logger.error(f"‚ùå Ë®òÈåÑÊü•Ë©¢ÂàÜÊûêÂ§±Êïó: {e}")

    # ========== Á¥¢ÂºïÁÆ°ÁêÜ ==========

    async def analyze_index_usage(
        self, table_name: str = None
    ) -> List[IndexRecommendation]:
        """ÂàÜÊûêÁ¥¢Âºï‰ΩøÁî®‰∏¶ÁîüÊàêÂª∫Ë≠∞"""
        try:
            recommendations = []

            # Áç≤ÂèñÊâÄÊúâË°®Ê†ºÊàñÊåáÂÆöË°®Ê†º
            tables = await self._get_tables(table_name)

            for table in tables:
                # ÂàÜÊûêÁèæÊúâÁ¥¢Âºï‰ΩøÁî®ÊÉÖÊ≥Å
                index_stats = await self._get_index_statistics(table)

                # ÂàÜÊûêÊü•Ë©¢Ê®°Âºè
                query_patterns = await self._get_query_patterns(table)

                # ÁîüÊàêÁ¥¢ÂºïÂª∫Ë≠∞
                table_recommendations = await self._generate_index_recommendations(
                    table, index_stats, query_patterns
                )

                recommendations.extend(table_recommendations)

            return recommendations

        except Exception as e:
            logger.error(f"‚ùå Á¥¢ÂºïÂàÜÊûêÂ§±Êïó: {e}")
            return []

    async def _get_tables(self, table_name: str = None) -> List[str]:
        """Áç≤ÂèñË°®Ê†ºÊ∏ÖÂñÆ"""
        try:
            async with db_pool.connection() as conn:
                async with conn.cursor() as cursor:
                    if table_name:
                        await cursor.execute(
                            """
                            SELECT table_name FROM information_schema.tables
                            WHERE table_schema = DATABASE() AND table_name = %s
                        """,
                            (table_name,),
                        )
                    else:
                        await cursor.execute(
                            """
                            SELECT table_name FROM information_schema.tables
                            WHERE table_schema = DATABASE() AND table_type = 'BASE TABLE'
                        """
                        )

                    return [row[0] for row in await cursor.fetchall()]

        except Exception as e:
            logger.error(f"‚ùå Áç≤ÂèñË°®Ê†ºÊ∏ÖÂñÆÂ§±Êïó: {e}")
            return []

    async def _get_index_statistics(self, table_name: str) -> Dict[str, Any]:
        """Áç≤ÂèñÁ¥¢ÂºïÁµ±Ë®àË≥áË®ä"""
        try:
            async with db_pool.connection() as conn:
                async with conn.cursor() as cursor:
                    # Áç≤ÂèñÁ¥¢ÂºïË≥áË®ä
                    await cursor.execute(
                        """
                        SELECT
                            index_name,
                            column_name,
                            cardinality,
                            sub_part,
                            nullable,
                            index_type
                        FROM information_schema.statistics
                        WHERE table_schema = DATABASE() AND table_name = %s
                        ORDER BY index_name, seq_in_index
                    """,
                        (table_name,),
                    )

                    index_info = await cursor.fetchall()

                    # Áç≤ÂèñÁ¥¢Âºï‰ΩøÁî®Áµ±Ë®à
                    await cursor.execute(
                        """
                        SELECT
                            index_name,
                            COUNT(*) as usage_count
                        FROM information_schema.statistics s
                        WHERE table_schema = DATABASE() AND table_name = %s
                        GROUP BY index_name
                    """,
                        (table_name,),
                    )

                    usage_stats = dict(await cursor.fetchall())

                    return {"index_info": index_info, "usage_stats": usage_stats}

        except Exception as e:
            logger.error(f"‚ùå Áç≤ÂèñÁ¥¢ÂºïÁµ±Ë®àÂ§±Êïó {table_name}: {e}")
            return {}

    async def _get_query_patterns(self, table_name: str) -> List[Dict[str, Any]]:
        """Áç≤ÂèñÊü•Ë©¢Ê®°Âºè"""
        try:
            async with db_pool.connection() as conn:
                async with conn.cursor() as cursor:
                    # ÂæûÊü•Ë©¢ÂàÜÊûêË®òÈåÑ‰∏≠Áç≤ÂèñÊ®°Âºè
                    await cursor.execute(
                        """
                        SELECT
                            query_text,
                            COUNT(*) as frequency,
                            AVG(execution_time) as avg_execution_time,
                            MAX(execution_time) as max_execution_time
                        FROM query_analysis_log
                        WHERE JSON_CONTAINS(tables_used, %s)
                        AND created_at > DATE_SUB(NOW(), INTERVAL 7 DAY)
                        GROUP BY query_hash
                        ORDER BY frequency DESC, avg_execution_time DESC
                        LIMIT 50
                    """,
                        (json.dumps([table_name]),),
                    )

                    patterns = []
                    for row in await cursor.fetchall():
                        patterns.append(
                            {
                                "query": row[0],
                                "frequency": row[1],
                                "avg_time": row[2],
                                "max_time": row[3],
                            }
                        )

                    return patterns

        except Exception as e:
            logger.error(f"‚ùå Áç≤ÂèñÊü•Ë©¢Ê®°ÂºèÂ§±Êïó {table_name}: {e}")
            return []

    async def _generate_index_recommendations(
        self, table_name: str, index_stats: Dict, query_patterns: List[Dict]
    ) -> List[IndexRecommendation]:
        """ÁîüÊàêÁ¥¢ÂºïÂª∫Ë≠∞"""
        recommendations = []

        try:
            # ÂàÜÊûê WHERE Ê¢ù‰ª∂‰∏≠Â∏∏Áî®ÁöÑÊ¨Ñ‰Ωç
            where_columns = self._extract_where_columns(query_patterns)

            # ÂàÜÊûê ORDER BY ‰∏≠Â∏∏Áî®ÁöÑÊ¨Ñ‰Ωç
            self._extract_order_columns(query_patterns)

            # ÂàÜÊûê JOIN Ê¢ù‰ª∂‰∏≠ÁöÑÊ¨Ñ‰Ωç
            self._extract_join_columns(query_patterns)

            # Áç≤ÂèñÁèæÊúâÁ¥¢Âºï
            existing_indexes = self._get_existing_indexes(index_stats)

            # ÁîüÊàê WHERE Ê¢ù‰ª∂Á¥¢ÂºïÂª∫Ë≠∞
            for column, frequency in where_columns.items():
                if not self._has_index_on_column(existing_indexes, column):
                    recommendation = IndexRecommendation(
                        table_name=table_name,
                        columns=[column],
                        index_type="BTREE",
                        reason=f"WHERE Ê¢ù‰ª∂‰∏≠È´òÈ†ª‰ΩøÁî® ({frequency} Ê¨°)",
                        estimated_benefit=min(frequency * 0.1, 10.0),
                        create_sql=f"CREATE INDEX idx_{table_name}_{column} ON {table_name} ({column})",
                    )
                    recommendations.append(recommendation)

            # ÁîüÊàêË§áÂêàÁ¥¢ÂºïÂª∫Ë≠∞
            composite_candidates = self._identify_composite_index_candidates(
                query_patterns
            )
            for columns in composite_candidates:
                if not self._has_composite_index(existing_indexes, columns):
                    columns_str = "_".join(columns)
                    recommendation = IndexRecommendation(
                        table_name=table_name,
                        columns=columns,
                        index_type="BTREE",
                        reason="Ë§áÂêàÊü•Ë©¢Ê¢ù‰ª∂ÂÑ™Âåñ",
                        estimated_benefit=5.0,
                        create_sql=f"CREATE INDEX idx_{table_name}_{columns_str} ON {table_name} ({', '.join(columns)})",
                    )
                    recommendations.append(recommendation)

        except Exception as e:
            logger.error(f"‚ùå ÁîüÊàêÁ¥¢ÂºïÂª∫Ë≠∞Â§±Êïó {table_name}: {e}")

        return recommendations

    def _extract_where_columns(self, query_patterns: List[Dict]) -> Dict[str, int]:
        """ÊèêÂèñ WHERE Ê¢ù‰ª∂‰∏≠ÁöÑÊ¨Ñ‰Ωç"""
        columns = {}

        for pattern in query_patterns:
            query = pattern["query"].upper()
            frequency = pattern["frequency"]

            # Á∞°ÂñÆÁöÑÊ≠£ÂâáÂåπÈÖç WHERE Ê¢ù‰ª∂
            where_matches = re.findall(r"WHERE\s+.*?(\w+)\s*[=<>!]", query)
            and_matches = re.findall(r"AND\s+(\w+)\s*[=<>!]", query)

            for match in where_matches + and_matches:
                columns[match] = columns.get(match, 0) + frequency

        return columns

    def _extract_order_columns(self, query_patterns: List[Dict]) -> Dict[str, int]:
        """ÊèêÂèñ ORDER BY ‰∏≠ÁöÑÊ¨Ñ‰Ωç"""
        columns = {}

        for pattern in query_patterns:
            query = pattern["query"].upper()
            frequency = pattern["frequency"]

            order_matches = re.findall(r"ORDER\s+BY\s+(\w+)", query)

            for match in order_matches:
                columns[match] = columns.get(match, 0) + frequency

        return columns

    def _extract_join_columns(self, query_patterns: List[Dict]) -> Dict[str, int]:
        """ÊèêÂèñ JOIN Ê¢ù‰ª∂‰∏≠ÁöÑÊ¨Ñ‰Ωç"""
        columns = {}

        for pattern in query_patterns:
            query = pattern["query"].upper()
            frequency = pattern["frequency"]

            join_matches = re.findall(r"JOIN\s+\w+\s+ON\s+.*?(\w+)\s*=", query)

            for match in join_matches:
                columns[match] = columns.get(match, 0) + frequency

        return columns

    def _get_existing_indexes(self, index_stats: Dict) -> List[Dict]:
        """Áç≤ÂèñÁèæÊúâÁ¥¢ÂºïË≥áË®ä"""
        indexes = []

        try:
            index_info = index_stats.get("index_info", [])

            current_index = None
            for info in index_info:
                index_name, column_name = info[0], info[1]

                if current_index is None or current_index["name"] != index_name:
                    if current_index:
                        indexes.append(current_index)

                    current_index = {"name": index_name, "columns": [column_name]}
                else:
                    current_index["columns"].append(column_name)

            if current_index:
                indexes.append(current_index)

        except Exception as e:
            logger.error(f"‚ùå Ëß£ÊûêÁèæÊúâÁ¥¢ÂºïÂ§±Êïó: {e}")

        return indexes

    def _has_index_on_column(self, existing_indexes: List[Dict], column: str) -> bool:
        """Ê™¢Êü•ÊòØÂê¶Â∑≤Â≠òÂú®Ë©≤Ê¨Ñ‰ΩçÁöÑÁ¥¢Âºï"""
        for index in existing_indexes:
            if column in index["columns"]:
                return True
        return False

    def _has_composite_index(
        self, existing_indexes: List[Dict], columns: List[str]
    ) -> bool:
        """Ê™¢Êü•ÊòØÂê¶Â∑≤Â≠òÂú®Ë§áÂêàÁ¥¢Âºï"""
        for index in existing_indexes:
            if set(columns).issubset(set(index["columns"])):
                return True
        return False

    def _identify_composite_index_candidates(
        self, query_patterns: List[Dict]
    ) -> List[List[str]]:
        """Ë≠òÂà•Ë§áÂêàÁ¥¢ÂºïÂÄôÈÅ∏"""
        candidates = []

        for pattern in query_patterns:
            query = pattern["query"].upper()

            # ÊâæÂá∫ÂêåÊôÇÂá∫ÁèæÂú® WHERE Ê¢ù‰ª∂‰∏≠ÁöÑÊ¨Ñ‰Ωç
            where_pattern = r"WHERE\s+(.*?)(?:ORDER\s+BY|GROUP\s+BY|HAVING|LIMIT|$)"
            where_match = re.search(where_pattern, query)

            if where_match:
                where_clause = where_match.group(1)
                columns = re.findall(r"(\w+)\s*[=<>!]", where_clause)

                if len(columns) > 1:
                    candidates.append(columns[:3])  # ÊúÄÂ§ö3ÂÄãÊ¨Ñ‰ΩçÁöÑË§áÂêàÁ¥¢Âºï

        return candidates

    # ========== ÊÄßËÉΩÁõ£Êéß ==========

    async def collect_database_metrics(self) -> DatabaseMetrics:
        """Êî∂ÈõÜË≥áÊñôÂ∫´ÊÄßËÉΩÊåáÊ®ô"""
        try:
            async with db_pool.connection() as conn:
                async with conn.cursor() as cursor:
                    # Áç≤ÂèñÂêÑÁ®ÆÊÄßËÉΩÊåáÊ®ô
                    metrics_queries = {
                        "query_cache_hit_rate": """
                            SELECT
                                ROUND(
                                    100 * (1 - Qcache_hits / (Qcache_hits + Qcache_inserts)), 2
                                ) as hit_rate
                            FROM
                                (SELECT
                                    VARIABLE_VALUE as Qcache_hits
                                 FROM information_schema.GLOBAL_STATUS
                                 WHERE VARIABLE_NAME = 'Qcache_hits') qh,
                                (SELECT
                                    VARIABLE_VALUE as Qcache_inserts
                                 FROM information_schema.GLOBAL_STATUS
                                 WHERE VARIABLE_NAME = 'Qcache_inserts') qi
                        """,
                        "slow_queries": """
                            SELECT VARIABLE_VALUE
                            FROM information_schema.GLOBAL_STATUS
                            WHERE VARIABLE_NAME = 'Slow_queries'
                        """,
                        "connections": """
                            SELECT
                                (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Threads_connected') as used,
                                (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_VARIABLES WHERE VARIABLE_NAME = 'max_connections') as max_conn
                        """,
                        "innodb_buffer_pool": """
                            SELECT
                                ROUND(
                                    100 * (
                                        (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Innodb_buffer_pool_read_requests') -
                                        (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Innodb_buffer_pool_reads')
                                    ) /
                                    (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Innodb_buffer_pool_read_requests'), 2
                                ) as buffer_hit_rate
                        """,
                    }

                    metrics_data = {}

                    for metric_name, query in metrics_queries.items():
                        try:
                            await cursor.execute(query)
                            result = await cursor.fetchone()

                            if metric_name == "connections":
                                metrics_data["connections_used"] = (
                                    int(result[0]) if result else 0
                                )
                                metrics_data["max_connections"] = (
                                    int(result[1]) if result else 0
                                )
                            else:
                                metrics_data[metric_name] = (
                                    float(result[0]) if result and result[0] else 0.0
                                )

                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Êî∂ÈõÜÊåáÊ®ôÂ§±Êïó {metric_name}: {e}")
                            metrics_data[metric_name] = 0.0

                    # ÂâµÂª∫ÊåáÊ®ôÁâ©‰ª∂
                    metrics = DatabaseMetrics(
                        query_cache_hit_rate=metrics_data.get(
                            "query_cache_hit_rate", 0.0
                        ),
                        slow_query_count=int(metrics_data.get("slow_queries", 0)),
                        connections_used=metrics_data.get("connections_used", 0),
                        max_connections=metrics_data.get("max_connections", 0),
                        innodb_buffer_pool_hit_rate=metrics_data.get(
                            "innodb_buffer_pool", 0.0
                        ),
                        table_scan_rate=0.0,  # ÈúÄË¶ÅÈ°çÂ§ñË®àÁÆó
                        temp_table_rate=0.0,  # ÈúÄË¶ÅÈ°çÂ§ñË®àÁÆó
                        key_read_hit_rate=0.0,  # ÈúÄË¶ÅÈ°çÂ§ñË®àÁÆó
                        timestamp=datetime.now(timezone.utc),
                    )

                    # Âø´ÂèñÊåáÊ®ô
                    self.metrics_cache = metrics
                    self.metrics_cache_time = datetime.now(timezone.utc)

                    return metrics

        except Exception as e:
            logger.error(f"‚ùå Êî∂ÈõÜË≥áÊñôÂ∫´ÊåáÊ®ôÂ§±Êïó: {e}")
            return DatabaseMetrics(
                query_cache_hit_rate=0.0,
                slow_query_count=0,
                connections_used=0,
                max_connections=0,
                innodb_buffer_pool_hit_rate=0.0,
                table_scan_rate=0.0,
                temp_table_rate=0.0,
                key_read_hit_rate=0.0,
                timestamp=datetime.now(timezone.utc),
            )

    async def get_optimization_summary(self, days: int = 7) -> Dict[str, Any]:
        """Áç≤ÂèñÂÑ™ÂåñÊëòË¶ÅÂ†±Âëä"""
        try:
            async with db_pool.connection() as conn:
                async with conn.cursor() as cursor:
                    # ÊÖ¢Êü•Ë©¢Áµ±Ë®à
                    await cursor.execute(
                        """
                        SELECT
                            COUNT(*) as total_slow_queries,
                            AVG(execution_time) as avg_slow_time,
                            MAX(execution_time) as max_slow_time,
                            COUNT(DISTINCT query_hash) as unique_slow_queries
                        FROM query_analysis_log
                        WHERE execution_time > %s
                        AND created_at > DATE_SUB(NOW(), INTERVAL %s DAY)
                    """,
                        (self.slow_query_threshold, days),
                    )

                    slow_query_stats = await cursor.fetchone()

                    # ÂÑ™ÂåñÁ≠âÁ¥öÂàÜ‰Ωà
                    await cursor.execute(
                        """
                        SELECT
                            optimization_level,
                            COUNT(*) as count
                        FROM query_analysis_log
                        WHERE created_at > DATE_SUB(NOW(), INTERVAL %s DAY)
                        GROUP BY optimization_level
                    """,
                        (days,),
                    )

                    optimization_levels = dict(await cursor.fetchall())

                    # ÊúÄÈ†ªÁπÅÁöÑÊü•Ë©¢
                    await cursor.execute(
                        """
                        SELECT
                            query_text,
                            COUNT(*) as frequency,
                            AVG(execution_time) as avg_time,
                            optimization_level
                        FROM query_analysis_log
                        WHERE created_at > DATE_SUB(NOW(), INTERVAL %s DAY)
                        GROUP BY query_hash
                        ORDER BY frequency DESC
                        LIMIT 10
                    """,
                        (days,),
                    )

                    frequent_queries = await cursor.fetchall()

                    # Á¥¢ÂºïÂª∫Ë≠∞Áµ±Ë®à
                    await cursor.execute(
                        """
                        SELECT
                            status,
                            COUNT(*) as count,
                            AVG(estimated_benefit) as avg_benefit
                        FROM index_recommendations
                        WHERE created_at > DATE_SUB(NOW(), INTERVAL %s DAY)
                        GROUP BY status
                    """,
                        (days,),
                    )

                    index_stats = await cursor.fetchall()

                    return {
                        "period_days": days,
                        "slow_queries": {
                            "total": slow_query_stats[0] if slow_query_stats else 0,
                            "avg_time": (
                                float(slow_query_stats[1])
                                if slow_query_stats and slow_query_stats[1]
                                else 0
                            ),
                            "max_time": (
                                float(slow_query_stats[2])
                                if slow_query_stats and slow_query_stats[2]
                                else 0
                            ),
                            "unique_count": (
                                slow_query_stats[3] if slow_query_stats else 0
                            ),
                        },
                        "optimization_levels": optimization_levels,
                        "frequent_queries": [
                            {
                                "query": (
                                    query[:100] + "..." if len(query) > 100 else query
                                ),
                                "frequency": freq,
                                "avg_time": float(avg_time),
                                "level": level,
                            }
                            for query, freq, avg_time, level in frequent_queries
                        ],
                        "index_recommendations": dict(index_stats),
                        "timestamp": datetime.now(timezone.utc).isoformat(),
                    }

        except Exception as e:
            logger.error(f"‚ùå Áç≤ÂèñÂÑ™ÂåñÊëòË¶ÅÂ§±Êïó: {e}")
            return {}

    async def _load_optimization_history(self):
        """ËºâÂÖ•ÂÑ™ÂåñÊ≠∑Âè≤"""
        # ÂØ¶ÁèæËºâÂÖ•Ê≠∑Âè≤Ë®òÈåÑÁöÑÈÇèËºØ

    async def _analyze_existing_tables(self):
        """ÂàÜÊûêÁèæÊúâË°®ÁµêÊßã"""
        # ÂØ¶ÁèæÂàÜÊûêË°®ÁµêÊßãÁöÑÈÇèËºØ


# ========== ÂÖ®ÂüüÂØ¶‰æã ==========

# ÂâµÂª∫ÂÖ®ÂüüË≥áÊñôÂ∫´ÂÑ™ÂåñÂô®ÂØ¶‰æã
db_optimizer = DatabaseOptimizer()


async def get_db_optimizer() -> DatabaseOptimizer:
    """Áç≤ÂèñË≥áÊñôÂ∫´ÂÑ™ÂåñÂô®ÂØ¶‰æã"""
    return db_optimizer


# Ë£ùÈ£æÂô®ÔºöËá™ÂãïÂàÜÊûêÊü•Ë©¢ÊÄßËÉΩ
def query_analyzed(func):
    """Êü•Ë©¢ÂàÜÊûêË£ùÈ£æÂô®"""

    async def wrapper(*args, **kwargs):
        start_time = time.time()

        try:
            result = await func(*args, **kwargs)

            # Â¶ÇÊûúÂü∑Ë°åÊôÇÈñìË∂ÖÈÅéÈñæÂÄºÔºåË®òÈåÑÂàÜÊûê
            execution_time = time.time() - start_time
            if execution_time > db_optimizer.slow_query_threshold:
                # ÈÄôË£°ÈúÄË¶ÅÁç≤ÂèñÂØ¶ÈöõÁöÑÊü•Ë©¢Ë™ûÂè•ÔºåÂèØËÉΩÈúÄË¶Å‰øÆÊîπÂáΩÊï∏Á∞ΩÂêç
                logger.warning(f"ÊÖ¢Êü•Ë©¢Ê™¢Ê∏¨: {func.__name__} - {execution_time:.3f}s")

            return result

        except Exception as e:
            logger.error(f"Êü•Ë©¢Âü∑Ë°åÂ§±Êïó {func.__name__}: {e}")
            raise

    return wrapper
