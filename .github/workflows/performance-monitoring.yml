name: ğŸ“Š CI/CD Performance Monitoring

on:
  workflow_run:
    workflows: 
      - "ğŸ§ª Test Coverage & Quality"
      - "ğŸ” Code Quality Checks" 
      - "ğŸ”’ Security Scans"
      - "âš¡ Optimized CI Pipeline"
      - "ğŸš€ Lightweight CI"
    types:
      - completed
  push:
    branches: [dev, main]
    paths:
      - '.github/workflows/performance-monitoring.yml'
  schedule:
    # æ¯å¤© UTC 02:00 ç”Ÿæˆæ—¥å ±å‘Š
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      analysis_type:
        description: 'åˆ†æé¡å‹'
        required: false
        default: 'full'
        type: choice
        options:
          - full
          - trends
          - bottlenecks

env:
  MONITORING_VERSION: v1.0.0

jobs:
  collect-metrics:
    name: ğŸ“ˆ æ”¶é›†æ•ˆèƒ½æŒ‡æ¨™
    runs-on: ubuntu-latest
    if: github.event.workflow_run.conclusion != 'skipped'
    timeout-minutes: 10

    outputs:
      has_data: ${{ steps.collect.outputs.has_data }}
      execution_time: ${{ steps.collect.outputs.execution_time }}
      workflow_name: ${{ steps.collect.outputs.workflow_name }}

    steps:
    - name: ğŸ“¥ æª¢å‡ºä»£ç¢¼
      uses: actions/checkout@v4
      with:
        fetch-depth: 100  # ç²å–æ›´å¤šæ­·å²ä»¥é€²è¡Œè¶¨å‹¢åˆ†æ

    - name: ğŸ è¨­ç½® Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: ğŸ“¦ å®‰è£ç›£æ§ä¾è³´
      run: |
        pip install requests python-dateutil matplotlib seaborn pandas

    - name: ğŸ“Š æ”¶é›† workflow åŸ·è¡Œæ•¸æ“š
      id: collect
      run: |
        cat << 'EOF' > collect_metrics.py
        import os
        import sys
        import json
        import requests
        from datetime import datetime, timedelta
        import dateutil.parser
        
        def collect_workflow_metrics():
            """æ”¶é›† workflow åŸ·è¡ŒæŒ‡æ¨™"""
            
            # GitHub API è¨­ç½®
            repo = os.environ.get('GITHUB_REPOSITORY', 'Craig-0219/potato')
            token = os.environ.get('GITHUB_TOKEN')
            
            if not token:
                print("âŒ GITHUB_TOKEN not available")
                return None
                
            headers = {
                'Authorization': f'token {token}',
                'Accept': 'application/vnd.github.v3+json'
            }
            
            # ç²å–æœ€è¿‘ 30 æ¬¡ workflow åŸ·è¡Œ
            url = f'https://api.github.com/repos/{repo}/actions/runs'
            params = {
                'per_page': 30,
                'branch': 'dev',
                'status': 'completed'
            }
            
            try:
                response = requests.get(url, headers=headers, params=params)
                response.raise_for_status()
                
                runs_data = response.json()
                metrics = []
                
                for run in runs_data.get('workflow_runs', []):
                    # è¨ˆç®—åŸ·è¡Œæ™‚é–“
                    created_at = dateutil.parser.parse(run['created_at'])
                    updated_at = dateutil.parser.parse(run['updated_at'])
                    duration_seconds = (updated_at - created_at).total_seconds()
                    
                    metrics.append({
                        'workflow_name': run['name'],
                        'run_id': run['id'],
                        'status': run['conclusion'],
                        'branch': run['head_branch'],
                        'duration_seconds': duration_seconds,
                        'duration_minutes': round(duration_seconds / 60, 2),
                        'created_at': run['created_at'],
                        'updated_at': run['updated_at'],
                        'trigger_event': run['event'],
                        'attempt': run['run_attempt']
                    })
                
                return metrics
                
            except Exception as e:
                print(f"âŒ æ”¶é›†æŒ‡æ¨™å¤±æ•—: {e}")
                return None
        
        # ä¸»é‚è¼¯
        if __name__ == '__main__':
            print("ğŸ“Š é–‹å§‹æ”¶é›† CI/CD æ•ˆèƒ½æŒ‡æ¨™...")
            
            metrics = collect_workflow_metrics()
            if not metrics:
                print("âš ï¸ æœªæ”¶é›†åˆ°æœ‰æ•ˆæ•¸æ“š")
                sys.exit(0)
            
            # è¼¸å‡ºçµ±è¨ˆæ‘˜è¦
            print(f"âœ… æ”¶é›†åˆ° {len(metrics)} å€‹åŸ·è¡Œè¨˜éŒ„")
            
            # è¨ˆç®—åŸºæœ¬çµ±è¨ˆ
            durations = [m['duration_minutes'] for m in metrics if m['duration_minutes'] > 0]
            if durations:
                avg_duration = sum(durations) / len(durations)
                max_duration = max(durations)
                min_duration = min(durations)
                
                print(f"ğŸ“Š åŸ·è¡Œæ™‚é–“çµ±è¨ˆ:")
                print(f"  â€¢ å¹³å‡: {avg_duration:.2f} åˆ†é˜")
                print(f"  â€¢ æœ€é•·: {max_duration:.2f} åˆ†é˜")  
                print(f"  â€¢ æœ€çŸ­: {min_duration:.2f} åˆ†é˜")
                
                # è¨­ç½® GitHub Actions è¼¸å‡º
                print(f"has_data=true", file=open(os.environ['GITHUB_OUTPUT'], 'a'))
                print(f"execution_time={avg_duration:.2f}", file=open(os.environ['GITHUB_OUTPUT'], 'a'))
                
                # è­˜åˆ¥æœ€å¸¸åŸ·è¡Œçš„ workflow
                workflow_counts = {}
                for m in metrics:
                    name = m['workflow_name']
                    workflow_counts[name] = workflow_counts.get(name, 0) + 1
                
                most_common = max(workflow_counts.items(), key=lambda x: x[1])
                print(f"workflow_name={most_common[0]}", file=open(os.environ['GITHUB_OUTPUT'], 'a'))
                
                # ä¿å­˜è©³ç´°æ•¸æ“šä¾›å¾ŒçºŒåˆ†æ
                with open('metrics_data.json', 'w') as f:
                    json.dump(metrics, f, indent=2)
                
                print("ğŸ’¾ æŒ‡æ¨™æ•¸æ“šå·²ä¿å­˜åˆ° metrics_data.json")
            else:
                print("âš ï¸ ç„¡æœ‰æ•ˆåŸ·è¡Œæ™‚é–“æ•¸æ“š")
        EOF
        
        python collect_metrics.py

    - name: ğŸ“¤ ä¸Šå‚³æŒ‡æ¨™æ•¸æ“š
      if: steps.collect.outputs.has_data == 'true'
      uses: actions/upload-artifact@v4
      with:
        name: performance-metrics-${{ github.run_id }}
        path: |
          metrics_data.json
        retention-days: 30

  analyze-bottlenecks:
    name: ğŸ” ç“¶é ¸åˆ†æ
    needs: collect-metrics
    runs-on: ubuntu-latest
    if: needs.collect-metrics.outputs.has_data == 'true'
    timeout-minutes: 15

    steps:
    - name: ğŸ“¥ æª¢å‡ºä»£ç¢¼
      uses: actions/checkout@v4

    - name: ğŸ è¨­ç½® Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: ğŸ“¦ å®‰è£åˆ†æä¾è³´
      run: |
        pip install matplotlib seaborn pandas numpy requests

    - name: ğŸ“¥ ä¸‹è¼‰æŒ‡æ¨™æ•¸æ“š
      uses: actions/download-artifact@v4
      with:
        name: performance-metrics-${{ github.run_id }}

    - name: ğŸ” åŸ·è¡Œç“¶é ¸åˆ†æ
      run: |
        cat << 'EOF' > analyze_bottlenecks.py
        import json
        import pandas as pd
        import matplotlib.pyplot as plt
        import seaborn as sns
        from datetime import datetime
        import numpy as np
        
        def analyze_performance_bottlenecks():
            """åˆ†æ CI/CD æ•ˆèƒ½ç“¶é ¸"""
            
            # è¼‰å…¥æ•¸æ“š
            try:
                with open('metrics_data.json', 'r') as f:
                    metrics = json.load(f)
            except FileNotFoundError:
                print("âŒ æ‰¾ä¸åˆ°æŒ‡æ¨™æ•¸æ“šæ–‡ä»¶")
                return
            
            if not metrics:
                print("âŒ æ²’æœ‰å¯åˆ†æçš„æ•¸æ“š")
                return
                
            df = pd.DataFrame(metrics)
            
            print("ğŸ” CI/CD æ•ˆèƒ½ç“¶é ¸åˆ†æå ±å‘Š")
            print("=" * 50)
            
            # 1. æŒ‰ workflow åˆ†æå¹³å‡åŸ·è¡Œæ™‚é–“
            print("\nğŸ“Š å„ Workflow å¹³å‡åŸ·è¡Œæ™‚é–“:")
            workflow_stats = df.groupby('workflow_name')['duration_minutes'].agg(['mean', 'median', 'max', 'count'])
            workflow_stats = workflow_stats.sort_values('mean', ascending=False)
            
            for name, stats in workflow_stats.iterrows():
                print(f"â€¢ {name}:")
                print(f"  - å¹³å‡: {stats['mean']:.2f} åˆ†é˜")
                print(f"  - ä¸­ä½æ•¸: {stats['median']:.2f} åˆ†é˜")
                print(f"  - æœ€é•·: {stats['max']:.2f} åˆ†é˜") 
                print(f"  - åŸ·è¡Œæ¬¡æ•¸: {stats['count']}")
            
            # 2. è­˜åˆ¥æ•ˆèƒ½ç“¶é ¸ (å‰ 3 å¤§è€—æ™‚ workflows)
            print("\nğŸš¨ æ•ˆèƒ½ç“¶é ¸ TOP 3:")
            top_bottlenecks = workflow_stats.head(3)
            bottleneck_list = []
            
            for i, (name, stats) in enumerate(top_bottlenecks.iterrows(), 1):
                print(f"{i}. {name}: å¹³å‡ {stats['mean']:.2f} åˆ†é˜")
                bottleneck_list.append({
                    'rank': i,
                    'workflow': name,
                    'avg_duration': stats['mean'],
                    'improvement_potential': max(0, stats['mean'] - 5.0)  # ç›®æ¨™æ˜¯ 5 åˆ†é˜å…§
                })
            
            # 3. æˆåŠŸç‡åˆ†æ
            print("\nâœ… æˆåŠŸç‡åˆ†æ:")
            success_rate = df.groupby('workflow_name')['status'].apply(
                lambda x: (x == 'success').sum() / len(x) * 100
            ).sort_values(ascending=False)
            
            for name, rate in success_rate.items():
                status = "âœ…" if rate >= 95 else "âš ï¸" if rate >= 90 else "âŒ"
                print(f"{status} {name}: {rate:.1f}% æˆåŠŸç‡")
            
            # 4. è¶¨å‹¢åˆ†æ (æœ€è¿‘ 10 æ¬¡åŸ·è¡Œ)
            print("\nğŸ“ˆ æœ€è¿‘è¶¨å‹¢åˆ†æ:")
            recent_df = df.head(10)
            recent_avg = recent_df['duration_minutes'].mean()
            overall_avg = df['duration_minutes'].mean()
            trend = "ğŸ“ˆ ä¸Šå‡" if recent_avg > overall_avg else "ğŸ“‰ ä¸‹é™"
            
            print(f"â€¢ æœ€è¿‘ 10 æ¬¡å¹³å‡: {recent_avg:.2f} åˆ†é˜")
            print(f"â€¢ æ•´é«”å¹³å‡: {overall_avg:.2f} åˆ†é˜")
            print(f"â€¢ è¶¨å‹¢: {trend} ({recent_avg - overall_avg:+.2f} åˆ†é˜)")
            
            # 5. å„ªåŒ–å»ºè­°
            print("\nğŸ’¡ å„ªåŒ–å»ºè­°:")
            total_potential_savings = sum(b['improvement_potential'] for b in bottleneck_list)
            
            print(f"1. å„ªåŒ–å‰ 3 å¤§ç“¶é ¸ï¼Œé ä¼°å¯ç¯€çœ {total_potential_savings:.1f} åˆ†é˜")
            print("2. å¯¦ä½œæ™ºèƒ½è·³éæ©Ÿåˆ¶ï¼Œé‡å°å°è®Šæ›´é¿å…å®Œæ•´æ¸¬è©¦")
            print("3. æ”¹å–„å¿«å–ç­–ç•¥ï¼Œæé«˜ä¾è³´å®‰è£æ•ˆç‡")
            
            if success_rate.min() < 95:
                failing_workflows = success_rate[success_rate < 95].index.tolist()
                print(f"4. æ”¹å–„å¤±æ•—ç‡è¼ƒé«˜çš„ workflows: {', '.join(failing_workflows)}")
            
            # ç”Ÿæˆç°¡å–®çš„å ±å‘Šæ‘˜è¦
            with open('performance_analysis.md', 'w') as f:
                f.write("# CI/CD æ•ˆèƒ½åˆ†æå ±å‘Š\n\n")
                f.write(f"**åˆ†ææ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"**æ•¸æ“šç¯„åœ**: æœ€è¿‘ {len(df)} æ¬¡åŸ·è¡Œ\n\n")
                
                f.write("## ğŸ“Š é—œéµæŒ‡æ¨™\n")
                f.write(f"- å¹³å‡åŸ·è¡Œæ™‚é–“: {overall_avg:.2f} åˆ†é˜\n")
                f.write(f"- æ•´é«”æˆåŠŸç‡: {(df['status'] == 'success').sum() / len(df) * 100:.1f}%\n")
                f.write(f"- æœ€é•·åŸ·è¡Œæ™‚é–“: {df['duration_minutes'].max():.2f} åˆ†é˜\n\n")
                
                f.write("## ğŸš¨ ä¸»è¦ç“¶é ¸\n")
                for b in bottleneck_list:
                    f.write(f"{b['rank']}. {b['workflow']}: {b['avg_duration']:.2f} åˆ†é˜\n")
                
                f.write(f"\n## ğŸ’° å„ªåŒ–æ½›åŠ›\n")
                f.write(f"é ä¼°å¯ç¯€çœåŸ·è¡Œæ™‚é–“: {total_potential_savings:.1f} åˆ†é˜\n")
                f.write(f"ç›®æ¨™åŸ·è¡Œæ™‚é–“: < 8 åˆ†é˜\n")
        
        if __name__ == '__main__':
            analyze_performance_bottlenecks()
        EOF
        
        python analyze_bottlenecks.py

    - name: ğŸ“¤ ä¸Šå‚³åˆ†æå ±å‘Š
      uses: actions/upload-artifact@v4
      with:
        name: performance-analysis-${{ github.run_id }}
        path: |
          performance_analysis.md
        retention-days: 30

  generate-summary:
    name: ğŸ“‹ ç”Ÿæˆæ•ˆèƒ½æ‘˜è¦
    needs: [collect-metrics, analyze-bottlenecks]
    runs-on: ubuntu-latest
    if: always() && needs.collect-metrics.outputs.has_data == 'true'
    timeout-minutes: 5

    steps:
    - name: ğŸ“¥ ä¸‹è¼‰åˆ†æçµæœ
      uses: actions/download-artifact@v4
      with:
        name: performance-analysis-${{ github.run_id }}

    - name: ğŸ“Š ç”Ÿæˆæ•ˆèƒ½æ‘˜è¦
      run: |
        echo "## ğŸš€ CI/CD æ•ˆèƒ½ç›£æ§å ±å‘Š" > performance_summary.md
        echo "" >> performance_summary.md
        echo "**ç›£æ§æ™‚é–“**: $(date '+%Y-%m-%d %H:%M:%S UTC')" >> performance_summary.md
        echo "**å¹³å‡åŸ·è¡Œæ™‚é–“**: ${{ needs.collect-metrics.outputs.execution_time }} åˆ†é˜" >> performance_summary.md
        echo "**ä¸»è¦ Workflow**: ${{ needs.collect-metrics.outputs.workflow_name }}" >> performance_summary.md
        echo "" >> performance_summary.md
        
        if [ -f "performance_analysis.md" ]; then
          echo "### è©³ç´°åˆ†æ" >> performance_summary.md
          cat performance_analysis.md >> performance_summary.md
        fi
        
        echo "" >> performance_summary.md
        echo "---" >> performance_summary.md
        echo "*è‡ªå‹•ç”Ÿæˆæ–¼: $(date)*" >> performance_summary.md

    - name: ğŸ“¤ ä¸Šå‚³æœ€çµ‚å ±å‘Š
      uses: actions/upload-artifact@v4  
      with:
        name: ci-cd-performance-report
        path: |
          performance_summary.md
          performance_analysis.md
        retention-days: 90

    - name: ğŸ“Š è¼¸å‡ºæ•ˆèƒ½æ‘˜è¦
      run: |
        echo "ğŸ¯ CI/CD æ•ˆèƒ½ç›£æ§å®Œæˆ!"
        echo ""
        echo "ğŸ“Š é—œéµæŒ‡æ¨™:"
        echo "â€¢ å¹³å‡åŸ·è¡Œæ™‚é–“: ${{ needs.collect-metrics.outputs.execution_time }} åˆ†é˜"
        echo "â€¢ ç›£æ§æ•¸æ“š: å·²æ”¶é›†ä¸¦åˆ†æ"
        echo "â€¢ ç“¶é ¸è­˜åˆ¥: å·²å®Œæˆ"
        echo ""
        echo "ğŸ“‹ å¾ŒçºŒè¡Œå‹•:"
        echo "1. æŸ¥çœ‹è©³ç´°åˆ†æå ±å‘Š (Artifacts)"
        echo "2. æ ¹æ“šç“¶é ¸åˆ†æå¯¦æ–½å„ªåŒ–"
        echo "3. å»ºç«‹æ•ˆèƒ½åŸºæº–è¿½è¹¤"
        echo ""
        echo "ğŸ”— ç›¸é—œæ–‡ä»¶:"
        echo "â€¢ æ•ˆèƒ½åˆ†æå ±å‘Š: performance_analysis.md"
        echo "â€¢ åŸå§‹æ•¸æ“š: metrics_data.json"